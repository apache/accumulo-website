<!DOCTYPE html>
<html lang="en">
<head>
<!--
    Licensed to the Apache Software Foundation (ASF) under one or more
    contributor license agreements.  See the NOTICE file distributed with
    this work for additional information regarding copyright ownership.
    The ASF licenses this file to You under the Apache License, Version 2.0
    (the "License"); you may not use this file except in compliance with
    the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing, software
    distributed under the License is distributed on an "AS IS" BASIS,
    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    See the License for the specific language governing permissions and
    limitations under the License.
-->
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">

<link rel="stylesheet" type="text/css" href="/css/bootstrap/5.3.3/dist/css/bootstrap.css">
<link rel="stylesheet" type="text/css" href="/css/fontawesome/fontawesome-free-6.6.0-web/css/all.css">
<link rel="stylesheet" type="text/css" href="/css/datatables/bs5/dt-2.0.8/datatables.css">
<link rel="stylesheet" type="text/css" href="/css/accumulo.css">

<title>Using HDFS Erasure Coding with Accumulo</title>

<script type="text/javascript" src="/js/jquery/3.7.1/jquery.js"></script>
<script type="text/javascript" src="/js/bootstrap/5.3.3/dist/js/bootstrap.bundle.js"></script>
<script type="text/javascript" src="/js/datatables/bs5/dt-2.0.8/datatables.js"></script>
<script type="text/javascript" src="https://www.apachecon.com/event-images/snippet.js"></script>
<script type="text/javascript" src="/js/accumulo.js"></script>
</head>
<body style="padding-top: 100px">

  <nav class="navbar navbar-expand-lg navbar-light fixed-top bg-light">
  <div class="container">
    <a class="navbar-brand" href="/">
      <img alt="Apache Accumulo" id="nav-logo" src="/images/accumulo-logo.png" width="200">
    </a>
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar-items">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbar-items">
      <ul class="navbar-nav me-auto">
        <li class="nav-item"><a class="nav-link" href="/downloads">Download</a></li>
        <li class="nav-item"><a class="nav-link" href="/tour">Tour</a></li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown">Releases</a>
          <ul class="dropdown-menu">
            <li><a class="dropdown-item" href="/release/accumulo-3.0.0/">3.0.0 (Latest non-LTM)</a></li>
            <li><a class="dropdown-item" href="/release/accumulo-2.1.3/">2.1.3 (Latest LTM)</a></li>
            <li><a class="dropdown-item" href="/release/accumulo-1.10.4/">1.10.4 (Legacy LTM)</a></li>
            <li><a class="dropdown-item" href="/release/">Archive</a></li>
          </ul>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown">Documentation</a>
          <ul class="dropdown-menu">
            <li><a class="dropdown-item" href="/docs/2.x">User Manual (2.x)</a></li>
            <li><a class="dropdown-item" href="/docs/2.x/apidocs">Javadocs (2.x)</a></li>
            <li><a class="dropdown-item" href="/api">Public API</a></li>
            <li><a class="dropdown-item" href="/quickstart-1.x">Quickstart (1.x)</a></li>
            <li><a class="dropdown-item" href="/accumulo2-maven-plugin">Accumulo Maven Plugin</a></li>
            <li><a class="dropdown-item" href="/1.10/accumulo_user_manual.html">User Manual (1.10)</a></li>
            <li><a class="dropdown-item" href="/1.10/apidocs">Javadocs (1.10)</a></li>
            <li><a class="dropdown-item" href="/external-docs">External Docs</a></li>
            <li><a class="dropdown-item" href="/docs-archive/">Archive</a></li>
          </ul>
        </li>
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown">Community</a>
          <ul class="dropdown-menu">
            <li><a class="dropdown-item" href="/contact-us">Contact Us</a></li>
            <li><a class="dropdown-item" href="/how-to-contribute">How To Contribute</a></li>
            <li><a class="dropdown-item" href="/people">People</a></li>
            <li><a class="dropdown-item" href="/related-projects">Related Projects</a></li>
          </ul>
        </li>
        <li class="nav-item"><a class="nav-link" href="/search">Search</a></li>
      </ul>
      <ul class="navbar-nav ms-auto">
        <li class="nav-item dropdown">
          <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown">
            <img alt="Apache Software Foundation" src="https://www.apache.org/foundation/press/kit/feather.svg" width="15"/>
          </a>
          <ul class="dropdown-menu dropdown-menu-end">
            <li><a class="dropdown-item" href="https://www.apache.org">Apache Homepage <span class="fa-solid fa-up-right-from-square"></span></a></li>
            <li><a class="dropdown-item" href="https://www.apache.org/licenses/">License <span class="fa-solid fa-up-right-from-square"></span></a></li>
            <li><a class="dropdown-item" href="https://www.apache.org/foundation/sponsorship">Sponsorship <span class="fa-solid fa-up-right-from-square"></span></a></li>
            <li><a class="dropdown-item" href="https://www.apache.org/security">Security <span class="fa-solid fa-up-right-from-square"></span></a></li>
            <li><a class="dropdown-item" href="https://www.apache.org/foundation/thanks">Thanks <span class="fa-solid fa-up-right-from-square"></span></a></li>
            <li><a class="dropdown-item" href="https://www.apache.org/foundation/policies/conduct">Code of Conduct <span class="fa-solid fa-up-right-from-square"></span></a></li>
            <li><a class="dropdown-item" href="https://www.apache.org/foundation/policies/privacy.html">Privacy Policy<span class="fa-solid fa-up-right-from-square"></span></a></li>
            <li><a class="dropdown-item" href="https://www.apache.org/events/current-event.html">Current Event <span class="fa-solid fa-up-right-from-square"></span></a></li>
          </ul>
        </li>
      </ul>
    </div>
  </div>
</nav>


  <div class="container">
    <div class="row">
      <div class="col-md-12">

        <div id="non-canonical" style="display: none; background-color: #F0E68C; padding-left: 1em;">
          Visit the official site at: <a href="https://accumulo.apache.org">https://accumulo.apache.org</a>
        </div>
        <div id="content">
          
          <h1 class="title">Using HDFS Erasure Coding with Accumulo</h1>
          
          <p>
<b>Author: </b>&nbsp;&nbsp;Ed Seidl<br>
<b>Date: </b>&nbsp;&nbsp;17 Sep 2019<br>

</p>

<p>HDFS normally stores multiple copies of each file for both performance and durability reasons.
The number of copies is controlled via HDFS replication settings, and by default is set to 3. Hadoop 3,
introduced the use of erasure coding (EC), which improves durability while decreasing overhead.
Since Accumulo 2.0 now supports Hadoop 3, it’s time to take a look at whether using
EC with Accumulo makes sense.</p>

<ul>
  <li><a href="#ec-intro">EC Intro</a></li>
  <li><a href="#ec-performance">EC Performance</a></li>
  <li><a href="#accumulo-performance-with-ec">Accumulo Performance with EC</a></li>
</ul>

<h3 id="ec-intro">EC Intro</h3>

<p>By default HDFS achieves durability via block replication.  Usually
the replication count is 3, resulting in a storage overhead of 200%. Hadoop 3
introduced EC as a better way to achieve durability.  More info can be
found <a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html">here</a>.
EC behaves much like RAID 5 or 6…for <em>k</em> blocks of data, <em>m</em> blocks of
parity data are generated, from which the original data can be recovered in the
event of disk or node failures (erasures, in EC parlance).  A typical EC scheme is Reed-Solomon 6-3, where
6 data blocks produce 3 parity blocks, an overhead of only 50%.  In addition
to doubling the available disk space, RS-6-3 is also more fault
tolerant…a loss of 3 data blocks can be tolerated, where triple replication
can only lose two blocks.</p>

<p>More storage, better resiliency, so what’s the catch?  One concern is
the time spent calculating the parity blocks.  Unlike replication
, where a client writes a block, and then the DataNodes replicate
the data, an EC HDFS client is responsible for computing the parity and sending that
to the DataNodes.  This increases the CPU and network load on the client.  The CPU
hit can be mitigated by using Intels ISA-L library, but only on CPUs
that support AVX or AVX2 instructions.  (See <a href="https://www.slideshare.net/HadoopSummit/debunking-the-myths-of-hdfs-erasure-coding-performance">EC Myths</a> and <a href="https://blog.cloudera.com/introduction-to-hdfs-erasure-coding-in-apache-hadoop/">EC Introduction</a>
for some interesting claims). In addition, unlike the serial replication I/O path,
the EC I/O path is parallel providing greater throughput. In our testing, sequential writes to
an EC directory were as much as 3 times faster than a replication directory
, and reads were up to 2 times faster.</p>

<p>Another side effect of EC is loss of data locality.  For performance reasons, EC
data blocks are striped, so multiple DataNodes must be contacted to read a single
block of data.  For large sequential reads this is not a
problem, but it can be an issue for small random lookups.  For the latter case,
using RS 6-3 with 64KB stripes mitigates some of the random lookup pain
without compromising sequential read/write performance.</p>

<h4 id="important-warning">Important Warning</h4>

<p>Before continuing, an important caveat;  the current implementation of EC on Hadoop supports neither hsync
nor hflush.  Both of these operations are silent no-ops (EC <a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#Limitations">limitations</a>).  We discovered this the hard
way when a data center power loss resulted in write-ahead log corruption, which were
stored in an EC directory.  To avoid this problem ensure all
WAL directories use replication.  It’s probably a good idea to keep the
accumulo namespace replicated as well, but we have no evidence to back up that assertion.  As with all
things, don’t test on production data.</p>

<h3 id="ec-performance">EC Performance</h3>

<p>To test EC performance, we created a series of clusters on AWS.  Our Accumulo stack consisted of
Hadoop 3.1.1 built with the Intel ISA-L library enabled, Zookeeper 3.4.13, and Accumulo 1.9.3 configured
to work with Hadoop 3 (we did our testing before the official release of Accumulo 2.0). The encoding
policy is set per-directory using the <a href="https://hadoop.apache.org/docs/r3.2.0/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#Administrative_commands">hdfs</a> command-line tool. To set the encoding policy
for an Accumulo table, first find the table ID (for instance using the Accumulo shell’s
“table -l” command), and then from the command line set the policy for the corresponding directory
under /accumulo/tables.  Note that changing the policy on a directory will set the policy for
child directories, but will not change any files contained within.  To change the policy on an existing
Accumulo table, you must first set the encoding policy, and then run a major compaction to rewrite
the RFiles for the table.</p>

<p>Our first tests were of sequential read and write performance straight to HDFS.  For this test we had
a cluster of 32 HDFS nodes (c5.4xlarge <a href="https://aws.amazon.com/ec2/instance-types/">AWS</a> instances), 16 Spark nodes (r5.4xlarge),
3 zookeepers (r5.xlarge), and 1 master (r5.2xlarge).</p>

<p>The first table below shows the results for writing a 1TB file.  The results are the average of three runs
for each of the directory encodings Reed-Solomon (RS) 6-3 with 64KB stripes, RS 6-3 with 1MB stripes,
RS 10-4 with 1MB stripes, and the default triple replication.  We also varied the number of concurrent
Spark executors, performing tests with 16 executors that did not stress the cluster in any area, and with
128 executors which exhausted our network bandwidth allotment of 5 Gbps. As can be seen, in the 16 executor
environment, we saw greater than a 3X bump in throughput using RS 10-4 with 1MB stripes over triple replication.
At saturation, the speed up was still over 2X, which is in line with the results from <a href="https://www.slideshare.net/HadoopSummit/debunking-the-myths-of-hdfs-erasure-coding-performance">EC Myths</a>. Also of note,
using RS 6-3 with 64KB stripes performed better than the same with 1MB stripes, which is a nice result for Accumulo,
as we’ll show later.</p>

<table>
  <thead>
    <tr>
      <th>Encoding</th>
      <th style="text-align: right">16 executors</th>
      <th style="text-align: right">128 executors</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Replication</td>
      <td style="text-align: right">2.19 GB/s</td>
      <td style="text-align: right">4.13 GB/s</td>
    </tr>
    <tr>
      <td>RS 6-3 64KB</td>
      <td style="text-align: right">6.33 GB/s</td>
      <td style="text-align: right">8.11 GB/s</td>
    </tr>
    <tr>
      <td>RS 6-3 1MB</td>
      <td style="text-align: right">6.22 GB/s</td>
      <td style="text-align: right">7.93 GB/s</td>
    </tr>
    <tr>
      <td>RS 10-4 1MB</td>
      <td style="text-align: right">7.09 GB/s</td>
      <td style="text-align: right">8.34 GB/s</td>
    </tr>
  </tbody>
</table>

<p>Our read tests are not as dramatic as those in <a href="https://www.slideshare.net/HadoopSummit/debunking-the-myths-of-hdfs-erasure-coding-performance">EC Myths</a>, but still looking good for EC.  Here we show the
results for reading back the 1TB file created in the write test using 16 Spark executors.  In addition to
the straight read tests, we also performed tests with 2 DataNodes disabled to simulate the performance hit
of failures which require data repair in the foreground.  Finally, we tested the read performance
after a background rebuild of the filesystem.  We did this to see if the foreground rebuild or
the loss of 2 DataNodes was the major contributor to any performance degradation.  As can be seen,
EC read performance is close to 2X faster than replication, even in the face of failures.</p>

<table>
  <thead>
    <tr>
      <th>Encoding</th>
      <th style="text-align: right">32 nodes<br />no failures</th>
      <th style="text-align: right">30 nodes<br />with failures</th>
      <th style="text-align: right">30 nodes<br />no failures</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Replication</td>
      <td style="text-align: right">3.95 GB/s</td>
      <td style="text-align: right">3.99 GB/s</td>
      <td style="text-align: right">3.89 GB/s</td>
    </tr>
    <tr>
      <td>RS 6-3 64KB</td>
      <td style="text-align: right">7.36 GB/s</td>
      <td style="text-align: right">7.27 GB/s</td>
      <td style="text-align: right">7.16 GB/s</td>
    </tr>
    <tr>
      <td>RS 6-3 1MB</td>
      <td style="text-align: right">6.59 GB/s</td>
      <td style="text-align: right">6.47 GB/s</td>
      <td style="text-align: right">6.53 GB/s</td>
    </tr>
    <tr>
      <td>RS 10-4 1MB</td>
      <td style="text-align: right">6.21 GB/s</td>
      <td style="text-align: right">6.08 GB/s</td>
      <td style="text-align: right">6.21 GB/s</td>
    </tr>
  </tbody>
</table>

<h3 id="accumulo-performance-with-ec">Accumulo Performance with EC</h3>

<p>While the above results are impressive, they are not representative of how Accumulo uses HDFS.  For starters,
Accumulo sequential I/O is doing far more than just reading or writing files; compression and serialization,
for example, place quite a load upon the tablet server CPUs.  An example to illustrate this is shown below.
The time in minutes to bulk-write 400 million rows to RFiles with 40 Spark executors is listed for both EC
using RS 6-3 with 1MB stripes and triple replication.  The choice of compressor has a much more profound
effect on the write times than the choice of underlying encoding for the directory being written to
(although without compression EC is much faster than replication).</p>

<table>
  <thead>
    <tr>
      <th>Compressor</th>
      <th style="text-align: right">RS 6-3 1MB</th>
      <th style="text-align: right">Replication</th>
      <th style="text-align: right">File size (GB)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>gz</td>
      <td style="text-align: right">2.7</td>
      <td style="text-align: right">2.7</td>
      <td style="text-align: right">21.3</td>
    </tr>
    <tr>
      <td>none</td>
      <td style="text-align: right">2.0</td>
      <td style="text-align: right">3.0</td>
      <td style="text-align: right">158.5</td>
    </tr>
    <tr>
      <td>snappy</td>
      <td style="text-align: right">1.6</td>
      <td style="text-align: right">1.6</td>
      <td style="text-align: right">38.4</td>
    </tr>
  </tbody>
</table>

<p>Of much more importance to Accumulo performance is read latency. A frequent use case for our group is to obtain a
number of row IDs from an index and then use a BatchScanner to read those individual rows.
In this use case, the time to access a single row is far more important than the raw I/O performance.  To test
Accumulo’s performance with EC for this use case, we did a series of tests against a 10 billion row table,
with each row consisting of 10 columns.  16 Spark executors each performed 10000 queries, where each query
sought 10 random rows.  Thus 16 million individual rows were returned in batches of 10.  For each batch of
10, the time in milliseconds was captured, and theses times were collected in a histogram of 50ms buckets, with
a catch-all bucket for queries that took over 1 second.  For this test we reconfigured our cluster to make use
of c5n.4xlarge nodes featuring must faster networking speeds (15 Gbps sustained vs 5 Gbps for
c5.4xlarge). Because these nodes are in short supply, we ran with only 16 HDFS nodes (c5n.4xlarge),
but still had 16 Spark nodes (also c5n.4xlarge).  Zookeeper and master nodes remained the same.</p>

<p>In the table below, we show the min, max, and average times in milliseconds for each batch of 10 across
four different encoding policies.  The clear winner here is replication, and the clear loser RS 10-4 with
1MB stripes, but RS 6-3 with 64KB stripes is not looking too bad.</p>

<table>
  <thead>
    <tr>
      <th>Encoding</th>
      <th style="text-align: right">Min</th>
      <th style="text-align: right">Avg</th>
      <th style="text-align: right">Max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RS 10-4 1MB</td>
      <td style="text-align: right">40</td>
      <td style="text-align: right">105</td>
      <td style="text-align: right">2148</td>
    </tr>
    <tr>
      <td>RS 6-3 1MB</td>
      <td style="text-align: right">30</td>
      <td style="text-align: right">68</td>
      <td style="text-align: right">1297</td>
    </tr>
    <tr>
      <td>RS 6-3 64KB</td>
      <td style="text-align: right">23</td>
      <td style="text-align: right">43</td>
      <td style="text-align: right">1064</td>
    </tr>
    <tr>
      <td>Replication</td>
      <td style="text-align: right">11</td>
      <td style="text-align: right">23</td>
      <td style="text-align: right">731</td>
    </tr>
  </tbody>
</table>

<p>The above results also hold in the event of errors.  The next table shows the same test, but with 2 DataNodes
disabled to simulate failures that require foreground rebuilds.  Again, replication wins, and RS 10-4 1MB
loses, but RS 6-3 64KB remains a viable option.</p>

<table>
  <thead>
    <tr>
      <th>Encoding</th>
      <th style="text-align: right">Min</th>
      <th style="text-align: right">Avg</th>
      <th style="text-align: right">Max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>RS 10-4 1MB</td>
      <td style="text-align: right">53</td>
      <td style="text-align: right">143</td>
      <td style="text-align: right">3221</td>
    </tr>
    <tr>
      <td>RS 6-3 1MB</td>
      <td style="text-align: right">34</td>
      <td style="text-align: right">113</td>
      <td style="text-align: right">1662</td>
    </tr>
    <tr>
      <td>RS 6-3 64KB</td>
      <td style="text-align: right">24</td>
      <td style="text-align: right">61</td>
      <td style="text-align: right">1402</td>
    </tr>
    <tr>
      <td>Replication</td>
      <td style="text-align: right">12</td>
      <td style="text-align: right">26</td>
      <td style="text-align: right">304</td>
    </tr>
  </tbody>
</table>

<p>The images below show a plots of the histograms.  The third plot was generated with 14 HDFS DataNodes, but after
all missing data had been repaired.  Again, this was done to see how much of the performance degradation could be
attributed to missing data, and how much to simply having less computing power available.</p>

<center>
<img src="/images/blog/201909_ec/ec-latency-16.png" width="75%" /><br /><br />

<img src="/images/blog/201909_ec/ec-latency-14e.png" width="75%" /><br /><br />

<img src="/images/blog/201909_ec/ec-latency-14.png" width="75%" />
</center>

<h3 id="conclusion">Conclusion</h3>
<p>HDFS with erasure coding has the potential to double your available Accumulo storage, at the cost of a hit in
random seek times, but a potential increase in sequential scan performance. We will be proposing some changes
to Accumulo to make working with EC a bit easier. Our initial thoughts are collected in this
Accumulo dev list <a href="https://lists.apache.org/thread.html/4ac5b0f664e15fa120e748892612f1e417b7dee3e1539669d179900c@%3Cdev.accumulo.apache.org%3E">post</a>.</p>



<p><strong>View all posts in the <a href="/news">news archive</a></strong></p>

        </div>

        
<footer>

  <p><a href="https://www.apache.org/foundation/contributing"><img src="https://www.apache.org/images/SupportApache-small.png" alt="Support the ASF" id="asf-logo" height="100" /></a></p>

  <p>Copyright © 2011-2025 <a href="https://www.apache.org">The Apache Software Foundation</a>.
Licensed under the <a href="https://www.apache.org/licenses/">Apache License, Version 2.0</a>.</p>

  <p>Apache®, the names of Apache projects and their logos, and the multicolor feather
logo are registered trademarks or trademarks of The Apache Software Foundation
in the United States and/or other countries.</p>

</footer>


      </div>
    </div>
  </div>
</body>
</html>
