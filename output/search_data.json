{
  
    "docs-2-x-administration-caching": {
      "title": "Caching",
      "content": "Accumulo tablet servers have block caches that buffer data in memory to limit reads from disk.This caching has the following benefits:  reduces latency when reading data  helps alleviate hotspots in tablesEach tablet server has an index and data block cache that is shared by all hosted tablets (see the tablet server diagramto learn more). A typical Accumulo read operation will perform a binary search over several index blocks followed by a linear scanof one or more data blocks. If these blocks are not in a cache, they will need to be retrieved from RFiles in HDFS. While the indexblock cache is enabled for all tables, the data block cache has to be enabled for a table by the user. It is typically only enabledfor tables where read performance is critical.ConfigurationThe tserver.cache.manager.class property controls which block cache implementation is used within the tablet server. Userscan supply their own implementation and set custom configuration properties to control its behavior (see org.apache.accumulo.core.spi.cache.BlockCacheManager$Configuration.java).The index and data block caches are configured for tables by the following properties:  table.cache.block.enable - enables data block cache on the table (default is false)  table.cache.index.enable - enables index block cache on the table (default is true)While the index block cache is enabled by default for all Accumulo tables, users must enable the data block cache bysetting table.cache.block.enable to true in the shell:config -t mytable -s table.cache.block.enable=trueOr programmatically using TableOperations.setProperty():client.tableOperations().setProperty(&quot;mytable&quot;, &quot;table.cache.block.enable&quot;, &quot;true&quot;);The size of the index and data block caches (which are shared by all tablets of tablet server) can be changed fromtheir defaults by setting the following properties:  tserver.cache.data.size  tserver.cache.index.size",
      "url": " /docs/2.x/administration/caching",
      "categories": "administration"
    },
  
    "docs-2-x-administration-compaction": {
      "title": "Compactions",
      "content": "In Accumulo each tablet has a list of files associated with it.  As data iswritten to Accumulo it is buffered in memory. The data buffered in memory iseventually written to files in DFS on a per-tablet basis. Files can also beadded to tablets directly by bulk import. In the background tablet servers runmajor compactions to merge multiple files into one. The tablet server has todecide which tablets to compact and which files within a tablet to compact.Within each tablet server there are one or more user configurable CompactionServices that compact tablets.  Compaction Services can be configured withone or more named queues, which may use internal executors or external executors.An internal executor will be configured with some sized threshold and number ofthreads in which to compact files. An external executor is configured only withthe name of an external queue.Each Accumulo table has a user configurable Compaction Dispatcher that decideswhich compaction services that table will use.  Accumulo generates metrics foreach compaction service which enable users to adjust compaction service settingsbased on actual activity.Each compaction service has a compaction planner that decides which files tocompact.  The default compaction planner uses the table property table.compaction.major.ratio to decide which files to compact.  Thecompaction ratio is real number &amp;gt;= 1.0.  Assume LFS is the size of the largestfile in a set, CR is the compaction ratio,  and FSS is the sum of file sizes ina set. The default planner looks for file sets where LFS*CR &amp;lt;= FSS.  By onlycompacting sets of files that meet this requirement the amount of work done bycompactions is O(N * logCR(N)).  Increasing the ratio willresult in less compaction work and more files per tablet.  More files pertablet means higher query latency. So adjusting this ratio is a trade-offbetween ingest and query performance.When CR=1.0 this will result in a goal of a single per file tablet, but theamount of work is O(N2) so 1.0 should be used with caution.  Forexample if a tablet has a 1G file and 1M file is added, then a compaction ofthe 1G and 1M file would be queued.Compaction services and dispatchers were introduced in Accumulo 2.1, so muchof this documentation only applies to Accumulo 2.1 and later.ConfigurationBelow are some Accumulo shell commands that do the following :  Create a compaction service named cs1 that has three executors.  The first executor named small has 8 threads and runs compactions less than 16M.  The second executor medium runs compactions less than 128M with 4 threads.  The last executor large runs all other compactions.  Create a compaction service named cs2 that has three executors.  It has similar config to cs1, but its executors have fewer threads. Limits total I/O of all compactions within the service to 40MB/s.  Configure table ci to use compaction service cs1 for system compactions and service cs2 for user compactions.config -s tserver.compaction.major.service.cs1.planner=org.apache.accumulo.core.spi.compaction.DefaultCompactionPlannerconfig -s &#39;tserver.compaction.major.service.cs1.planner.opts.executors=[{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;16M&quot;,&quot;numThreads&quot;:8},{&quot;name&quot;:&quot;medium&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;128M&quot;,&quot;numThreads&quot;:4},{&quot;name&quot;:&quot;large&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;numThreads&quot;:2}]&#39;config -s tserver.compaction.major.service.cs2.planner=org.apache.accumulo.core.spi.compaction.DefaultCompactionPlannerconfig -s &#39;tserver.compaction.major.service.cs2.planner.opts.executors=[{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;16M&quot;,&quot;numThreads&quot;:4},{&quot;name&quot;:&quot;medium&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;128M&quot;,&quot;numThreads&quot;:2},{&quot;name&quot;:&quot;large&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;numThreads&quot;:1}]&#39;config -s tserver.compaction.major.service.cs2.rate.limit=40Mconfig -t ci -s table.compaction.dispatcher=org.apache.accumulo.core.spi.compaction.SimpleCompactionDispatcherconfig -t ci -s table.compaction.dispatcher.opts.service=cs1config -t ci -s table.compaction.dispatcher.opts.service.user=cs2For more information see the javadoc for compaction,DefaultCompactionPlanner andSimpleCompactionDispatcherThe names of the compaction services and executors are used for logging and metrics.External CompactionsIn Accumulo 2.1 we introduced a new optional feature that allows compactions to runoutside of the Tablet Server.  External compactions introduces two new server processesin an Accumulo deployment:      Compactor: Accumulo process that runs external compactions and is started with the name of a queue for which it will perform compactions.  In a typical deployment there will be many of these processes running, some for queue A, queue B, etc.  This process will only run a single compaction at a time and will communicate with the Compaction Coordinator to get a compaction job and report its status.        Compaction Coordinator: a process that manages the compaction queues for all external compactions in the system and assigns compaction tasks to Compactors. In a typical deployment there will be one instance of this process in use at a time with a backup process waiting to become primary (much like the primary and secondary manager processes). This process communicates with the TabletServers to get external compaction job information and report back their status.  Starting the ComponentsThe CompactionCoordinator and Compactor components are started in the same manner as the other Accumulo services.To start a CompactionCoordinator:accumulo compaction-coordinator &amp;amp;To start a Compactor:accumulo compactor -q &amp;lt;queueName&amp;gt;ConfigurationConfiguration for external compactions is very similar to the internal compaction example above.In the example below we create a Compaction Service cs1 and configure it with a queuenamed DCQ1. We then define the Compaction Dispatcher on table testTable and configure thetable to use the cs1 Compaction Service for planning and executing all compactions.config -s tserver.compaction.major.service.cs1.planner=org.apache.accumulo.core.spi.compaction.DefaultCompactionPlannerconfig -s &#39;tserver.compaction.major.service.cs1.planner.opts.executors=[{&quot;name&quot;:&quot;all&quot;,&quot;type&quot;:&quot;external&quot;,&quot;queue&quot;:&quot;DCQ1&quot;}]&#39;config -t testTable -s table.compaction.dispatcher=org.apache.accumulo.core.spi.compaction.SimpleCompactionDispatcherconfig -t testTable -s table.compaction.dispatcher.opts.service=cs1Note that you can mix internal and external options, for example:config -s &#39;tserver.compaction.major.service.cs1.planner.opts.executors=[{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;16M&quot;,&quot;numThreads&quot;:8},{&quot;name&quot;:&quot;medium&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;128M&quot;,&quot;numThreads&quot;:4},{&quot;name&quot;:&quot;large&quot;,&quot;type&quot;:&quot;external&quot;,&quot;queue&quot;:&quot;LargeQ&quot;}]&#39;OverviewThe CompactionCoordinator is responsible for managing the global external compaction work queue. For each external compaction queue, the tablet server will maintain an in memory priority queue of the tablets loaded on it that require external compactions. The coordinator polls all tservers to get summary information about their external compaction queues to combine the summary information to determine which tablet server to contact next to get work.  The coordinator does not maintain per tablet information, it only maintains enough information to allow it to know which tablet server to contact next for a given queue.  The tablet server will then know what specific tablet in that queue needs to compact.When a Compactor is free to perform work, it asks the CompactionCoordinator for the next compaction job. The CompactionCoordinator contacts the next TabletServer that has the highest priority for the Compactor’s queue. The TabletServer returns the information necessary for the compaction to occur to the CompactionCoordinator, which is passed on to the Compactor. The Compaction Coordinator maintains an in-memory list of running compactions and also inserts an entry into the metadata table for the tablet to denote that an external compaction is running. When the Compactor has finished the compaction, it notifies the CompactionCoordinator which inserts an entry into the metadata table to denote that the external compaction completed and it attempts to notify the TabletServer. If successful, the TabletServer commits the major compaction. If the TabletServer is down, or the Tablet has become hosted on a different TabletServer, then the CompactionCoordinator will fail to notify the TabletServer, but the metadata table entries will remain. The major compaction will be committed in the future by the TabletServer hosting the Tablet.External compactions handle faults and major system events in Accumulo. When a compactor process dies this will be detected and any files it had reserved in a tablet will be unreserved.  When a tserver dies, this will not impact any external compactions running on behalf of tablets that tserver was hosting.  The case of tablets not being hosted on a tserver when an external compaction tries to commit is also handled.  Tablets being deleted (by split, merge, or table deletion) will cause any associated running external compactions to be canceled.  When a user initiated compaction is canceled, any external compactions running as part of that will be canceled.External Compaction in ActionBelow are some examples of log entries and metadata table entries for external compactions. First, here are some metadata entries for table 2 . You can see that there are three files of different sizes (file size and number of entries are stored in the value portion of the metadata table rows with the “file” column qualifier).2&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/A0000047.rf []   12330,990002&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/F0000048.rf []   1196,10002&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/F000004j.rf []   1302,10002&amp;lt; last:10000bf4e0a0004 []  localhost:99972&amp;lt; loc:10000bf4e0a0004 []   localhost:99972&amp;lt; srv:compact []   1112&amp;lt; srv:dir []   default_tablet2&amp;lt; srv:flush [] 1132&amp;lt; srv:lock []  tservers/localhost:9997/zlock#1950397a-b2ca-4685-b70b-67ae3cd578b9#0000000000$10000bf4e0a00042&amp;lt; srv:time []  M16183256480932&amp;lt; ~tab:~pr []  x00Below are excerpts from the TabletServer, CompactionCoordinator, Compactor logs and metadata table. I have merged the logs in time order to make it easier to see what is happening.In the logs below the Compactor requested a compaction job from the Coordinator with an ExternalCompactionId of de6afc1d-64ae-4abf-8bce-02ec0a79aa6c. The Coordinator knew that TabletServer localhost:9997 had a Tablet that needed compacting and contacted it to get the details. The CompactionManager, a componentrunning in the TabletServer, returned the information to the Coordinator. The Coordinator then updates the metadata table (below the logs) for the external compaction and returns the information to the Compactor:2021-04-13T14:54:10,580 [compactor.Compactor] INFO : Attempting to get next job, eci = ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c2021-04-13T14:54:10,580 [coordinator.CompactionCoordinator] DEBUG: getCompactionJob called for queue DCQ1 by compactor localhost:91012021-04-13T14:54:10,580 [coordinator.CompactionCoordinator] DEBUG: Found tserver localhost:9997 with priority 288230376151711747 compaction for queue DCQ12021-04-13T14:54:10,580 [coordinator.CompactionCoordinator] DEBUG: Getting compaction for queue DCQ1 from tserver localhost:99972021-04-13T14:54:10,581 [compactions.CompactionManager] DEBUG: Attempting to reserve external compaction, queue:DCQ1 priority:288230376151711747 compactor:localhost:91012021-04-13T14:54:10,596 [compactions.CompactionManager] DEBUG: Reserved external compaction ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c2021-04-13T14:54:10,596 [coordinator.CompactionCoordinator] DEBUG: Returning external job ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c to localhost:91012&amp;lt; ecomp:ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c []   {&quot;inputs&quot;:[&quot;hdfs://localhost:8020/accumulo/tables/2/default_tablet/F0000048.rf&quot;,&quot;hdfs://localhost:8020/accumulo/tables/2/default_tablet/A0000047.rf&quot;],&quot;tmp&quot;:&quot;hdfs://localhost:8020/accumulo/tables/2/default_tablet/A000004k.rf_tmp&quot;,&quot;dest&quot;:&quot;hdfs://localhost:8020/accumulo/tables/2/default_tablet/A000004k.rf&quot;,&quot;compactor&quot;:&quot;localhost:9101&quot;,&quot;kind&quot;:&quot;USER&quot;,&quot;executorId&quot;:&quot;DCQ1&quot;,&quot;priority&quot;:288230376151711747}2&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/A0000047.rf []   12330,990002&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/F0000048.rf []   1196,10002&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/F000004j.rf []   1302,10002&amp;lt; last:10000bf4e0a0004 []  localhost:99972&amp;lt; loc:10000bf4e0a0004 []   localhost:99972&amp;lt; srv:compact []   1112&amp;lt; srv:dir []   default_tablet2&amp;lt; srv:flush [] 1132&amp;lt; srv:lock []  tservers/localhost:9997/zlock#1950397a-b2ca-4685-b70b-67ae3cd578b9#0000000000$10000bf4e0a00042&amp;lt; srv:time []  M16183256480932&amp;lt; ~tab:~pr []  x00Next, the Compactor runs the compaction successfully and reports the status back to the Coordinator. The Coordinator inserts a final state marker into the metadata table (below the logs).2021-04-13T14:54:11,597 [compactor.Compactor] INFO : Received next compaction job: TExternalCompactionJob(externalCompactionId:ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c, extent:TKeyExtent(table:32, endRow:null, prevEndRow:null), files:[InputFile(metadataFileEntry:hdfs://localhost:8020/accumulo/tables/2/default_tablet/F0000048.rf, size:0, entries:0, timestamp:0), InputFile(metadataFileEntry:hdfs://localhost:8020/accumulo/tables/2/default_tablet/A0000047.rf, size:0, entries:0, timestamp:0)], priority:3, readRate:0, writeRate:0, iteratorSettings:IteratorConfig(iterators:[]), type:FULL, reason:USER, outputFile:hdfs://localhost:8020/accumulo/tables/2/default_tablet/A000004k.rf_tmp, propagateDeletes:false, kind:USER)2021-04-13T14:54:11,598 [compactor.Compactor] INFO : Starting up compaction runnable for job: TExternalCompactionJob(externalCompactionId:ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c, extent:TKeyExtent(table:32, endRow:null, prevEndRow:null), files:[InputFile(metadataFileEntry:hdfs://localhost:8020/accumulo/tables/2/default_tablet/F0000048.rf, size:0, entries:0, timestamp:0), InputFile(metadataFileEntry:hdfs://localhost:8020/accumulo/tables/2/default_tablet/A0000047.rf, size:0, entries:0, timestamp:0)], priority:3, readRate:0, writeRate:0, iteratorSettings:IteratorConfig(iterators:[]), type:FULL, reason:USER, outputFile:hdfs://localhost:8020/accumulo/tables/2/default_tablet/A000004k.rf_tmp, propagateDeletes:false, kind:USER)2021-04-13T14:54:11,599 [compactor.Compactor] INFO : CompactionCoordinator address is: localhost:91002021-04-13T14:54:11,599 [coordinator.CompactionCoordinator] INFO : Compaction status update, id: ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c, timestamp: 1618325651599, state: STARTED, message: Compaction started2021-04-13T14:54:12,601 [compactor.Compactor] INFO : Starting compactor2021-04-13T14:54:12,601 [compactor.Compactor] INFO : Progress checks will occur every 1 seconds2021-04-13T14:54:12,718 [ratelimit.SharedRateLimiterFactory] DEBUG: RateLimiter &#39;read_rate_limiter&#39;: 69,672 of 0 permits/second2021-04-13T14:54:12,718 [ratelimit.SharedRateLimiterFactory] DEBUG: RateLimiter &#39;write_rate_limiter&#39;: 45,120 of 0 permits/second2021-04-13T14:54:13,179 [compactor.Compactor] INFO : Compaction completed successfully ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c2021-04-13T14:54:13,180 [compactor.Compactor] INFO : CompactionCoordinator address is: localhost:91002021-04-13T14:54:13,181 [coordinator.CompactionCoordinator] INFO : Compaction status update, id: ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c, timestamp: 1618325653180, state: SUCCEEDED, message: Compaction completed successfully2021-04-13T14:54:14,182 [compactor.Compactor] INFO : Compaction thread finished.2021-04-13T14:54:14,182 [compactor.Compactor] INFO : Updating coordinator with compaction completion.2021-04-13T14:54:14,184 [coordinator.CompactionCoordinator] INFO : Compaction completed, id: ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c, stats: CompactionStats(entriesRead:100000, entriesWritten:100000, fileSize:12354)2021-04-13T14:54:14,185 [coordinator.CompactionFinalizer] INFO : Writing completed external compaction to metadata table: {&quot;extent&quot;:{&quot;tableId&quot;:&quot;2&quot;},&quot;state&quot;:&quot;FINISHED&quot;,&quot;fileSize&quot;:12354,&quot;entries&quot;:100000}2021-04-13T14:54:14,223 [coordinator.CompactionFinalizer] INFO : Queueing tserver notification for completed external compaction: {&quot;extent&quot;:{&quot;tableId&quot;:&quot;2&quot;},&quot;state&quot;:&quot;FINISHED&quot;,&quot;fileSize&quot;:12354,&quot;entries&quot;:100000}2021-04-13T14:54:14,290 [coordinator.CompactionFinalizer] INFO : Notifying tserver localhost:9997[10000bf4e0a0004] that compaction {&quot;extent&quot;:{&quot;tableId&quot;:&quot;2&quot;},&quot;state&quot;:&quot;FINISHED&quot;,&quot;fileSize&quot;:12354,&quot;entries&quot;:100000} has finished.2&amp;lt; ecomp:ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c []   {&quot;inputs&quot;:[&quot;hdfs://localhost:8020/accumulo/tables/2/default_tablet/F0000048.rf&quot;,&quot;hdfs://localhost:8020/accumulo/tables/2/default_tablet/A0000047.rf&quot;],&quot;tmp&quot;:&quot;hdfs://localhost:8020/accumulo/tables/2/default_tablet/A000004k.rf_tmp&quot;,&quot;dest&quot;:&quot;hdfs://localhost:8020/accumulo/tables/2/default_tablet/A000004k.rf&quot;,&quot;compactor&quot;:&quot;localhost:9101&quot;,&quot;kind&quot;:&quot;USER&quot;,&quot;executorId&quot;:&quot;DCQ1&quot;,&quot;priority&quot;:288230376151711747}2&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/A0000047.rf []   12330,990002&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/F0000048.rf []   1196,10002&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/F000004j.rf []   1302,10002&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/F000004l.rf []   841,10002&amp;lt; last:10000bf4e0a0004 []  localhost:99972&amp;lt; loc:10000bf4e0a0004 []   localhost:99972&amp;lt; srv:compact []   1112&amp;lt; srv:dir []   default_tablet2&amp;lt; srv:flush [] 1142&amp;lt; srv:lock []  tservers/localhost:9997/zlock#1950397a-b2ca-4685-b70b-67ae3cd578b9#0000000000$10000bf4e0a00042&amp;lt; srv:time []  M16183256530802&amp;lt; ~tab:~pr []  x00~ecompECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c : []    {&quot;extent&quot;:{&quot;tableId&quot;:&quot;2&quot;},&quot;state&quot;:&quot;FINISHED&quot;,&quot;fileSize&quot;:12354,&quot;entries&quot;:100000}Finally, the TabletServer commits the compaction.2021-04-13T14:54:14,290 [tablet.CompactableImpl] DEBUG: Attempting to commit external compaction ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c2021-04-13T14:54:14,325 [tablet.files] DEBUG: Compacted 2&amp;lt;&amp;lt; for USER created hdfs://localhost:8020/accumulo/tables/2/default_tablet/A000004k.rf from [A0000047.rf, F0000048.rf]2021-04-13T14:54:14,326 [tablet.CompactableImpl] DEBUG: Completed commit of external compaction ECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c2&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/A000004k.rf []   12354,1000002&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/F000004j.rf []   1302,10002&amp;lt; file:hdfs://localhost:8020/accumulo/tables/2/default_tablet/F000004l.rf []   841,10002&amp;lt; last:10000bf4e0a0004 []  localhost:99972&amp;lt; loc:10000bf4e0a0004 []   localhost:99972&amp;lt; srv:compact []   1122&amp;lt; srv:dir []   default_tablet2&amp;lt; srv:flush [] 1142&amp;lt; srv:lock []  tservers/localhost:9997/zlock#1950397a-b2ca-4685-b70b-67ae3cd578b9#0000000000$10000bf4e0a00042&amp;lt; srv:time []  M16183256530802&amp;lt; ~tab:~pr []  x00LoggingThe names of compaction services and executors are used in logging.  The logmessages below are from a tserver with the configuration above with data beingwritten to the ci table.  Also, a compaction of the table was forced from theshell.2020-06-25T16:34:31,669 [tablet.files] DEBUG: Compacting 3;667;6 on cs1.small for SYSTEM from [C00001cm.rf, C00001a7.rf, F00001db.rf] size 15 MB2020-06-25T16:34:45,165 [tablet.files] DEBUG: Compacted 3;667;6 for SYSTEM created hdfs://localhost:8020/accumulo/tables/3/t-000006f/C00001de.rf from [C00001cm.rf, C00001a7.rf, F00001db.rf]2020-06-25T16:35:01,965 [tablet.files] DEBUG: Compacting 3;667;6 on cs1.medium for SYSTEM from [C00001de.rf, A000017v.rf, F00001e7.rf] size 33 MB2020-06-25T16:35:11,686 [tablet.files] DEBUG: Compacted 3;667;6 for SYSTEM created hdfs://localhost:8020/accumulo/tables/3/t-000006f/A00001er.rf from [C00001de.rf, A000017v.rf, F00001e7.rf]2020-06-25T16:37:12,521 [tablet.files] DEBUG: Compacting 3;667;6 on cs2.medium for USER from [F00001f8.rf, A00001er.rf] size 35 MB config []2020-06-25T16:37:17,917 [tablet.files] DEBUG: Compacted 3;667;6 for USER created hdfs://localhost:8020/accumulo/tables/3/t-000006f/A00001fr.rf from [F00001f8.rf, A00001er.rf]MetricsThe numbers of major and minor compactions running and queued is visible on theAccumulo monitor page. This allows you to see if compactions are backing upand adjustments to the above settings are needed. When adjusting the number ofthreads available for compactions, consider the number of cores and other tasksrunning on the nodes.The numbers displayed on the Accumulo monitor are an aggregate of allcompaction services and executors.  Accumulo emits metrics about the number ofcompactions queued and running on each compaction executor.  Accumulo alsoemits metrics about the number of files per tablets.  These metrics can be usedto guide adjusting compaction ratios and compaction service configurations to ensuretablets do not have to many files.For example if metrics show that some compaction executors within a compactionservice are under utilized while others are over utilized, then theconfiguration for compaction service may need to be adjusted.  If the metricsshow that all compaction executors are fully utilized for long periods thenmaybe the compaction ratio on a table needs to be increased.User compactionsCompactions can be initiated manually for a table. To initiate a minorcompaction, use the flush command in the shell. To initiate a major compaction,use the compact command in the shell:user@myinstance mytable&amp;gt; compact -t mytableIf needed, the compaction can be canceled using compact --cancel -t mytable.The compact command will compact all tablets in a table to one file. Even tabletswith one file are compacted. This is useful for the case where a major compactionfilter is configured for a table. In 1.4, the ability to compact a range of a tablewas added. To use this feature specify start and stop rows for the compact command.This will only compact tablets that overlap the given row range.",
      "url": " /docs/2.x/administration/compaction",
      "categories": "administration"
    },
  
    "docs-2-x-administration-erasure-coding": {
      "title": "Erasure Coding",
      "content": "With the release of version 3.0.0, Hadoop introduced the use of Erasure Coding(EC) in HDFS.  By default HDFS achieves durability via block replication.Usually the replication count is 3, resulting in a storage overhead of 200%.Hadoop 3 introduced EC as a better way to achieve durability. EC behaves muchlike RAID 5 or 6…for k blocks of data, m blocks of parity data are generated,from which the original data can be recovered in the event of disk or nodefailures (erasures, in EC parlance).  A typical EC scheme is Reed-Solomon 6-3,where 6 data blocks produce 3 parity blocks, an overhead of only 50%.  Inaddition to doubling the available disk space, RS-6-3 is also more faulttolerant…a loss of 3 data blocks can be tolerated, whereas triple replicationcan only sustain a loss of two.To use EC with Accumulo, it is highly recommended that you first rebuild Hadoopwith support for Intel’s ISA-L library. Instructions for doing this can be foundhereImportant WarningAs notedhere,the current EC implementation does not support hflush() and hsync(). Thesefunctions are no-ops, which means that EC coded files are not guaranteed tobe written to disk after a sync or flush.  For this reason, EC should neverbe used for the Accumulo write-ahead logs.  Data loss may, and most likely will,occur. It is also recommended that tables in the accumulo namespace (root andmetadata for example) continue to use replication.EC and ThreadsDue to the striped nature of an EC encoded file, an EC enabled HDFS client is threaded.This becomes an issue when an Accumulo client or service is configured to use multiplethreads to read or write to HDFS, and becomes especially problematic when doing bulkimports. By default, Accumulo will use eight times the number of cores on the clientmachine to scan the files to be imported and map them to tablet files. Each threadcreated to scan the input files will create on the order of k threads to performparallel I/O. RS-10-4 on a 16 core machine, for instance, will spawn over a thousandthreads to perform this operation. If sufficient memory is not available, this operationwill fail without providing a meaningful error message to the user.  This particularproblem can be ameliorated by setting the bulk.threads client property to 1C (i.e.one thread per core), down from the default of 8C.  Similar care should be takenwhen setting other thread limits.HDFS ec CommandEncoding policy in HDFS is set at the directory level, with children inheritingpolicies from their parents if not explicitly set.  The encoding policy for a directorycan be manipulated via the hdfs ec command, documentedhere.The first step is to determine which policies are configured for your HDFS instance.This is done via the -listPolicies command.  The following listing shows that thereare 5 configured policies, of which only 3 (RS-10-4-1024k, RS-6-3-1024k, and RS-6-3-64k)are enabled for use.$ hdfs ec -listPoliciesErasure Coding Policies:ErasureCodingPolicy=[Name=RS-10-4-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=10, numParityUnits=4]], CellSize=1048576, Id=5], State=ENABLEDErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1], State=ENABLEDErasureCodingPolicy=[Name=RS-6-3-64k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3, options=]], CellSize=65536, Id=65], State=ENABLEDErasureCodingPolicy=[Name=RS-LEGACY-6-3-1024k, Schema=[ECSchema=[Codec=rs-legacy, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=3], State=DISABLEDErasureCodingPolicy=[Name=XOR-2-1-1024k, Schema=[ECSchema=[Codec=xor, numDataUnits=2, numParityUnits=1]], CellSize=1048576, Id=4], State=DISABLEDTo set the encoding policy for a directory, use the -setPolicy command.$ hadoop fs -mkdir foo$ hdfs ec -setPolicy -policy RS-6-3-64k -path fooSet RS-6-3-64k erasure coding policy on fooTo get the encoding policy for a directory, use the -getPolicy command.$ hdfs ec -getPolicy -path fooRS-6-3-64kNew directories created under foo will inherit the EC policy.$ hadoop fs -mkdir foo/bar$ hdfs ec -getPolicy -path foo/barRS-6-3-64kAnd changing the policy for a parent will also change its children.  The -setPolicycommand here issues a warning that existing files will not be converted. Toswitch the policy for an existing file, you must create a new file (througha copy, for instance).  For Accumulo, if you change the encoding policy fora table’s directories, you would then have to perform a major compaction onthe table to convert the table’s RFiles to the desired encoding.$ hdfs ec -setPolicy -policy RS-6-3-1024k -path fooSet RS-6-3-1024k erasure coding policy on fooWarning: setting erasure coding policy on a non-empty directory will not automatically convert existing files to RS-6-3-1024k erasure coding policy$ hdfs ec -getPolicy -path fooRS-6-3-1024k$ hdfs ec -getPolicy -path foo/barRS-6-3-1024kConfiguring EC for a New InstanceIf you wish to create a new instance with a single encoding policy for all tables,you simply need to change the encoding policy on the tables directory afterrunning accumulo init (seeQuick Start guide).  Tokeep the tables in the accumulo namespace using replication, youwould then need to manually change them back to using replication.  AssumingAccumulo is configured to use /accumulo as its root, you would do the following:$ hdfs ec -setPolicy -policy RS-6-3-64k -path /accumulo/tablesSet RS-6-3-64k erasure coding policy on /accumulo/tables$ hdfs ec -setPolicy -replicate -path /accumulo/tables/!0Set replication erasure coding policy on /accumulo/tables/!0$ hdfs ec -setPolicy -replicate -path /accumulo/tables/+rSet replication erasure coding policy on /accumulo/tables/+r$ hdfs ec -setPolicy -replicate -path /accumulo/tables/+repSet replication erasure coding policy on /accumulo/tables/+repCheck that the policies are set correctly:$ hdfs ec -getPolicy -path /accumulo/tablesRS-6-3-64k$ hdfs ec -getPolicy -path /accumulo/tables/!0The erasure coding policy of /accumulo/tables/!0 is unspecifiedAny directories subsequently created under /accumulo/tables willbe erasure coded.Configuring EC for an Existing InstanceFor an existing installation, the instructions are the same, but with thecaveat that changing the encoding policy for an existing directory will notchange the files within the directory. Converting existing tables to ECrequires a major compaction to complete the process.  For instance, toconvert test.table1 to RS-6-3-64k, you would first find the table IDvia the accumulo shell, use hdfs ec to change the encoding for thedirectory /accumulo/tables/&amp;lt;tableID&amp;gt;, and then compact the table.$ accumulo shelluser@instance&amp;gt; tables -laccumulo.metadata    =&amp;gt;        !0accumulo.replication =&amp;gt;      +repaccumulo.root        =&amp;gt;        +rtest.table1          =&amp;gt;         3test.table2          =&amp;gt;         4test.table3          =&amp;gt;         5trace                =&amp;gt;         1user@instance&amp;gt; quit$ hdfs ec -setPolicy -policy RS-6-3-64k -path /accumulo/tables/3Set RS-6-3-64k erasure coding policy on /accumulo/tables/3$ accumulo shelluser@instance&amp;gt; compact -t test.table1Defining Custom EC PoliciesHadoop by default will enable only a single EC policy, which isdetermined by the value of the dfs.namenode.ec.system.default.policyconfiguration setting.  To enable an existing policy, use the hdfs ec -enablePolicycommand.  To define custom policies, you must first edit theuser_ec_policies.xml file found in the Hadoop configuration directory,and then run the hdfs ec -addPolicies command.  For example, to addRS-6-3-64k as a policy, you first edit user_ec_policies.xml and addthe following:&amp;lt;configuration&amp;gt;&amp;lt;layoutversion&amp;gt;1&amp;lt;/layoutversion&amp;gt;&amp;lt;schemas&amp;gt;  &amp;lt;!-- schema id is only used to reference internally in this document --&amp;gt;  &amp;lt;schema id=&quot;RSk6m3&quot;&amp;gt;    &amp;lt;codec&amp;gt;rs&amp;lt;/codec&amp;gt;    &amp;lt;k&amp;gt;6&amp;lt;/k&amp;gt;    &amp;lt;m&amp;gt;3&amp;lt;/m&amp;gt;    &amp;lt;options&amp;gt; &amp;lt;/options&amp;gt;  &amp;lt;/schema&amp;gt;&amp;lt;/schemas&amp;gt;&amp;lt;policies&amp;gt;  &amp;lt;policy&amp;gt;    &amp;lt;schema&amp;gt;RSk6m3&amp;lt;/schema&amp;gt;    &amp;lt;cellsize&amp;gt;65536&amp;lt;/cellsize&amp;gt;  &amp;lt;/policy&amp;gt;&amp;lt;/policies&amp;gt;&amp;lt;/configuration&amp;gt;Here the schema “RSk6m3” defines a Reed-Solomon encoding with k=6data blocks and m=3 parity blocks.  This schema is then used to definea policy that uses RS-6-3 encoding with a stripe size of 64k.  To addthis policy:$ hdfs ec -addPolicies -policyFile /hadoop/etc/hadoop/user_ec_policies.xml2019-11-19 15:35:23,703 INFO util.ECPolicyLoader: Loading EC policy file /hadoop/etc/hadoop/user_ec_policies.xmlAdd ErasureCodingPolicy RS-6-3-64k succeed.To enable the policy:$ hdfs ec -enablePolicy -policy RS-6-3-64kErasure coding policy RS-6-3-64k is enabled",
      "url": " /docs/2.x/administration/erasure-coding",
      "categories": "administration"
    },
  
    "docs-2-x-administration-fate": {
      "title": "FATE",
      "content": "Accumulo must implement a number of distributed, multi-step operations to supportthe client API. Creating a new table is a simple example of an atomic client callwhich requires multiple steps in the implementation: get a unique table ID, configuredefault table permissions, populate information in ZooKeeper to record the table’sexistence, create directories in HDFS for the table’s data, etc. Implementing thesesteps in a way that is tolerant to node failure and other concurrent operations isvery difficult to achieve. Accumulo includes a Fault-Tolerant Executor (FATE) whichis widely used server-side to implement the client API safely and correctly.Fault-Tolerant Executor (FATE) is the implementation detail which ensures that tables increation when the Manager dies will be successfully created when another Manager process isstarted. This alleviates the need for any external tools to correct some bad state – Accumulocan undo the failure and self-heal without any external intervention.OverviewFATE consists of two primary components: a repeatable, persisted operation (REPO), a storagelayer for REPOs and an execution system to run REPOs. Accumulo uses ZooKeeper as the storagelayer for FATE and the Accumulo Manager acts as the execution system to run REPOs.The important characteristic of REPOs are that they implemented in a way that is idempotent:every operation must be able to undo or replay a partial execution of itself. Requiring theimplementation of the operation to support this functional greatly simplifies the executionof these operations. This property is also what guarantees safety in light of failure conditions.REPO StackA FATE transaction is composed of a sequence of Repeatable persisted operations (REPO).  In order to start a FATE transaction,a REPO is pushed onto a per-transaction REPO stack.  The top of the stack always contains thenext REPO the FATE transaction should execute.  When a REPO is successful it may return anotherREPO which is pushed on the stack.FATE Structure in ZooKeeperThe storage layer in ZooKeeper is organized by storing each FATE transaction in a unique path basedon the FATE transaction id. The base path for FATE transactions is:/accumulo/[INSTANCE_ID]/fate/tx_[TXID]The data stored on the transaction id node provides the current FATE transaction status (e.g. NEW, IN_PROGRESS,SUCCESS, FAILED,…)Under the transaction id node, there will be a number of REPOs and a debug node that provides additionalinformation. The debug information is added when the transaction is created and is the command class simple name.The REPOs form a stack of operations that will be performed in order and in ZooKeeper are numbered repo_0000000000to repo_# The REPO with the largest number is the top of the stack. The top of the stack is the REPO currentlyrunning or the next REPO that will start on the next execution. The REPO with the lowest number(usually repo_0000000000) is the operation that spawned the FATE operations.Sample FATE ZooKeeper paths:/accumulo/dcbf6855-8eac-4b44-a4a9-7ad39caafe9a/fate/tx_4dd46d49d60f1a17/accumulo/dcbf6855-8eac-4b44-a4a9-7ad39caafe9a/fate/tx_4dd46d49d60f1a17/debug/accumulo/dcbf6855-8eac-4b44-a4a9-7ad39caafe9a/fate/tx_4dd46d49d60f1a17/repo_0000000002/accumulo/dcbf6855-8eac-4b44-a4a9-7ad39caafe9a/fate/tx_4dd46d49d60f1a17/repo_0000000000AdministrationSometimes, it is useful to inspect the current FATE operations, both pending and executing.For example, a command that is not completing could be blocked on the execution of anotheroperation. Accumulo provides an Accumulo shell command to interact with fate.The fate admin command accepts a number of arguments for different functionality:list/print, summary, cancel, fail, delete, dump.The command for launching the fate admin command is:&amp;gt; accumulo admin fate --[option]The Accumulo admin help command option accumulo admin -h shows the expected usage information for the fate andother admin commands.List/PrintWithout any additional arguments, this command will print all operations that still exist inthe FATE store (ZooKeeper). This will include active, pending, and completed operations (completedoperations are lazily removed from the store). Each operation includes a unique “transaction ID”, thestate of the operation (e.g. NEW, SUBMITTED, IN_PROGRESS, FAILED), any locks thetransaction actively holds and any locks it is waiting to acquire.This option can also accept transaction IDs which will restrict the list of transactions shown.Summary (new in 2.1)Similar to the List/Print command, this command prints a snapshot of all operations in the FATE store (ZooKeeper).The information includes summary counts of:  Operation States (NEW, SUBMITTED, IN_PROGRESS, FAILED)  The FATE transaction commands  The current executing steps  Expanded FATE information detailsThe expanded FATE details supplement the information provided by list/print by including the running duration since theFATE was created, the names of the namespace and tables for locks the transaction holds or is waiting to acquire.(Note: depending on the operation and the step, the expanded details fields may be incomplete or unknown when thesnapshot information is gathered.)This option accepts a filter for the details section by the state of the operation(e.g. NEW, SUBMITTED, IN_PROGRESS, FAILED). The command also provides the option to output the informationformatted as json.Sample output:Report Time: 2022-07-07T11:42:02ZStatus counts:  IN_PROGRESS: 2Command counts:  CompactRange: 2Step counts:  CompactionDriver: 2Fate transactions (oldest first):Status Filters: [NONE]Running txn_id              Status      Command         Step (top)          locks held:(table id, name)             locks waiting:(table id, name)0:00:04 0c143900c230c1df    IN_PROGRESS CompactRange    CompactionDriver    held:[R:(1,ns:ns1), R:(2,t:ns1.table1)] waiting:[]0:00:03 55f59a2ae838e19e    IN_PROGRESS CompactRange    CompactionDriver    held:[R:(1,ns:ns1), R:(2,t:ns1.table1)] waiting:[]CancelThis command can be used to cancel NEW or SUBMITTED FATE transactions. This command requiresone or more transaction ids.FailThis command can be used to manually fail a FATE transaction and requires a transaction IDas an argument. Failing an operation is not a normal procedure and should only be performedby an administrator who understands the implications of why they are failing the operation.DeleteThis command requires a transaction ID and will delete any locks that the transactionholds. Like the fail command, this command should only be used in extreme circumstancesby an administrator that understands the implications of the command they are about toinvoke. It is not normal to invoke this command.DumpThis command accepts zero more transaction IDs.  If given no transaction IDs,it will dump all active transactions.  A FATE operations is compromised as asequence of REPOs.  In order to start a FATE transaction, a REPO is pushed ontoa per-transaction REPO stack.  The top of the stack always contains the nextREPO the FATE transaction should execute.  When a REPO is successful it mayreturn another REPO which is pushed on the stack.  The dump command willprint all of the REPOs on each transactions stack.  The REPOs are serialized toJSON in order to make them human-readable.",
      "url": " /docs/2.x/administration/fate",
      "categories": "administration"
    },
  
    "docs-2-x-administration-in-depth-install": {
      "title": "In-depth Installation",
      "content": "This document provides detailed instructions for installing Accumulo. For basicinstructions, see the quick start.HardwareBecause we are running essentially two or three systems simultaneously layeredacross the cluster: HDFS, Accumulo and MapReduce, it is typical for hardware toconsist of 4 to 8 cores, and 8 to 32 GB RAM. This is so each running process can haveat least one core and 2 - 4 GB each.One core running HDFS can typically keep 2 to 4 disks busy, so each machine maytypically have as little as 2 x 300GB disks and as much as 4 x 1TB or 2TB disks.It is possible to do with less than this, such as with 1u servers with 2 cores and 4GBeach, but in this case it is recommended to only run up to two processes permachine – i.e. DataNode and TabletServer or DataNode and MapReduce worker butnot all three. The constraint here is having enough available heap space for all theprocesses on a machine.NetworkAccumulo communicates via remote procedure calls over TCP/IP for both passingdata and control messages. In addition, Accumulo uses HDFS clients tocommunicate with HDFS. To achieve good ingest and query performance, sufficientnetwork bandwidth must be available between any two machines.In addition to needing access to ports associated with HDFS and ZooKeeper, Accumulo willuse the following default ports. Please make sure that they are open, or changetheir value in accumulo.properties.            Port      Description      Property Name                  4445      Shutdown Port (Accumulo MiniCluster)      n/a              9132      Accumulo Compaction Coordinator      compaction.coordinator.port.client              9133      Accumulo Compactor      compactor.port.client              9995      Accumulo HTTP monitor      monitor.port.client              9996      Accumulo Scan Server      sserver.port.client              9997      Accumulo Tablet Server      tserver.port.client              9998      Accumulo GC      gc.port.client              9999      Accumulo Manager Server      manager.port.client              12234      Accumulo Tracer      trace.port.client              42424      Accumulo Proxy Server      n/a              10001      Accumulo Manager Replication service      manager.replication.coordinator.port              10002      Accumulo TabletServer Replication service      replication.receipt.service.port      In addition, the user can provide 0 and an ephemeral port will be chosen instead. Thisephemeral port is likely to be unique and not already bound. Thus, configuring ports touse 0 instead of an explicit value, should, in most cases, work around any issues ofrunning multiple distinct Accumulo instances (or any other process which tries to use thesame default ports) on the same hardware. Finally, the *.port.client properties will workwith the port range syntax (M-N) allowing the user to specify a range of ports for theservice to attempt to bind. The ports in the range will be tried in a 1-up manner startingat the low end of the range to, and including, the high end of the range.Download TarballDownload a binary distribution of Accumulo and install it to a directory on a disk withsufficient space:cd &amp;lt;install directory&amp;gt;tar xzf accumulo-2.1.3-bin.tar.gzcd accumulo-2.1.3Repeat this step on each machine in your cluster. Typically, the same &amp;lt;install directory&amp;gt;is chosen for all machines in the cluster.There are four scripts in the bin/ directory that are used to manage Accumulo:  accumulo - Runs Accumulo command-line tools and starts Accumulo processes  accumulo-service - Runs Accumulo processes as services  accumulo-cluster - Manages Accumulo cluster on a single node or several nodes  accumulo-util - Accumulo utilities for creating configuration, native libraries, etc.These scripts will be used in the remaining instructions to configure and run Accumulo.DependenciesAccumulo requires HDFS and ZooKeeper to be configured and runningbefore starting. Password-less SSH should be configured between at least theAccumulo manager and TabletServer machines. It is also a good idea to run NetworkTime Protocol (NTP) within the cluster to ensure nodes’ clocks don’t get too out ofsync, which can cause problems with automatically timestamped data.ConfigurationThe Accumulo tarball contains a conf/ directory where Accumulo looks for configuration. If youinstalled Accumulo using downstream packaging, the conf/ could be something else like/etc/accumulo/.Before starting Accumulo, the configuration files accumulo-env.sh and accumulo.properties mustexist in conf/ and be properly configured. If you are using accumulo-cluster to launch acluster, the conf/ directory must also contain a cluster.yaml file. You can either create these filesmanually or run accumulo-cluster create-config.Logging is configured in accumulo-env.sh to use three log4j configuration files in conf/. Thefile used depends on the Accumulo command or service being run. Logging for most Accumulo services(i.e. Manager, TabletServer, Garbage Collector) is configured by log4j2-service.properties. All Accumulo commands (i.e init,shell, etc) are configured by log4j2.properties.Configure accumulo-env.shAccumulo needs to know where to find the software it depends on. Edit accumulo-env.shand specify the following:  Enter the location of Hadoop for $HADOOP_HOME  Enter the location of ZooKeeper for $ZOOKEEPER_HOME  Optionally, choose a different location for Accumulo logs using $ACCUMULO_LOG_DIRAccumulo uses HADOOP_HOME and ZOOKEEPER_HOME to locate Hadoop and Zookeeper jarsand add them to the CLASSPATH variable. If you are running a vendor-specific release of Hadoopor Zookeeper, you may need to change how your CLASSPATH is built in accumulo-env.sh. IfAccumulo has problems later on finding jars, run accumulo classpath to print Accumulo’sclasspath.You may want to change the default memory settings for Accumulo’s TabletServer which areby set in the JAVA_OPTS settings for ‘tservers’ in accumulo-env.sh. Note thesyntax is that of the Java JVM command line options. This value should be less than thephysical memory of the machines running TabletServers.There are similar options for the manager’s memory usage and the garbage collectorprocess. Reduce these if they exceed the physical RAM of your hardware andincrease them, within the bounds of the physical RAM, if a process fails because ofinsufficient memory.Note that you will be specifying the Java heap space in accumulo-env.sh. You shouldmake sure that the total heap space used for the Accumulo tserver and the HadoopDataNode and TaskTracker is less than the available memory on each worker node inthe cluster. On large clusters, it is recommended that the Accumulo manager, HadoopNameNode, secondary NameNode, and Hadoop JobTracker all be run on separatemachines to allow them to use more heap space. If you are running these on thesame machine on a small cluster, likewise make sure their heap space settings fitwithin the available memory.Native MapThe tablet server uses a data structure called a MemTable to store sorted key/valuepairs in memory when they are first received from the client. When a minor compactionoccurs, this data structure is written to HDFS. The MemTable will default to usingmemory in the JVM but a JNI version, called the native map, can be used to significantlyspeed up performance by utilizing the memory space of the native operating system. Thenative map also avoids the performance implications brought on by garbage collectionin the JVM by causing it to pause much less frequently.Building32-bit and 64-bit Linux and Mac OS X versions of the native map can be built by executingaccumulo-util build-native. If your system’s default compiler options are insufficient,you can add additional compiler options to the command line, such as options for thearchitecture. These will be passed to the Makefile in the environment variable USERFLAGS.Examples:accumulo-util build-nativeaccumulo-util build-native -m32After building the native map from the source, you will find the artifact inlib/native. Upon starting up, the tablet server will lookin this directory for the map library. If the file is renamed or moved from itstarget directory, the tablet server may not be able to find it. The system canalso locate the native maps shared library by setting LD_LIBRARY_PATH(or DYLD_LIBRARY_PATH on Mac OS X) in accumulo-env.sh.Native Maps ConfigurationAs mentioned, Accumulo will use the native libraries if they are found in the expectedlocation and tserver.memory.maps.native.enabled is set to true (which is the default).Using the native maps over JVM Maps nets a noticeable improvement in ingest rates; however,certain configuration variables are important to modify when increasing the size of thenative map.To adjust the size of the native map, modify the value of tserver.memory.maps.max. When increasingthis value, it is also important to adjust the values below:  table.compaction.minor.logs.threshold - maximum number of write-ahead log files that a tabletcan reference before they will be automatically minor compacted  tserver.walog.max.size - maximum size of a write-ahead log.The maximum size of the native maps for a server should be less than the product of the write-aheadlog maximum size and minor compaction threshold for log files:$table.compaction.minor.logs.threshold * $tserver.walog.max.size &amp;gt;= $tserver.memory.maps.maxThis formula ensures that minor compactions won’t be automatically triggered before the nativemaps can be completely saturated.Subsequently, when increasing the size of the write-ahead logs, it can also be importantto increase the HDFS block size that Accumulo uses when creating the files for the write-ahead log.This is controlled via tserver.wal.blocksize. A basic recommendation is that whentserver.walog.max.size is larger than 2GB in size, set tserver.wal.blocksize to 2GB.Increasing the block size to a value larger than 2GB can result in decreased writeperformance to the write-ahead log file which will slow ingest.Cluster SpecificationIf you are using accumulo-cluster to start a cluster, configure the following on themachine that will serve as the Accumulo manager:  Run accumulo-cluster create-config to create the cluster.yaml file.  Write the IP address or domain name of the Accumulo Manager to the manager section.  Write the IP addresses or domain name of the machines that will be TabletServers to thetserver section.Note that if using domain names rather than IP addresses, DNS must be configuredproperly for all machines participating in the cluster. DNS can be a confusing sourceof errors.Configure accumulo.propertiesSpecify appropriate values for the following properties in accumulo.properties:  instance.zookeeper.host - Enables Accumulo to find ZooKeeper. Accumulo uses ZooKeeperto coordinate settings between processes and helps finalize TabletServer failure.  instance.secret - The instance needs a secret to enable secure communication between servers.Configure your secret and make sure that the accumulo.properties file is not readable to otherusers. For alternatives to storing the instance.secret in plaintext, please read theSensitive Configuration Values section.Some settings can be modified via the Accumulo shell and take effect immediately, but some settingsrequire a process restart to take effect. See the configuration overviewdocumentation for details.Hostnames in configuration filesAccumulo has a number of configuration files which can contain references to other hosts in yournetwork. While IP addresses, short hostnames, or fully qualified domain names (FQDN) are all technically valid,it is good practice to always use FQDNs for both Accumulo and other processes in your Hadoopcluster. Failing to consistently use FQDNs can have unexpected consequences in how Accumulo usesthe FileSystem.A common way for this problem can be observed is via applications that use Bulk Ingest. The AccumuloManager coordinates moving the input files to Bulk Ingest to an Accumulo-managed directory. However,Accumulo cannot safely move files across different Hadoop FileSystems. This is problematic becauseAccumulo also cannot make reliable assertions across what is the same FileSystem which is specifiedwith different names. Naively, while 127.0.0.1:8020 might be a valid identifier for an HDFSinstance, Accumulo identifies localhost:8020 as a different HDFS instance than 127.0.0.1:8020.Deploy ConfigurationCopy accumulo-env.sh and accumulo.properties from the conf/ directory on the manager to allAccumulo tablet servers. The “host” configuration files accumulo-cluster only need to be onservers where that command is run.Sensitive Configuration ValuesAccumulo has a number of properties that can be specified via the accumulo.propertiesfile which are sensitive in nature, instance.secret and trace.token.property.passwordare two common examples. Both of these properties, if compromised, have the abilityto result in data being leaked to users who should not have access to that data.In Hadoop-2.6.0, a new CredentialProvider class was introduced which serves as a commonimplementation to abstract away the storage and retrieval of passwords from plaintextstorage in configuration files. Any Property marked with the Sensitive annotationis a candidate for use with these CredentialProviders. For versions of Hadoop which lackthese classes, the feature will just be unavailable for use.A comma separated list of CredentialProviders can be configured using the Accumulo Propertygeneral.security.credential.provider.paths. Each configured URL will be consultedwhen the Configuration object for accumulo.properties is accessed.Using a JavaKeyStoreCredentialProvider for storageOne of the implementations provided in Hadoop-2.6.0 is a Java KeyStore CredentialProvider.Each entry in the KeyStore is the Accumulo Property key name. For example, to store theinstance.secret, the following command can be used:  hadoop credential create instance.secret --provider jceks://file/etc/accumulo/conf/accumulo.jceksThe command will then prompt you to enter the secret to use and create a keystore in:  /path/to/accumulo/conf/accumulo.jceksThen, accumulo.properties must be configured to use this KeyStore as a CredentialProvider:general.security.credential.provider.paths=jceks://file/path/to/accumulo/conf/accumulo.jceksThis configuration will then transparently extract the instance.secret fromthe configured KeyStore and eliminates human readable storage of the sensitiveproperty.A KeyStore can also be stored in HDFS, which will make the KeyStore readily available toall Accumulo servers. If the local filesystem is used, be aware that each Accumulo serverwill expect the KeyStore in the same location.Client ConfigurationAccumulo clients are configured in a different way than Accumulo servers. Accumulo clientsare created using Java builder methods or a accumulo-client.propertiesfile containing client properties.Custom Table TagsAccumulo has the ability for users to add custom tags to tables. This allowsapplications to set application-level metadata about a table. These tags can beanything from a table description, administrator notes, date created, etc.This is done by naming and setting a property with a prefix table.custom.*.Currently, table properties are stored in ZooKeeper. This means that the numberand size of custom properties should be restricted on the order of 10’s of propertiesat most without any properties exceeding 1MB in size. ZooKeeper’s performance can bevery sensitive to an excessive number of nodes and the sizes of the nodes. Applicationswhich leverage the user of custom properties should take these warnings intoconsideration. There is no enforcement of these warnings via the API.Configuring the ClassLoaderAccumulo builds its Java classpath in accumulo-env.sh. This classpath can be viewed by runningaccumulo classpath.After an Accumulo application has started, it will load classes from the locations specified in thedeprecated general.classpaths property. Additionally, Accumulo will load classes from thelocations specified in the general.dynamic.classpaths property and will monitor and reload them ifthey change. The reloading feature is useful during the development and testing of iterators as newor modified iterator classes can be deployed to Accumulo without having to restart the database.Accumulo also has an alternate configuration for the classloader which will allow it to load classesfrom remote locations. This mechanism uses Apache Commons VFS which enables locations such as httpand hdfs to be used. This alternate configuration also uses the general.classpaths property in thesame manner described above. It differs in that you need to configure the general.vfs.classpathsproperty instead of the general.dynamic.classpaths property. As in the default configuration, thisalternate configuration will also monitor the vfs locations for changes and reload if necessary.ClassLoader ContextsWith the addition of the VFS based classloader, we introduced the notion of classloader contexts. Acontext is identified by a name and references a set of locations from which to load classes and canbe specified in the accumulo.properties file or added using the config command in the shell.Below is an example for specify the app1 context in the accumulo.properties file:# Application A classpath, loads jars from HDFS and local file systemgeneral.vfs.context.classpath.app1=hdfs://localhost:8020/applicationA/classpath/.*.jar,file:///opt/applicationA/lib/.*.jarThe default behavior follows the Java ClassLoader contract in that classes, if they exist, areloaded from the parent classloader first. You can override this behavior by delegating to the parentclassloader after looking in this classloader first. An example of this configuration is:general.vfs.context.classpath.app1.delegation=postTo use contexts in your application you can set the table.classpath.context on yourtables or use the setClassLoaderContext() method on Scanner and BatchScanner passing in the nameof the context, app1 in the example above. Setting the property on the table allows your minc, majc,and scan iterators to load classes from the locations defined by the context. Passing the contextname to the scanners allows you to override the table setting to load only scan time iterators froma different location.InitializationAccumulo must be initialized to create the structures it uses internally to locatedata across the cluster. HDFS is required to be configured and running beforeAccumulo can be initialized.Once HDFS is started, initialization can be performed by executing accumulo init. This script willprompt for a name for this instance of Accumulo. The instance name is used to identify a set oftables and instance-specific settings. The script will then write some information into HDFS soAccumulo can start properly.The initialization script will prompt you to set a root password. Once Accumulo is initialized itcan be started.RunningStarting AccumuloMake sure Hadoop is configured on all of the machines in the cluster, including access to a sharedHDFS instance. Make sure HDFS and ZooKeeper are running. Make sure ZooKeeper is configured andrunning on at least one machine in the cluster. Start Accumulo using accumulo-cluster start.To verify that Accumulo is running, check the Accumulo monitor. In addition, the Shellcan provide some information about the status of tables via reading the metadata tables.Stopping AccumuloTo shutdown cleanly, run accumulo-cluster stop and the manager will orchestrate theshutdown of all the tablet servers. Shutdown waits for all minor compactions to finish, so it maytake some time for particular configurations.Adding a Tablet ServerUpdate your conf/cluster.yaml file to account for the addition.Next, ssh to each of the hosts you want to add and run:accumulo-service tserver startMake sure the host in question has the new configuration, or else the tabletserver won’t start; at a minimum this needs to be on the host(s) being added,but in practice it’s good to ensure consistent configuration across all nodes.Decommissioning a Tablet ServerIf you need to take a node out of operation, you can trigger a graceful shutdown of a tabletserver. Accumulo will automatically rebalance the tablets across the available tablet servers.accumulo admin stop &amp;lt;host(s)&amp;gt; {&amp;lt;host&amp;gt; ...}Alternatively, you can ssh to each of the hosts you want to remove and run:accumulo-service tserver stopBe sure to update your conf/cluster.yaml file to account for the removal of these hosts. Bear in mindthat the monitor will not re-read the tservers file automatically, so it will report thedecommissioned servers as down; it’s recommended that you restart the monitor so that the node listis up to date.The steps described to decommission a node can also be used (without removal of the host from theconf/cluster.yaml file) to gracefully stop a node. This will ensure that the tabletserver is cleanlystopped and recovery will not need to be performed when the tablets are re-hosted.Restarting process on a nodeOccasionally, it might be necessary to restart the processes on a specific node. In additionto the accumulo-cluster script, Accumulo has a accumulo-service script thatcan be used to start/stop processes on a node.A note on rolling restartsFor sufficiently large Accumulo clusters, restarting multiple TabletServers within a short windowcan place significant load on the Manager server. If slightly lower availability is acceptable, thisload can be reduced by globally setting table.suspend.duration to a positive value.With table.suspend.duration set to, say, 5m, Accumulo will wait for 5 minutes for any deadTabletServer to return before reassigning that TabletServer’s responsibilities to otherTabletServers. If the TabletServer returns to the cluster before the specified timeout has elapsed,Accumulo will assign the TabletServer its original responsibilities.Tablet Status: Normally tablets will be in a HOSTED state. When a tserver goes off-line, the tabletsassigned will transition to UNASSIGNED until they are reassigned by the Manager process to anotheractive tserver.  With the table.suspend.duration set to &amp;gt; 0, a tablet will go from HOSTED toSUSPENDED when the tserver goes offline. The tablets will stay SUSPENDED until the tserver comesback online or, if the table.suspend.duration has passed. If the table.suspend.duration has passedbefore the tserver has returned, it will then become UNASSIGNED and eligible for reassignmentby the Manager. If a tablet is UNASSIGNED it will not enter the SUSPENDED state.It is important not to choose too large a value for table.suspend.duration, as during this time,all scans against the data that TabletServer had hosted will block (or time out).Running multiple TabletServers on a single nodeWith very powerful nodes, it may be beneficial to run more than one TabletServer on a givennode. This decision should be made carefully and with much deliberation as Accumulo is designedto be able to scale to using 10’s of GB of RAM and 10’s of CPU cores.Accumulo TabletServers bind certain ports on the host to accommodate remote procedure calls to/fromother nodes. Running more than one TabletServer on a host requires that you set the environmentvariable ACCUMULO_SERVICE_INSTANCE to an instance number (i.e 1, 2) for each instance that isstarted. Also, set the these properties in accumulo.properties:  tserver.port.search = true  replication.receipt.service.port = 0In order to start multiple TabletServers on a node, the accumulo command must be used:ACCUMULO_SERVICE_INSTANCE=1 ./bin/accumulo tserver &amp;amp;&amp;gt; ./logs/tserver1.out &amp;amp;ACCUMULO_SERVICE_INSTANCE=2 ./bin/accumulo tserver &amp;amp;&amp;gt; ./logs/tserver2.out &amp;amp;Running multiple TabletServers per node in Accumulo 2.1.0 and laterStarting with Accumulo 2.1.0, the accumulo-cluster script can be used along with environmentvariable NUM_TSERVERS as a convenient alternative to the accumulo command to start / stopmultiple TabletServers per node. For example, the following commands can be used to start / stop2 TabletServers on the current node:NUM_TSERVERS=2 ./bin/accumulo-cluster start-hereNUM_TSERVERS=2 ./bin/accumulo-cluster stop-hereTo start / stop the entire Accumulo cluster with 2 TabletServers per worker node, use:NUM_TSERVERS=2 ./bin/accumulo-cluster startNUM_TSERVERS=2 ./bin/accumulo-cluster stopOther commands like accumulo-cluster start-tservers and accumulo-cluster stop-tservers supportthe use of NUM_TSERVERS to specify the number of TabletServers per worker node.When accumulo-cluster is used along with NUM_TSERVERS greater than 1, the resultant log filesand redirected stdout / stderr files for each TabletServer running on the node have the instancenumber as part of their respective filenames.Lastly, starting with Accumulo 2.1.0 the accumulo-env.sh script ensures that Accumulo metricsare correctly associated with the respective instance number for each TabletServer on a node.LoggingAccumulo processes each write to a set of log files. By default, these logs are found at directoryset by ACCUMULO_LOG_DIR in accumulo-env.sh.Audit LoggingAccumulo logs many user-initiated actions, and whether they succeeded or failed, to an slf4j loggernamed org.apache.accumulo.audit. This logger can be configured in the user’s logging framework(such as log4j or logback). In the tarball, the configuration file conf/log4j2-service.propertiesdemonstrates basic audit logging with example configuration options for log4j.RecoveryIn the event of TabletServer failure or error on shutting Accumulo down, somemutations may not have been minor compacted to HDFS properly. In this case,Accumulo will automatically reapply such mutations from the write-ahead logeither when the tablets from the failed server are reassigned by the Manager (in thecase of a single TabletServer failure) or the next time Accumulo starts (in the event offailure during shutdown).Recovery is performed by asking a tablet server to sort the logs so that tablets can easily findtheir missing updates. The sort status of each file is displayed on Accumulo monitor status page.Once the recovery is complete any tablets involved should return to an online state. Until thenthose tablets will be unavailable to clients.The Accumulo client library is configured to retry failed mutations and in manycases clients will be able to continue processing after the recovery process withoutthrowing an exception.Migrating Accumulo from non-HA Namenode to HA NamenodeThe following steps will allow a non-HA instance to be migrated to an HA instance. Consider an HDFSURL hdfs://namenode.example.com:8020 which is going to be moved to hdfs://nameservice1.Before moving HDFS over to the HA namenode, use accumulo admin volumes to confirmthat the only volume displayed is the volume from the current namenode’s HDFS URL.Listing volumes referenced in zookeeper    Volume : hdfs://namenode.example.com:8020/accumuloListing volumes referenced in accumulo.root tablets section    Volume : hdfs://namenode.example.com:8020/accumuloListing volumes referenced in accumulo.root deletes section (volume replacement occurs at deletion time)Listing volumes referenced in accumulo.metadata tablets section    Volume : hdfs://namenode.example.com:8020/accumuloListing volumes referenced in accumulo.metadata deletes section (volume replacement occurs at deletion time)After verifying the current volume is correct, shut down the cluster and transition HDFS to the HAnameservice.Edit accumulo.properties to notify accumulo that a volume is being replaced. First, add the newnameservice volume to the instance.volumes property. Next, add the instance.volumes.replacementsproperty in the form of old new. It’s important to not include the volume that’s being replaced ininstance.volumes, otherwise it’s possible accumulo could continue to write to the volume.# instance.dfs.uri and instance.dfs.dir should not be setinstance.volumes=hdfs://nameservice1/accumuloinstance.volumes.replacements=hdfs://namenode.example.com:8020/accumulo hdfs://nameservice1/accumuloRun accumulo init --add-volumes and start up the accumulo cluster. Verify that thenew nameservice volume shows up with accumulo admin volumes.Listing volumes referenced in zookeeper    Volume : hdfs://namenode.example.com:8020/accumulo    Volume : hdfs://nameservice1/accumuloListing volumes referenced in accumulo.root tablets section    Volume : hdfs://namenode.example.com:8020/accumulo    Volume : hdfs://nameservice1/accumuloListing volumes referenced in accumulo.root deletes section (volume replacement occurs at deletion time)Listing volumes referenced in accumulo.metadata tablets section    Volume : hdfs://namenode.example.com:8020/accumulo    Volume : hdfs://nameservice1/accumuloListing volumes referenced in accumulo.metadata deletes section (volume replacement occurs at deletion time)Some erroneous GarbageCollector messages may still be seen for a small period while data istransitioning to the new volumes. This is expected and can usually be ignored.Achieving Stability in a VM EnvironmentFor testing, demonstration, and even operation uses, Accumulo is ofteninstalled and run in a virtual machine (VM) environment. The majority oflong-term operational uses of Accumulo are on bare-metal cluster. However, thecore design of Accumulo and its dependencies do not preclude running stably forlong periods within a VM. Many of Accumulo’s operational robustness features tohandle failures like periodic network partitioning in a large cluster carryover well to VM environments. This guide covers general recommendations formaximizing stability in a VM environment, including some of the common failuremodes that are more common when running in VMs.Known failure modes: Setup and TroubleshootingIn addition to the general failure modes of running Accumulo, VMs can introduce acouple of environmental challenges that can affect process stability. Clockdrift is something that is more common in VMs, especially when VMs aresuspended and resumed. Clock drift can cause Accumulo servers to assume thatthey have lost connectivity to the other Accumulo processes and/or lose theirlocks in Zookeeper. VM environments also frequently have constrained resources,such as CPU, RAM, network, and disk throughput and capacity. Accumulo generallydeals well with constrained resources from a stability perspective (optimizingperformance will require additional tuning, which is not covered in thissection), however there are some limits.Physical MemoryOne of those limits has to do with the Linux out of memory killer. A commonfailure mode in VM environments (and in some bare metal installations) is whenthe Linux out of memory killer decides to kill processes in order to avoid akernel panic when provisioning a memory page. This often happens in VMs due tothe large number of processes that must run in a small memory footprint. Inaddition to the Linux core processes, a single-node Accumulo setup requires aHadoop Namenode, a Hadoop Secondary Namenode a Hadoop Datanode, a Zookeeperserver, an Accumulo Manager, an Accumulo GC and an Accumulo TabletServer.Typical setups also include an Accumulo Monitor, an Accumulo Tracer, a HadoopResourceManager, a Hadoop NodeManager, provisioning software, and clientapplications. Between all of these processes, it is not uncommon toover-subscribe the available RAM in a VM. We recommend setting up VMs withoutswap enabled, so rather than performance grinding to a halt when physicalmemory is exhausted the kernel will randomly select processes to kill in orderto free up memory.Calculating the maximum possible memory usage is essential in creating a stableAccumulo VM setup. Safely engineering memory allocation for stability is amatter of then bringing the calculated maximum memory usage under the physicalmemory by a healthy margin. The margin is to account for operating system-leveloperations, such as managing process, maintaining virtual memory pages, andfile system caching. When the java out-of-memory killer finds your process, youwill probably only see evidence of that in /var/log/messages. Out-of-memoryprocess kills do not show up in Accumulo or Hadoop logs.To calculate the max memory usage of all java virtual machine (JVM) processesadd the maximum heap size (often limited by a -Xmx… argument, such as inaccumulo.properties) and the off-heap memory usage. Off-heap memory usageincludes the following:  “Permanent Space”, where the JVM stores Classes, Methods, and other code elements. This can belimited by a JVM flag such as -XX:MaxPermSize:100m, and is typically tens of megabytes.  Code generation space, where the JVM stores just-in-time compiled code. This is typically smallenough to ignore  Socket buffers, where the JVM stores send and receive buffers for each socket.  Thread stacks, where the JVM allocates memory to manage each thread.  Direct memory space and JNI code, where applications can allocate memory outside of theJVM-managed space. For Accumulo, this includes the native in-memory maps that are allocated withthe memory.maps.max parameter in accumulo.properties.  Garbage collection space, where the JVM stores information used for garbage collection.You can assume that each Hadoop and Accumulo process will use ~100-150MB forOff-heap memory, plus the in-memory map of the Accumulo TServer process. Asimple calculation for physical memory requirements follows:Physical memory needed  = (per-process off-heap memory) + (heap memory) + (other processes) + (margin)  = (number of java processes * 150M + native map) + (sum of -Xmx settings for java process)      + (total applications memory, provisioning memory, etc.) + (1G)  = (11*150M +500M) + (1G +1G +1G +256M +1G +256M +512M +512M +512M +512M +512M) + (2G) + (1G)  = (2150M) + (7G) + (2G) + (1G)  = ~12GBThese calculations can add up quickly with the large number of processes,especially in constrained VM environments. To reduce the physical memoryrequirements, it is a good idea to reduce maximum heap limits and turn offunnecessary processes. If you’re not using YARN in your application, you canturn off the ResourceManager and NodeManager. If you’re not expecting tore-provision the cluster frequently you can turn off or reduce provisioningprocesses such as Salt Stack minions and managers.Disk SpaceDisk space is primarily used for two operations: storing data and storing logs.While Accumulo generally stores all of its key/value data in HDFS, Accumulo,Hadoop, and Zookeeper all store a significant amount of logs in a directory ona local file system. Care should be taken to make sure that (a) limitations tothe amount of logs generated are in place, and (b) enough space is available tohost the generated logs on the partitions that they are assigned. When space isnot available to log, processes will hang. This can cause interruptions inavailability of Accumulo, as well as cascade into failures of variousprocesses.Hadoop, Accumulo, and Zookeeper use log4j as a logging mechanism, and each ofthem has a way of limiting the logs and directing them to a particulardirectory. Logs are generated independently for each process, so whenconsidering the total space you need to add up the maximum logs generated byeach process. Typically, a rolling log setup in which each process can generatesomething like 10 100MB files is instituted, resulting in a maximum file systemusage of 1GB per process. Default setups for Hadoop and Zookeeper are oftenunbounded, so it is important to set these limits in the logging configurationfiles for each subsystem. Consult the user manual for each system forinstructions on how to limit generated logs.Zookeeper InteractionAccumulo is designed to scale up to thousands of nodes. At that scale,intermittent interruptions in network service and other rare failures ofcompute nodes become more common. To limit the impact of node failures onoverall service availability, Accumulo uses a heartbeat monitoring system thatleverages Zookeeper’s ephemeral locks. There are several conditions that canoccur that cause Accumulo process to lose their Zookeeper locks, some of whichare true interruptions to availability and some of which are false positives.Several of these conditions become more common in VM environments, where theycan be exacerbated by resource constraints and clock drift.Tested VersionsEach release of Accumulo is built with a specific version of ApacheHadoop, Apache ZooKeeper and Apache Thrift. We expect Accumulo towork with versions that are API compatible with those versions.However, this compatibility is not guaranteed because Hadoop, ZooKeeperand Thrift may not provide guarantees between their own versions. Wehave also found that certain versions of Accumulo and Hadoop includedbugs that greatly affected overall stability. Thrift is particularlyprone to compatibility changes between versions, and you must use thesame version your Accumulo is built with.Please check the release notes for your Accumulo version or use themailing lists for more info.",
      "url": " /docs/2.x/administration/in-depth-install",
      "categories": "administration"
    },
  
    "docs-2-x-administration-monitoring-metrics": {
      "title": "Monitoring &amp; Metrics",
      "content": "MonitoringAccumulo MonitorThe Accumulo Monitor provides a web UI with information on the health and status of Accumulo.The monitor can be viewed at:  http://localhost:9995 - if Accumulo is running locally (for development)  http://&amp;lt;MONITOR_HOST&amp;gt;:9995/ - if Accumulo is running on a clusterThe Overview page (shown below) contains some summary information about the Accumulo instance and graphsdisplaying various metrics over time. These include ingest and scan performance and other useful measurements.The Manager Server, Tablet Servers, and Tables pages display metrics grouped in different ways (e.g. by tablet server or by table).Metrics typically include number of entries (key/value pairs), ingest and query rates.The number of running scans, major and minor compactions are in the form number_running (number_queued).Another important metric is hold time, which is the amount of time a tablet has been waiting but unable to flush its memory in a minor compaction.The Server Activity page graphically displays tablet server status, with each server represented as a circle or square.Different metrics may be assigned to the nodes’ color and speed of oscillation.The Overall Avg metric is only used on the Server Activity page, and represents the average of all the other metrics (after normalization).Similarly, the Overall Max metric picks the metric with the maximum normalized value.The Garbage Collector page displays a list of garbage collection cycles, the number of files found of each type (including deletion candidates in use and files actually deleted), and the length of the deletion cycle.The Traces page displays data for recent traces performed (see the following section for information on tracing).The Recent Logs page displays warning and error logs forwarded to the monitor from all Accumulo processes.Also, the XML and JSON links provide metrics in XML and JSON formats, respectively.The Accumulo monitor does a best-effort to not display any sensitive information to users; however,the monitor is intended to be a tool used with care. It is not a production-grade webservice. It isa good idea to whitelist access to the monitor via an authentication proxy or firewall. Itis strongly recommended that the Monitor is not exposed to any publicly-accessible networks.Things highlighted in yellow may be in need of attention.If anything is highlighted in red on the monitor page, it is something that definitely needs attention.SSLSSL may be enabled for the monitor page by setting the following properties in the accumulo.properties file:  monitor.ssl.keyStore  monitor.ssl.keyStorePassword  monitor.ssl.trustStore  monitor.ssl.trustStorePasswordIf the Accumulo conf directory has been configured (in particular the accumulo-env.sh file must be set up), theaccumulo-util gen-monitor-cert command can be used to create the keystore and truststore files with random passwords. The commandwill print out the properties that need to be added to the accumulo.properties file. The stores can also be generated manually with theJava keytool command, whose usage can be seen in the accumulo-util script.If desired, the SSL ciphers allowed for connections can be controlled via the following properties in accumulo.properties:  monitor.ssl.include.ciphers  monitor.ssl.exclude.ciphersIf SSL is enabled, the monitor URL can only be accessed via https.This also allows you to access the Accumulo shell through the monitor page.The left navigation bar will have a new link to Shell.An Accumulo user name and password must be entered for access to the shell.MetricsAccumulo can emit metrics using the Micrometer library. Support for the Hadoop Metrics2 framework was removed in version 2.1.0.ConfigurationMicrometer supports sending metrics to multiple monitoring systems. A Metrics sink in Micrometer is called aMeter Registry. To enable this feature you need to set the property general.micrometer.enabled to true andoptionally set general.micrometer.jvm.metrics.enabled to true to include jvm metrics. Accumulo provides amechanism for the user to specify which Meter Registry it should use with the property general.micrometer.factory.The value for this property should be the name of a class that implementsMeterRegistryFactory.Each server process should have log messages from the org.apache.accumulo.core.metrics.MetricsUtil class that detailswhether or not metrics are enabled and which MeterRegistryFactory class has been configured. Be sure to check theAccumulo processes log files when debugging missing metrics output.Metric NamesSee the javadoc for MetricsProducer for a list of metric names that will beemitted and a mapping to their prior names when Accumulo was using Hadoop Metrics2.",
      "url": " /docs/2.x/administration/monitoring-metrics",
      "categories": "administration"
    },
  
    "docs-2-x-administration-multivolume": {
      "title": "Multi-Volume Installations",
      "content": "This is an advanced configuration setting for very large clustersunder a lot of write pressure.The HDFS NameNode holds all of the metadata about the files inHDFS. For fast performance, all of this information needs to be storedin memory.  A single NameNode with 64G of memory can store themetadata for tens of millions of files. However, when scaling beyond athousand nodes, an active Accumulo system can generate lots of updatesto the file system, especially when data is being ingested.  The largenumber of write transactions to the NameNode, and the speed of asingle edit log, can become the limiting factor for large scaleAccumulo installations.You can see the effect of slow write transactions when the AccumuloGarbage Collector takes a long time (more than 5 minutes) to deletethe files Accumulo no longer needs.  If your Garbage Collectorroutinely runs in less than a minute, the NameNode is performing well.However, if you do begin to experience slow-down and poor GCperformance, Accumulo can be configured to use multiple NameNodeservers.  The configuration instance.volumes should be set to acomma-separated list, using full URI references to different NameNodeservers:instance.volumes=hdfs://ns1:9001,hdfs://ns2:9001The introduction of multiple volume support in 1.6 changed the way Accumulostores pointers to files.  It now stores fully qualified URI references tofiles.  Before 1.6, Accumulo stored paths that were relative to a tabledirectory.   After an upgrade these relative paths will still exist and areresolved using instance.dfs.dir, instance.dfs.uri, and Hadoop configuration inthe same way they were before 1.6.If the URI for a namenode changes (e.g. namenode was running on host1 and itsmoved to host2), then Accumulo will no longer function.  Even if Hadoop andAccumulo configurations are changed, the fully qualified URIs stored inAccumulo will still contain the old URI.  To handle this Accumulo has theconfiguration property instance.volumes.replacements for replacing URI storedin its metadata.  The example configuration below will replace ns1 with nsA andns2 with nsB in Accumulo metadata. For this property to take affect, Accumulo willneed to be restarted.instance.volumes.replacements=hdfs://ns1:9001 hdfs://nsA:9001, hdfs://ns2:9001 hdfs://nsB:9001Using viewfs or HA namenode, introduced in Hadoop 2, offers another option formanaging the fully qualified URIs stored in Accumulo.  Viewfs and HA namenodeboth introduce a level of indirection in the Hadoop configuration.   Forexample assume viewfs:///nn1 maps to hdfs://nn1 in the Hadoop configuration.If viewfs://nn1 is used by Accumulo, then it’s easy to map viewfs://nn1 tohdfs://nnA by changing the Hadoop configuration w/o doing anything to Accumulo.A production system should probably use a HA namenode.  Viewfs may be useful ona test system with a single non HA namenode.You may also want to configure your cluster to use Federation,available in Hadoop 2.0, which allows DataNodes to respond to multipleNameNode servers, so you do not have to partition your DataNodes byNameNode.",
      "url": " /docs/2.x/administration/multivolume",
      "categories": "administration"
    },
  
    "docs-2-x-administration-replication": {
      "title": "Replication",
      "content": "OverviewReplication is a feature of Accumulo which provides a mechanism to automaticallycopy data to other systems, typically for the purpose of disaster recovery,high availability, or geographic locality. It is best to consider this featureas a framework for automatic replication instead of the ability to copy datafrom to another Accumulo instance as copying to another Accumulo cluster isonly an implementation detail. The local Accumulo cluster is hereby referredto as the primary while systems being replicated to are known aspeers.This replication framework makes two Accumulo instances, where one instancereplicates to another, eventually consistent between one another, as opposedto the strong consistency that each single Accumulo instance still holds. Thatis to say, attempts to read data from a table on a peer which has pending replicationfrom the primary will not wait for that data to be replicated before running the scan.This is desirable for a number of reasons, the most important is that the replicationframework is not limited by network outages or offline peers, but only by the HDFSspace available on the primary system.Replication configurations can be considered as a directed graph which allows cycles.The systems in which data was replicated from is maintained in each Mutation whichallow each system to determine if a peer already has the data in whichthe system wants to send.Data is replicated by using the Write-Ahead logs (WAL) that each TabletServer isalready maintaining. TabletServers record which WALs have data that need to bereplicated to the accumulo.metadata table. The Manager uses these records,combined with the local Accumulo table that the WAL was used with, to create recordsin the replication table which track which peers the given WAL should bereplicated to. The Manager latter uses these work entries to assign the actualreplication task to a local TabletServer using ZooKeeper. A TabletServer will geta lock in ZooKeeper for the replication of this file to a peer, and proceed toreplicate to the peer, recording progress in the replication table asdata is successfully replicated on the peer. Later, the Manager and Garbage Collectorwill remove records from the accumulo.metadata and replication tablesand files from HDFS, respectively, after replication to all peers is complete.ConfigurationConfiguration of Accumulo to replicate data to another system can be categorizedinto the following sections.Site ConfigurationEach system involved in replication (even the primary) needs a name that uniquelyidentifies it across all peers in the replication graph. This should be consideredfixed for an instance, and set using replication.name in accumulo.properties.# Unique name for this system used by replicationreplication.name=primaryInstance ConfigurationFor each peer of this system, Accumulo needs to know the name of that peer,the class used to replicate data to that system and some configuration informationto connect to this remote peer. In the case of Accumulo, this additional datais the Accumulo instance name and ZooKeeper quorum; however, this varies on thereplication implementation for the peer.These can be set in accumulo.properties to ease deployments; however, as they maychange, it can be useful to set this information using the Accumulo shell.To configure a peer with the name peer1 which is an Accumulo system with an instance name of accumulo_peerand a ZooKeeper quorum of 10.0.0.1,10.0.2.1,10.0.3.1, invoke the followingcommand in the shell.root@accumulo_primary&amp;gt; config -sreplication.peer.peer1=org.apache.accumulo.tserver.replication.AccumuloReplicaSystem,accumulo_peer,10.0.0.1,10.0.2.1,10.0.3.1Since this is an Accumulo system, we also want to set a username and passwordto use when authenticating with this peer. On our peer, we make a special userwhich has permission to write to the tables we want to replicate data into, “replication”with a password of “password”. We then need to record this in the primary’s configuration.root@accumulo_primary&amp;gt; config -s replication.peer.user.peer1=replicationroot@accumulo_primary&amp;gt; config -s replication.peer.password.peer1=passwordAlternatively, when configuring replication on Accumulo running Kerberos, a keytabfile per peer can be configured instead of a password. The provided keytabs must be readableby the unix user running Accumulo. They keytab for a peer can be unique from thekeytab used by Accumulo or any keytabs for other peers.accumulo@EXAMPLE.COM@accumulo_primary&amp;gt; config -s replication.peer.user.peer1=replication@EXAMPLE.COMaccumulo@EXAMPLE.COM@accumulo_primary&amp;gt; config -s replication.peer.keytab.peer1=/path/to/replication.keytabTable ConfigurationNow, we presently have a peer defined, so we just need to configure which tables willreplicate to that peer. We also need to configure an identifier to determine wherethis data will be replicated on the peer. Since we’re replicating to another Accumulocluster, this is a table ID. In this example, we want to enable replication onmy_table and configure our peer accumulo_peer as a target, sendingthe data to the table with an ID of 2 in accumulo_peer.root@accumulo_primary&amp;gt; config -t my_table -s table.replication=trueroot@accumulo_primary&amp;gt; config -t my_table -s table.replication.target.accumulo_peer=2To replicate a single table on the primary to multiple peers, the second commandin the above shell snippet can be issued, for each peer and remote identifier pair.MonitoringBasic information about replication status from a primary can be found on the AccumuloMonitor server, using the Replication link the sidebar.On this page, information is broken down into the following sections:  Files pending replication by peer and target  Files queued for replication, with progress madeWork AssignmentDepending on the schema of a table, different implementations of the WorkAssignerused could be configured. The implementation is controlled via the property replication.work.assignerand the full class name for the implementation. This can be configured via the shell or accumulo.properties.Two implementations of WorkAssigner are provided:      The UnorderedWorkAssigner can be used to overcome the limitationof only a single WAL being replicated to a target and peer at any time. Depending on the table schema,it’s possible that multiple versions of the same Key with different values are infrequent or nonexistent.In this case, parallel replication to a peer and target is possible without any downsides. In the casewhere this implementation is used were column updates are frequent, it is possible that there will bean inconsistency between the primary and the peer.        The SequentialWorkAssigner is configured for aninstance. The SequentialWorkAssigner ensures that, per peer and each remote identifier, each WAL isreplicated in the order in which they were created. This is sufficient to ensure that updates to a tablewill be replayed in the correct order on the peer. This implementation has the downside of only replicatinga single WAL at a time.  ReplicaSystemsReplicaSystem is the interface which allows abstraction of replication of datato peers of various types. Presently, only an AccumuloReplicaSystem is providedwhich will replicate data to another Accumulo instance. A ReplicaSystem implementationis run inside of the TabletServer process, and can be configured as mentioned in Instance Configurationsection of this document. Theoretically, an implementation of this interface could send data to other filesystems, databases, etc.AccumuloReplicaSystemThe AccumuloReplicaSystem uses Thrift to communicate with a peer Accumulo instanceand replicate the necessary data. The TabletServer running on the primary will communicatewith the Manager on the peer to request the address of a TabletServer on the peer whichthis TabletServer will use to replicate the data.The TabletServer on the primary will then replicate data in batches of a configurablesize (replication.max.unit.size). The TabletServer on the peer will report how manyrecords were applied back to the primary, which will be used to record how many recordswere successfully replicated. The TabletServer on the primary will continue to replicatedata in these batches until no more data can be read from the file.Other ConfigurationThere are a number of configuration values that can be used to control howthe implementation of various components operate.  replication.max.work.queue - Maximum number of files queued for replication at one time  replication.work.assignment.sleep - Time between invocations of the WorkAssigner  replication.worker.threads - Size of threadpool used to replicate data to peers  replication.receipt.service.port - Thrift service port to listen for replication requests, can use ‘0’ for a random port  replication.work.attempts - Number of attempts to replicate to a peer before aborting the attempt  replication.receiver.min.threads - Minimum number of idle threads for handling incoming replication  replication.receiver.threadcheck.time - Time between attempting adjustments of thread pool for incoming replications  replication.max.unit.size - Maximum amount of data to be replicated in one RPC  replication.work.assigner - WorkAssigner implementation  tserver.replication.batchwriter.replayer.memory - Size of BatchWriter cache to use in applying replication requestsExample Practical ConfigurationA real-life example is now provided to give concrete application of replication configuration. Thisexample is a two instance Accumulo system, one primary system and one peer system. They are calledprimary and peer, respectively. Each system also have a table of the same name, my_table. The instancename for each is also the same (primary and peer), and both have ZooKeeper hosts on a node with a hostnamewith that name as well (primary:2181 and peer:2181).We want to configure these systems so that my_table on primary replicates to my_table on peer.accumulo.propertiesWe can assign the “unique” name that identifies this Accumulo instance among all others that might participatein replication together. In this example, we will use the names provided in the description.Primaryreplication.name=primaryPeerreplication.name=peermanagers and tservers filesBe sure to use non-local IP addresses. Other nodes need to connect to it and using localhost will likely result ina local node talking to another local node.Start both instancesThe rest of the configuration is dynamic and is best configured on the fly (in ZooKeeper) than in accumulo.properties.PeerThe next series of command are to be run on the peer system. Create a user account for the primary instance called“peer”. The password for this account will need to be saved in the configuration on the primaryroot@peer&amp;gt; createtable my_tableroot@peer&amp;gt; createuser peerroot@peer&amp;gt; grant -t my_table -u peer Table.WRITEroot@peer&amp;gt; grant -t my_table -u peer Table.READroot@peer&amp;gt; tables -lRemember what the table ID for ‘my_table’ is. You’ll need that to configure the primary instance.PrimaryNext, configure the primary instance.Set up the tableroot@primary&amp;gt; createtable my_tableDefine the Peer as a replication peer to the PrimaryWe’re defining the instance with replication.name of peer as a peer. We provide the implementation of ReplicaSystemthat we want to use, and the configuration for the AccumuloReplicaSystem. In this case, the configuration is the AccumuloInstance name for peer and the ZooKeeper quorum string. The configuration key is of the formreplication.peer.$peer_name.root@primary&amp;gt; config -s replication.peer.peer=org.apache.accumulo.tserver.replication.AccumuloReplicaSystem,peer,$peer_zk_quorumSet the authentication credentialsWe want to use that special username and password that we created on the peer, so we have a means to write data tothe table that we want to replicate to. The configuration key is of the form “replication.peer.user.$peer_name”.root@primary&amp;gt; config -s replication.peer.user.peer=peerroot@primary&amp;gt; config -s replication.peer.password.peer=peerEnable replication on the tableNow that we have defined the peer on the primary and provided the authentication credentials, we need to configureour table with the implementation of ReplicaSystem we want to use to replicate to the peer. In this case, our peeris an Accumulo instance, so we want to use the AccumuloReplicaSystem.The configuration for the AccumuloReplicaSystem is the table ID for the table on the peer instance that wewant to replicate into. Be sure to use the correct value for $peer_table_id. The configuration key is ofthe form “table.replication.target.$peer_name”.root@primary&amp;gt; config -t my_table -s table.replication.target.peer=$peer_table_idFinally, we can enable replication on this table.root@primary&amp;gt; config -t my_table -s table.replication=trueExtra considerations for useWhile this feature is intended for general-purpose use, its implementation does carry some baggage. Like any software,replication is a feature that operates well within some set of use cases but is not meant to support all use cases.For the benefit of the users, we can enumerate these cases.LatencyAs previously mentioned, the replication feature uses the Write-Ahead Log files for a number of reasons, one of whichis to prevent the need for data to be written to RFiles before it is available to be replicated. While this can helpreduce the latency for a batch of Mutations that have been written to Accumulo, the latency is at least seconds to tensof seconds for replication once ingest is active. For a table which replication has just been enabled on, this is likelyto take a few minutes before replication will begin.Once ingest is active and flowing into the system at a regular rate, replication should be occurring at a similar rate,given sufficient computing resources. Replication attempts to copy data at a rate that is to be considered low latencybut is not a replacement for custom indexing code which can ensure near real-time referential integrity on secondary indexes.Table-Configured IteratorsAccumulo Iterators tend to be a heavy hammer which can be used to solve a variety of problems. In general, it is highlyrecommended that Iterators which are applied at major compaction time are both idempotent and associative due to thenon-determinism in which some set of files for a Tablet might be compacted. In practice, this translates to common patterns,such as aggregation, which are implemented in a manner resilient to duplication (such as using a Set instead of a List).Due to the asynchronous nature of replication and the expectation that hardware failures and network partitions will exist,it is generally not recommended to not configure replication on a table which has Iterators set which are not idempotent.While the replication implementation can make some simple assertions to try to avoid re-replication of data, it is notpresently guaranteed that all data will only be sent to a peer once. Data will be replicated at least once. Typically,this is not a problem as the VersioningIterator will automatically deduplicate this over-replication because they willhave the same timestamp; however, certain Combiners may result in inaccurate aggregations.As a concrete example, consider a table which has the SummingCombiner configured to sum all values formultiple versions of the same Key. For some key, consider a set of numeric values that are written to a table on theprimary: [1, 2, 3]. On the primary, all of these are successfully written and thus the current value for the given keywould be 6, (1 + 2 + 3). Consider, however, that each of these updates to the peer were done independently (becauseother data was also included in the write-ahead log that needed to be replicated). The update with a value of 1 wassuccessfully replicated, and then we attempted to replicate the update with a value of 2 but the remote server neverresponded. The primary does not know whether the update with a value of 2 was actually applied or not, so theonly recourse is to re-send the update. After we receive confirmation that the update with a value of 2 was replicated,we will then replicate the update with 3. If the peer did never apply the first update of ‘2’, the summation is accurate.If the update was applied but the acknowledgement was lost for some reason (system failure, network partition), theupdate will be resent to the peer. Because addition is non-idempotent, we have created an inconsistency between theprimary and peer. As such, the SummingCombiner wouldn’t be recommended on a table being replicated.While there are changes that could be made to the replication implementation which could attempt to mitigate this risk,presently, it is not recommended to configure Iterators or Combiners which are not idempotent to support cases whereinaccuracy of aggregations is not acceptable.Duplicate KeysIn Accumulo, when more than one key exists that are exactly the same, keys that are equal down to the timestamp,the retained value is non-deterministic. Replication introduces another level of non-determinism in this case.For a table that is being replicated and has multiple equal keys with different values inserted into it, the finalvalue in that table on the primary instance is not guaranteed to be the final value on all replicas.For example, say the values that were inserted on the primary instance were value1 and value2 and the finalvalue was value1, it is not guaranteed that all replicas will have value1 like the primary. The final value isnon-deterministic for each instance.As is the recommendation without replication enabled, if multiple values for the same key (sans timestamp) are written toAccumulo, it is strongly recommended that the value in the timestamp properly reflects the intended version bythe client. That is to say, newer values inserted into the table should have larger timestamps. If the time betweenwriting updates to the same key is significant (order minutes), this concern can likely be ignored.Bulk ImportsCurrently, files that are bulk imported into a table configured for replication are not replicated. There is notechnical reason why it was not implemented, it was simply omitted from the initial implementation. This is considered afair limitation because bulk importing generated files multiple locations is much simpler than bifurcating “live” ingestdata into two instances. Given some existing bulk import process which creates files and them imports them into anAccumulo instance, it is trivial to copy those files to a new HDFS instance and import them into another Accumuloinstance using the same process. Hadoop’s distcp command provides an easy way to copy large amounts of data to anotherHDFS instance which makes the problem of duplicating bulk imports very easy to solve.Table SchemaThe following describes the kinds of keys, their format, and their general function for the purposes of individualsunderstanding what the replication table describes. Because the replication table is essentially a state machine,this data is often the source of truth for why Accumulo is doing what it is with respect to replication. There arethree “sections” in this table: “repl”, “work”, and “order”.Repl sectionThis section is for the tracking of a WAL file that needs to be replicated to one or more Accumulo remote tables.This entry is tracking that replication needs to happen on the given WAL file, but also that the local Accumulo table,as specified by the column qualifier “local table ID”, has information in this WAL file.The structure of the key-value is as follows:&amp;lt;HDFS_uri_to_WAL&amp;gt; repl:&amp;lt;local_table_id&amp;gt; [] -&amp;gt; &amp;lt;protobuf&amp;gt;This entry is created based on a replication entry from the Accumulo metadata table, and is deleted from the replication tablewhen the WAL has been fully replicated to all remote Accumulo tables.Work sectionThis section is for the tracking of a WAL file that needs to be replicated to a single Accumulo table in a remoteAccumulo cluster. If a WAL must be replicated to multiple tables, there will be multiple entries. The Value for thisKey is a serialized ProtocolBuffer message which encapsulates the portion of the WAL which was already sent forthis file. The “replication target” is the unique location of where the file needs to be replicated: the identifierfor the remote Accumulo cluster and the table ID in that remote Accumulo cluster. The protocol buffer in the valuetracks the progress of replication to the remote cluster.&amp;lt;HDFS_uri_to_WAL&amp;gt; work:&amp;lt;replication_target&amp;gt; [] -&amp;gt; &amp;lt;protobuf&amp;gt;The “work” entry is created when a WAL has an “order” entry, and deleted after the WAL is replicated to allnecessary remote clusters.Order sectionThis section is used to order and schedule (create) replication work. In some cases, data with the same timestampmay be provided multiple times. In this case, it is important that WALs are replicated in the same order they werecreated/used. In this case (and in cases where this is not important), the order entry ensures that oldest WALsare processed most quickly and pushed through the replication framework.&amp;lt;time_of_WAL_closing&amp;gt;x00&amp;lt;HDFS_uri_to_WAL&amp;gt; order:&amp;lt;local_table_id&amp;gt; [] -&amp;gt; &amp;lt;protobuf&amp;gt;The “order” entry is created when the WAL is closed (no longer being written to) and is removed whenthe WAL is fully replicated to all remote locations.",
      "url": " /docs/2.x/administration/replication",
      "categories": "administration"
    },
  
    "docs-2-x-administration-scan-executors": {
      "title": "Scan Executors",
      "content": "Accumulo scans operate by repeatedly fetching batches of data from a tabletserver.  On the tablet server side, a thread pool fetches batches.In Java threads pools are called executors.  By default, a single executor pertablet server handles all scans in FIFO order.  For some workloads, the singleFIFO executor is suboptimal.  For example, consider many unimportant scansreading lots of data mixed with a few important scans reading small amounts ofdata.  The long scans noticeably increase the latency of the short scans.Accumulo offers two mechanisms to help improve situations like this: multiplescan executors and per executor prioritizers.  Additional scan executors cangive tables dedicated resources.  For each scan executor, an optionalprioritizer can reorder queued work.Configuring and using Scan ExecutorsBy default, Accumulo sets tserver.scan.executors.default.threads=16 whichcreates the default scan executor.  To configure additional scan executors,chose a unique name and configure tserver.scan.executors.*.  Settingthe following causes each tablet server to create a scan executor with thespecified threads.tserver.scan.executors.&amp;lt;name&amp;gt;.threads=&amp;lt;number&amp;gt;Optionally, some of the following can be set.  The priority settingdetermines thread priority.  The prioritizer settings specifies a class thatorders pending work.tserver.scan.executors.&amp;lt;name&amp;gt;.priority=&amp;lt;number 1 to 10&amp;gt;tserver.scan.executors.&amp;lt;name&amp;gt;.prioritizer=&amp;lt;class name&amp;gt;tserver.scan.executors.&amp;lt;name&amp;gt;.prioritizer.opts.&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;After creating an executor, configure table.scan.dispatcher to use it.  Adispatcher is Java subclass of ScanDispatcherthat decides which scan executor should service a table.  Set the following tableproperty to configure a dispatcher.table.scan.dispatcher=&amp;lt;class name&amp;gt;Scan dispatcher options can be set with properties like the following.table.scan.dispatcher.opts.&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;The default value for table.scan.dispatcher is SimpleScanDispatcher.SimpleScanDispatcher supports an executor option for choosing a scanexecutor.  If this option is not set, then SimpleScanDispatcher will dispatchto the scan executor named default.To tie everything together, consider the following use case.  Create tables named LOW1 and LOW2 using a scan executor with a single thread.  Create a table named HIGH with a dedicated scan executor with 8 threads.  Create tables named NORM1 and NORM2 using the default scan executor.  Set the default executor to 4 threads.The following shell commands implement this use case.createtable LOW1createtable LOW2createtable HIGHcreatetable NORM1createtable NORM2config -s tserver.scan.executors.default.threads=4config -s tserver.scan.executors.low.threads=1config -s tserver.scan.executors.high.threads=8Tablet servers should be restarted after configuring scan executors, then tables can be configured.config -t LOW1 -s table.scan.dispatcher=org.apache.accumulo.core.spi.scan.SimpleScanDispatcherconfig -t LOW1 -s table.scan.dispatcher.opts.executor=lowconfig -t LOW2 -s table.scan.dispatcher=org.apache.accumulo.core.spi.scan.SimpleScanDispatcherconfig -t LOW2 -s table.scan.dispatcher.opts.executor=lowconfig -t HIGH -s table.scan.dispatcher=org.apache.accumulo.core.spi.scan.SimpleScanDispatcherconfig -t HIGH -s table.scan.dispatcher.opts.executor=highWhile not necessary because it’s the default, it is safer to also settable.scan.dispatcher=org.apache.accumulo.core.spi.scan.SimpleScanDispatcherfor each table.  This ensures things work as expected in the case wheretable.scan.dispatcher was set at the system or namespace level.Configuring and using Scan Prioritizers.When all scan executor threads are busy, incoming work is queued.  Bydefault, this queue has a FIFO order.  A ScanPrioritizer can be configured toreorder the queue.  Accumulo ships with the IdleRatioScanPrioritizer whichorders the queue by the ratio of run time to idle time.  For example, a scanwith a run time of 50ms and an idle time of 200ms would have a ratio of .25.If .25 were the lowest ratio on the queue, then it would be the next in line.The following configures the IdleRatioScanPrioritizer for the default scanexecutor.tserver.scan.executors.default.prioritizer=org.apache.accumulo.core.spi.scan.IdleRatioScanPrioritizerUsing the IdleRatioScanPrioritizer in a test with 50 long running scans and 5threads repeatedly doing small random lookups made a significant difference.In this test the average lookup time for the 5 threads went from 250ms to 5 ms.Providing hints from the client side.Scanners can provide hints to ScanDispatchers and ScanPriotizers by callingsetExecutionHints on the Scanner.  What, if anything, is done with thesehints depends on what is configured for the table and system.  Accumulo’sdefault configuration ignores hints. The following shell commands make itpossible to choose an executor and set priorities from a scanner for thetable tex.config -s tserver.scan.executors.special.threads=8config -s tserver.scan.executors.special.prioritizer=org.apache.accumulo.core.spi.scan.HintScanPrioritizerconfig -s tserver.scan.executors.special.prioritizer.opts.priority.alpha=1config -s tserver.scan.executors.special.prioritizer.opts.priority.gamma=3createtable texconfig -t tex -s table.scan.dispatcher=org.apache.accumulo.core.spi.scan.SimpleScanDispatcherconfig -t tex -s table.scan.dispatcher.opts.executor.alpha=specialconfig -t tex -s table.scan.dispatcher.opts.executor.gamma=specialThe HintScanPrioritizer honorshints of the form priority=&amp;lt;integer&amp;gt; or scan_type=&amp;lt;type&amp;gt; to prioritizescans, with lower integers resulting in a higher priority.  When a hintspecifies a scan type it is mapped to a priority based on the prioritizerconfiguration.The SimpleScanDispatcher, which is the default dispatcher, supportsexecutor.&amp;lt;type&amp;gt;=&amp;lt;executor&amp;gt; options. When a scanner sets a hint of the formscan_type=&amp;lt;type&amp;gt; it will use the executor configured for that type.After restarting tservers, the following command will start a scan that usesthe executor special with a priority of 3.  The scan dispatcher maps the scantype gamma to the executor special.  The prioritizer maps the scan typegamma to a priority of 3.scan -t tex --execution-hints scan_type=gammaThe following command will start a scan that uses the executor special with apriority of 1.scan -t tex --execution-hints scan_type=alphaExecution Hints can also be used to influence how the block caches are used fora scan. The following configuration would modify the gamma executor to use blocksin the cache if they are already cached, but would never load missing blocks into thecache.config -t tex -s table.scan.dispatcher.opts.cacheUsage.gamma=opportunisticOther valid values are disabled which does not use data in the block caches,enabled which uses the block cache as it normally would and table which enablesthe block cache for the scan if it’s enabled on the table.",
      "url": " /docs/2.x/administration/scan-executors",
      "categories": "administration"
    },
  
    "docs-2-x-administration-upgrading": {
      "title": "Upgrading Accumulo",
      "content": "Upgrading from 1.10 or 2.0 to 2.1Please read these directions in their entirety before beginning. Please contact us with anyquestions you have about this process.IMPORTANT! Before running any Accumulo 2.1 upgrade utilities or services, you will need toupgrade to Java 11, Hadoop 3, and at least ZooKeeper 3.5 (at least 3.8 was current at the time ofthis writing and is recommended).The basic upgrade sequence is:  upgrade to at least Accumulo 1.10 first (if necessary)  stop Accumulo 1.10 or 2.0  prepare your installation of Accumulo 2.1 through whatever means you obtain the binaries andconfigure it in your environment  start ZooKeeper and HDFS.  (optional - but recommended) create a ZooKeeper snapshot  (optional - but recommended) validate the ZooKeeper ACLs. See ZooKeeper ACLs  (required if not using the provided scripts to start 2.1) run the RenameMasterDirInZK utility  (optional) run the pre-upgrade utility to convert the configuration in ZooKeeper  start Accumulo 2.1 for the first time to complete the upgradeIMPORTANT! before starting any upgrade process you need to make sure there are no outstandingFATE transactions. This includes transactions that have completed with SUCCESS or FAILED buthave not been removed by the automatic clean-up process. This is required because the internalserialization of FATE transactions is not guaranteed to be compatible between versions, so ANYFATE transaction that is present will fail the upgrade. Procedures to manage FATE transactions,including commands to fail and delete transactions, are included in the FATE Administrationdocumentation.Two significant changes occurred in 2.1 that are particularly important to note for upgrades:  properties and services that referenced master are renamed manager and  the internal property storage format in ZooKeeper has changed - instead of each table, namespace,and the system configuration using separate ZooKeeper nodes for each of their properties, theyeach now use only a single ZooKeeper node for all of their respective properties.Details on renaming the properties and the ZooKeeper property conversion are provided in thefollowing sections. Additional information on configuring 2.1 is available here.Create ZooKeeper snapshot (optional - but recommended)Before upgrading to 2.1, it is suggested that you create a snapshot of the current ZooKeepercontents to be a backup in case issues occur and you need to rollback. There are no provisions toroll back to a previous Accumulo version once an upgrade process has been completed other thanrestoring from a snapshot of ZooKeeper.$ACCUMULO_HOME/bin/accumulo dump-zoo --xml --root /accumulo | tee PATH_TO_SNAPSHOTIf you need to restore from the ZooKeeper snapshot see these instructions.Rename master Properties, Config Files, and Script ReferencesIt is strongly recommended as a part of the upgrade to rename any properties inaccumulo.properties (or properties specified on the command line) starting with master. to usethe equivalent property starting with manager. instead, as the old properties will not beavailable in subsequent major releases. This version may log or display warnings if older propertiesare observed.Any reference to master in other scripts (e.g., invoking accumulo-service master from an initscript) should be renamed to manager (for example, accumulo-service manager).If the manager is not started using the provided accumulo-cluster or accumulo-service scripts,then a one-time upgrade step will need to be performed. Run the RenameMasterDirInZK utility afterinstalling 2.1 but before starting it.${ACCUMULO_HOME}/bin/accumulo org.apache.accumulo.manager.upgrade.RenameMasterDirInZKPre-Upgrade the property storage in ZooKeeper (optional)As mentioned above, the property storage in ZooKeeper has changed from many nodes per table,namespace, and the system configuration, to just a single node for each of those. Upgrading to usethe new format does happen automatically when Accumulo 2.1 servers start up. However, you canoptionally choose to convert them using a pre-upgrade step with the following command line utility.The property conversion can be done using a command line utility or it will occur automatically whenthe manager is started for the first time. Using the command line utility is optional, but mayprovide more flexibility in handling issues if they were to occur. With ZooKeeper running, thecommand to convert the properties is:$ACCUMULO_HOME/bin/accumulo config-upgradeThe utility will print messages about its progress as it converts them.2022-11-03T14:35:44,596 [conf.SiteConfiguration] INFO : Found Accumulo configuration on classpath at /opt/fluo-uno/install/accumulo-3.0.0-SNAPSHOT/conf/accumulo.properties2022-11-03T14:35:45,511 [util.ConfigPropertyUpgrader] INFO : Upgrade system config properties for a1518a8b-f007-41ee-af2c-5cc760abe7fd2022-11-03T14:35:45,675 [util.ConfigTransformer] INFO : property transform for SystemPropKey{InstanceId=a1518a8b-f007-41ee-af2c-5cc760abe7fd&#39;} took 29ms ms, delete count: 1, error count: 02022-11-03T14:35:45,683 [util.ConfigPropertyUpgrader] INFO : Upgrading namespace +accumulo base path: /accumulo/a1518a8b-f007-41ee-af2c-5cc760abe7fd/namespaces/+accumulo/conf...2022-11-03T14:35:45,737 [util.ConfigPropertyUpgrader] INFO : Upgrading table !0 base path: /accumulo/a1518a8b-f007-41ee-af2c-5cc760abe7fd/tables/!0/conf2022-11-03T14:35:45,813 [util.ConfigTransformer] INFO : property transform for TablePropKey{TableId=!0&#39;} took 72ms ms, delete count: 26, error count: 0...If the upgrade utility is not used, similar messages will print to the server logs when 2.1 starts.When the property conversion is complete, you can verify the configuration using thezoo-info-viewer utility (new in 2.1)$ACCUMULO_HOME/bin/accumulo zoo-info-viewer  --print-propsCreate new cluster configuration fileThe accumulo-cluster script now uses a single file that defines the location of the managers,tservers, etc. You can create this file using the command accumulo-cluster create-config. You willthen need to transfer the contents of the current individual files to this new consolidated file.Encrypted InstancesWarning: Upgrading a previously encrypted instance with the experimental encryption propertiesis not supported as the implementation and properties have changed. You may be able to disableencryption and compact your files without encryption, in order to upgrade. Encryption remains anexperimental feature, and may change between versions. It should be used with care. If you needhelp, consider reaching out to our mailing list.Upgrading from 1.8/9/10 to 2.0Follow the steps below to upgrade your Accumulo instance and client to 2.0.Upgrade Accumulo instanceIMPORTANT! Before upgrading to Accumulo 2.0, you will need to upgrade to Java 8 and Hadoop 3.x.Upgrading to Accumulo 2.0 is done by stopping Accumulo 1.8/9 and starting Accumulo 2.0.Before stopping Accumulo 1.8/9, install Accumulo 2.0 and configure it by following the 2.0 quick start.There are several changes to scripts and configuration in 2.0 so be careful when using configuration or automated setup designed for 1.8/9.Below are some changes in 2.0 that you should be aware of:  accumulo.properties has replaced accumulo-site.xml. You can either convert accumulo-site.xml by handfrom XML to properties or use the following Accumulo command.    accumulo convert-config -x old/accumulo-site.xml -p new/accumulo.properties        The following server properties were deprecated for 2.0:          general.classpaths      tserver.metadata.readahead.concurrent.max      tserver.readahead.concurrent.max        accumulo-client.properties has replaced client.conf. The client propertiesin the new file are different so take care when customizing.  accumulo-cluster script has replaced the start-all.sh &amp;amp; stop-all.sh scripts.          Default host files (i.e masters, monitor, gc) are no longer in conf/ directory of tarball but can be created using accumulo-cluster create-config      Tablet server hosts must be listed in a tservers file instead of a slaves file. To minimize confusion, Accumulo will not start if the old slaves file is present.        accumulo-service script can be used to start/stop Accumulo services (i.e master, tablet server, monitor) on a single node.          Can be used even if Accumulo was started using accumulo-cluster script.        accumulo-env.sh constructs environment variables (such as JAVA_OPTS and CLASSPATH) used when running Accumulo processes          This file was used in Accumulo 1.x but has changed significantly for 2.0      Environment variables (such as $cmd, $bin, $conf) are set before accumulo-env.sh is loaded and can be used to customize environment.      The JAVA_OPTS variable is constructed in accumulo-env.sh to pass command line arguments to the java command that the starts Accumulo processes(i.e. java $JAVA_OPTS main.class.for.$cmd).      The CLASSPATH variable sets the Java classpath used when running Accumulo processes. It can be modified to upgrade dependencies or use vendor-specificdistributions of Hadoop.        Logging is configured in accumulo-env.sh for Accumulo processes. The following log4j configuration files in the conf/ directory will be used ifaccumulo-env.sh is not modified. These files can be modified to turn on/off logging for Accumulo processes:          log4j-service.properties for all Accumulo services (except monitor)      logj4-monitor.properties for Accumulo monitor      log4j.properties for Accumulo clients and commands        MapReduce jobs that read/write from Accumulo must configure their dependencies differently.  Run the command accumulo shell to access the shell using configuration in conf/accumulo-client.propertiesWhen your Accumulo 2.0 installation is properly configured, stop Accumulo 1.8/9 and start Accumulo 2.0:./accumulo-1.9.3/bin/stop-all.sh./accumulo-2.0.1/bin/accumulo-cluster startIt is recommended that users test this upgrade on development or test clusters before attempting it on production clusters.Upgrade Accumulo clientsThere several client API changes in 2.0. In most cases, new API was introduced and the old API was only deprecated. While it is recommendedthat users start using the new API, the old API will continue to be supported through 2.x.Below is a list of client API changes that users are required to make for 2.0:  Update your pom.xml use Accumulo 2.0. Also, update any Hadoop &amp;amp; ZooKeeper dependencies in your pom.xml to match the versions running on your cluster.    &amp;lt;dependency&amp;gt;  &amp;lt;groupId&amp;gt;org.apache.accumulo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;accumulo-core&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;2.0.1&amp;lt;/version&amp;gt;&amp;lt;/dependency&amp;gt;        ClientConfiguration objects can no longer be created using new ClientConfiguration().          Use ClientConfiguration.create() instead        Some API deprecated in 1.x releases was dropped  Aggregators have been removedBelow is a list of recommended client API changes:  The API for creating Accumulo clients has changed in 2.0.          The old API using ZooKeeperInstance, Connector, Instance, and ClientConfiguration has been deprecated.      Connector objects can be created from an AccumuloClient object using Connector.from()        Accumulo’s MapReduce API has changed in 2.0.          A new API has been introduced in the org.apache.accumulo.hadoop package of the accumulo-hadoop-mapreduce jar.      The old API in the org.apache.accumulo.core.client package of the accumulo-core has been deprecated and willeventually be removed.      For both the old and new API, you must configure dependencies differentlywhen creating your MapReduce job.      Upgrading from 1.7 to 1.8Upgrades from 1.7 to 1.8 are possible with little effort as no changes were made at the data layer and RPC changes were made in a backwards-compatible way. The recommended way is to stop Accumulo 1.7, perform the Accumulo upgrade to 1.8, and then start 1.8. Like previous versions, after 1.8 is started on a 1.7 instance, a one-time upgrade will happen by the Master which will prevent a downgrade back to 1.7. Upgrades are still one way. Upgrades from versions prior to 1.7 to 1.8 should follow the below path to 1.7 and then perform the upgrade to 1.8 – direct upgrades to 1.8 for versions other than 1.7 are untested.Existing configuration files from 1.7 should be compared against the examples provided in 1.8. The 1.7 configuration files should all function with 1.8 code, but you will likely want to include changes found in the 1.8.0 release notes and these release notes for 1.8.1.For upgrades from prior to 1.7, follow the upgrade instructions to 1.7 first.Upgrading from 1.7.x to 1.7.yThe recommended way to upgrade from a prior 1.7.x release is to stop Accumulo, upgrade to 1.7.y and then start 1.7.y.When upgrading, there is a known issue if the upgrade fails due to outstanding FATE operations, see ACCUMULO-4496 The workaround if this situation is encountered:  Start tservers  Start shell  Run fate print to list all  If completed, just delete with fate delete  Start masters once there are no more fate operationsIf any of the FATE operations are not complete, you should rollback the upgrade and troubleshoot completing them with your prior version. When performing an upgrade between major versions, the upgrade is one-way, therefore it is important that you do not have any outstanding FATE operations before starting the upgrade.Upgrading from 1.6 to 1.7Upgrades from 1.6 to 1.7 are possible with little effort as no changes were made at the data layer and RPC changes were made in a backwards-compatible way. The recommended way is to stop Accumulo 1.6, perform the Accumulo upgrade to 1.7, and then start 1.7. Like previous versions, after 1.7.0 is started on a 1.6 instance, a one-time upgrade will happen by the Master which will prevent a downgrade back to 1.6. Upgrades are still one way. Upgrades from versions prior to 1.6 to 1.7 should follow the below path to 1.6 and then perform the upgrade to 1.7 – direct upgrades to 1.7 for versions other than 1.6 are untested.After upgrading to 1.7.0, users will notice the addition of a replication table in the accumulo namespace. This table is created and put offline to avoid any additional maintenance if the data-center replication feature is not in use.Existing configuration files from 1.6 should be compared against the examples provided in 1.7. The 1.6 configuration files should all function with 1.7 code, but you will likely want to include a new file (hadoop-metrics2-accumulo.properties) to enable the new metrics subsystem. Read the section on Hadoop Metrics2 in the Administration chapter of the Accumulo User Manual.For each of the other new features, new configuration properties exist to support the feature. Refer to the added sections in the User Manual for the feature for information on how to properly configure and use the new functionality.Upgrading from 1.5 to 1.6This happens automatically the first time Accumulo 1.6 is started.If your instance previously upgraded from 1.4 to 1.5, you must verify that your1.5 instance has no outstanding local write ahead logs. You can do this by ensuringeither:  All of your tables are online and the Monitor shows all tablets hosted  The directory for write ahead logs (logger.dir.walog) from 1.4 has no files remaining  on any tablet server / logger hostsTo upgrade from 1.5 to 1.6 you must:  Verify that there are no outstanding FATE operations          Under 1.5 you can list what’s in FATE by running$ACCUMULO_HOME/bin/accumulo org.apache.accumulo.server.fate.Admin print      Note that operations in any state will prevent an upgrade. It is safeto delete operations with status SUCCESSFUL. For others, you should restartyour 1.5 cluster and allow them to finish.        Stop the 1.5 instance.  Configure 1.6 to use the hdfs directory and zookeepers that 1.5 was using.  Copy other 1.5 configuration options as needed.  Start Accumulo 1.6.The upgrade process must make changes to Accumulo’s internal state in both ZooKeeper and  the table metadata. This process may take some time if Tablet Servers have to go through  recovery. During this time, the Monitor will claim that the Master is down and some  services may send the Monitor log messages about failure to communicate with each other.  These messages are safe to ignore. If you need detail on the upgrade’s progress you should  view the local logs on the Tablet Servers and active Master.Upgrading from 1.4 to 1.6To upgrade from 1.4 to 1.6 you must perform a manual initial step.Prior to upgrading you must:  Verify that there are no outstanding FATE operations          Under 1.4 you can list what’s in FATE by running $ACCUMULO_HOME/bin/accumulo org.apache.accumulo.server.fate.Admin print      Note that operations in any state will prevent an upgrade. It is safe to delete operations with status SUCCESSFUL. For others,  you should restart your 1.4 cluster and allow them to finish.        Stop the 1.4 instance.  Configure 1.6 to use the hdfs directory, walog directories, and zookeepers that 1.4 was using.  Copy other 1.4 configuration options as needed.Prior to starting the 1.6 instance you will need to run the LocalWALRecovery tool on each node that previously ran an instance of the Logger role.$ACCUMULO_HOME/bin/accumulo org.apache.accumulo.tserver.log.LocalWALRecoveryThe recovery tool will rewrite the 1.4 write ahead logs into a format that 1.6 can read. After this step has completed on all nodes, start the 1.6 cluster to continue the upgrade.The upgrade process must make changes to Accumulo’s internal state in both ZooKeeper and the table metadata. This process may take some time if Tablet Servers have to go through recovery. During this time, the Monitor will claim that the Master is down and some services may send the Monitor log messages about failure to communicate with each other. While the upgrade is in progress, the Garbage Collector may complain about invalid paths. The Master may also complain about failure to create the trace table because it already exists. These messages are safe to ignore. If other error messages occur, you should seek out support before continuing to use Accumulo. If you need detail on the upgrade’s progress you should view the local logs on the Tablet Servers and active Master.Note that the LocalWALRecovery tool does not delete the local files. Once you confirm that 1.6 is successfully running, you should delete these files on the local filesystem.Upgrading from 1.4 to 1.5This happens automatically the first time Accumulo 1.5 is started.  Stop the 1.4 instance.  Configure 1.5 to use the hdfs directory, walog directories, and zookeepersthat 1.4 was using.  Copy other 1.4 configuration options as needed.  Start Accumulo 1.5.",
      "url": " /docs/2.x/administration/upgrading",
      "categories": "administration"
    },
  
    "docs-2-x-configuration-client-properties": {
      "title": "Client Properties (2.x)",
      "content": "Below are properties set in accumulo-client.properties that configure Accumulo clients. All properties have been part of the API since 2.0.0 (unless otherwise specified):            Property      Default value      Since      Description                   instance.name      empty      2.0.0      Name of Accumulo instance to connect to               instance.zookeepers      localhost:2181      2.0.0      Zookeeper connection information for Accumulo instance               instance.zookeepers.timeout      30s      2.0.0      Zookeeper session timeout               auth.type      password      2.0.0      Authentication method (i.e password, kerberos, PasswordToken, KerberosToken, etc)               auth.principal      empty      2.0.0      Accumulo principal/username for chosen authentication method               auth.token      empty      2.0.0      Authentication token (ex. mypassword, /path/to/keytab)               batch.writer.durability      default      2.0.0      The durability used to write to the write-ahead log. Legal values are: none, which skips the write-ahead log; log, which sends the data to the write-ahead log, but does nothing to make it durable; flush, which pushes data to the file system; and sync, which ensures the data is written to disk. Setting this property will change the durability for the BatchWriter session. A value of “default” will use the table’s durability setting.               batch.writer.latency.max      120s      2.0.0      Max amount of time (in seconds) to hold data in memory before flushing it               batch.writer.memory.max      50M      2.0.0      Max memory (in bytes) to batch before writing               batch.writer.threads.max      3      2.0.0      Maximum number of threads to use for writing data to tablet servers.               batch.writer.timeout.max      0      2.0.0      Max amount of time (in seconds) an unresponsive server will be re-tried. An exception is thrown when this timeout is exceeded. Set to zero for no timeout.               batch.scanner.num.query.threads      3      2.0.0      Number of concurrent query threads to spawn for querying               scanner.batch.size      1000      2.0.0      Number of key/value pairs that will be fetched at time from tablet server               ssl.enabled      false             Enable SSL for client RPC               ssl.keystore.password      empty             Password used to encrypt keystore               ssl.keystore.path      empty      2.0.0      Path to SSL keystore file               ssl.keystore.type      jks             Type of SSL keystore               ssl.truststore.password      empty             Password used to encrypt truststore               ssl.truststore.path      empty      2.0.0      Path to SSL truststore file               ssl.truststore.type      jks             Type of SSL truststore               ssl.use.jsse      false             Use JSSE system properties to configure SSL               sasl.enabled      false             Enable SASL for client RPC               sasl.kerberos.server.primary      accumulo             Kerberos principal/primary that Accumulo servers use to login               sasl.qop      auth             SASL quality of protection. Valid values are ‘auth’, ‘auth-int’, and ‘auth-conf’               trace.span.receivers      org.apache.accumulo.tracer.ZooTraceClient             A list of span receiver classes to send trace spans               trace.zookeeper.path      /tracers      2.0.0      The zookeeper node where tracers are registered      ",
      "url": " /docs/2.x/configuration/client-properties",
      "categories": "configuration"
    },
  
    "docs-2-x-configuration-client-properties3": {
      "title": "Client Properties (3.x)",
      "content": "Below are properties set in accumulo-client.properties that configure Accumulo clients. All properties have been part of the API since 2.0.0 (unless otherwise specified):            Property      Default value      Since      Description                   instance.name      empty      2.0.0      Name of Accumulo instance to connect to               instance.zookeepers      localhost:2181      2.0.0      Zookeeper connection information for Accumulo instance               instance.zookeepers.timeout      30s      2.0.0      Zookeeper session timeout               auth.type      password      2.0.0      Authentication method (i.e password, kerberos, PasswordToken, KerberosToken, etc)               auth.principal      empty      2.0.0      Accumulo principal/username for chosen authentication method               auth.token      empty      2.0.0      Authentication token (ex. mypassword, /path/to/keytab)               batch.writer.durability      default      2.0.0      The durability used to write to the write-ahead log. Legal values are: none, which skips the write-ahead log; log, which sends the data to the write-ahead log, but does nothing to make it durable; flush, which pushes data to the file system; and sync, which ensures the data is written to disk. Setting this property will change the durability for the BatchWriter session. A value of “default” will use the table’s durability setting.               batch.writer.latency.max      120s      2.0.0      Max amount of time (in seconds) to hold data in memory before flushing it               batch.writer.memory.max      50M      2.0.0      Max memory (in bytes) to batch before writing               batch.writer.threads.max      3      2.0.0      Maximum number of threads to use for writing data to tablet servers.               batch.writer.timeout.max      0      2.0.0      Max amount of time (in seconds) an unresponsive server will be re-tried. An exception is thrown when this timeout is exceeded. Set to zero for no timeout.               batch.scanner.num.query.threads      3      2.0.0      Number of concurrent query threads to spawn for querying               scanner.batch.size      1000      2.0.0      Number of key/value pairs that will be fetched at time from tablet server               ssl.enabled      false             Enable SSL for client RPC               ssl.keystore.password      empty             Password used to encrypt keystore               ssl.keystore.path      empty      2.0.0      Path to SSL keystore file               ssl.keystore.type      jks             Type of SSL keystore               ssl.truststore.password      empty             Password used to encrypt truststore               ssl.truststore.path      empty      2.0.0      Path to SSL truststore file               ssl.truststore.type      jks             Type of SSL truststore               ssl.use.jsse      false             Use JSSE system properties to configure SSL               sasl.enabled      false             Enable SASL for client RPC               sasl.kerberos.server.primary      accumulo             Kerberos principal/primary that Accumulo servers use to login               sasl.qop      auth             SASL quality of protection. Valid values are ‘auth’, ‘auth-int’, and ‘auth-conf’      ",
      "url": " /docs/2.x/configuration/client-properties3",
      "categories": "configuration"
    },
  
    "docs-2-x-configuration-files": {
      "title": "Configuration Files",
      "content": "Accumulo has the following configuration files which can be found in theconf/ directory of the Accumulo release tarball.accumulo.propertiesThe accumulo.properties file configures Accumulo server processes usingserver properties. This file can be found in the conf/directory. It is needed on every host that runs Accumulo processes. Therefore, any configuration should bereplicated to all hosts of the Accumulo cluster. If a property is not configured here, it might have beenconfigured another way.  See the quick start for help withconfiguring this file.accumulo-client.propertiesThe accumulo-client.properties file configures Accumulo client processes usingclient properties. If accumulo shell is run without arguments,the Accumulo connection information in this file will be used. This file can be used to create an AccumuloClientin Java using the following code:AccumuloClient client = Accumulo.newClient().from(&quot;/path/to/accumulo-client.properties&quot;).build();See the quick start for help with configuring this file.accumulo-env.shThe accumulo-env.sh file configures the Java classpath and JVM options needed to runAccumulo processes. See the quick start for help with configuring this file.Log configuration fileslog4j2-service.propertiesSince 2.1, the log4j2-service.properties file configures logging for most Accumulo services(i.e Manager, Tablet Server, Garbage Collector, Monitor). Prior to 2.1 this file was named log4j-service.propertiesand did not apply to the Monitor which was configured in a separate log4j-monitor.properties.log4j2.propertiesThe log4j2.properties file configures logging for Accumulo commands (i.e accumulo init,accumulo shell, etc).cluster.yamlThe accumulo-cluster script uses the cluster.yaml file to determine where Accumulo processes should be run.This file is not in the conf/ directory of the Accumulo release tarball by default. It can be created by runningthe command accumulo-cluster create-config. The cluster.yaml file contains the following sections:gcContains a list of hosts where Garbage Collector processes should run. While only one host is needed, others can be specifiedto run standby Garbage Collectors that can take over if the lead Garbage Collector fails.managerContains a list of hosts where Manager processes should run. While only one host is needed, others can be specifiedto run on standby Managers that can take over if the lead Manager fails.monitorContains a list of hosts where Monitor processes should run. While only one host is needed, others can be specifiedto run standby Monitors that can take over if the lead Monitor fails.tserverContains list of hosts where Tablet Server processes should run. While only one host is needed, it is recommended thatmultiple tablet servers are run for improved fault tolerance and performance.sserverContains a list of hosts where ScanServer processes should run. While only one host is needed, it is recommendedthat multiple ScanServers are run for improved performance.compaction coordinatorContains a list of hosts where CompactionCoordinator processes should run. While only one host is needed,others can be specified to run standby CompactionCoordinators that can take over if the lead CompactionCoordinator fails.compaction compactorContains a list of hosts where Compactor processes should run. While only one host is needed, it is recommended thatmultiple Compactors are run for improved external compaction performance.",
      "url": " /docs/2.x/configuration/files",
      "categories": "configuration"
    },
  
    "docs-2-x-configuration-overview": {
      "title": "Configuration Overview",
      "content": "Configuration is managed differently for Accumulo clients and servers.Client ConfigurationAccumulo clients are created using Java builder methods, a Java properties object or anaccumulo-client.properties file containing client properties.Server ConfigurationAccumulo processes (i.e. manager, tablet server, monitor, etc.) are configured by server properties whose values can beset in the following configuration locations (with increasing precedence):  Default - All properties have a default value  Site - Properties set in accumulo.properties  System - Properties set using shell or Java API that apply to entire Accumulo instance  Namespace - Table properties set using shell or Java API that apply to a table namespace  Table - Table properties set using shell or Java API that apply to a table.If a property is set in multiple locations, the value in the location with the highest precedence is used.These configuration locations are described in detail below:DefaultAll server properties have a default value. Default values are set in the source code and can be viewed for each property on the server properties page.While default values have the lowest precedence, they are usually optimal.  However, there are cases where a change can increase query and ingest performance.SiteSite configuration refers to server properties set in the accumulo.properties file which can be found in the conf/ directory. Site configuration will override the default valueof a property. If you are running Accumulo on a cluster, any updates to accumulo.properties must be synced across the cluster. Accumulo processes (manager, tserver, etc) read theirlocal accumulo.properties on start up so processes must be restarted to apply changes. Certain properties can only be set in accumulo.properties. These properties have zk mutable: noin their description. Setting properties in accumulo.properties allows you to configure tablet servers with different settings.Site configuration can be overriden when starting an Accumulo process on the command line (by using the -o option):accumulo tserver -o instance.secret=mysecret -o instance.zookeeper.host=localhost:2181Overriding properties is useful if you can’t change accumulo.properties. It’s done when running Accumulo using Docker.SystemSystem configuration refers to server properties set for the entire Accumulo instance/cluster. These settings are stored in ZooKeeper and can be identified by zk mutable: yesin their description on the server properties page. System configuration will override any site configuration set in accumulo.properties. While most system configurationsettings take effect immediately, some require a restart of the process which is indicated in the zk mutable section of their description. System configuration can be set usingthe following shell command:config -s PROPERTY=VALUEThey can also be set using InstanceOperations in the Java API:client.instanceOperations().setProperty(&quot;table.durability&quot;, &quot;flush&quot;);The java api also supports adding, modifying and removing multiple properties in a single operation:client.instanceOperations().modifyProperties(properties -&amp;gt; {  properties.remove(&quot;table.file.max&quot;);  properties.put(&quot;table.bloom.enabled&quot;, &quot;true&quot;);  properties.put(&quot;table.bloom.error.rate&quot;, &quot;0.75&quot;);  properties.put(&quot;table.bloom.size&quot;, &quot;128000&quot;);});NamespaceNamespace configuration refers to table.* properties set for a certain table namespace (i.e. group of tables). These settings are stored in ZooKeeper. Namespace configurationwill override System configuration and can be set using the following shell command:config -ns NAMESPACE -s PROPERTY=VALUEIt can also be set using NamespaceOperations in the Java API:client.namespaceOperations().setProperty(&quot;mynamespace&quot;, &quot;table.durability&quot;, &quot;sync&quot;);The java api also supports adding, modifying and removing multiple properties in a single operation:client.namespaceOperations().modifyProperties(&quot;mynamespace&quot;, properties -&amp;gt; {        properties.remove(&quot;table.file.max&quot;);        properties.put(&quot;table.bloom.enabled&quot;, &quot;true&quot;);        properties.put(&quot;table.bloom.error.rate&quot;, &quot;0.75&quot;);        properties.put(&quot;table.bloom.size&quot;, &quot;128000&quot;);        });### TableTable configuration refers to [table.* properties] set for a certain table. These settings are stored in ZooKeeper and can be set using the following shell command:    config -t TABLE -s PROPERTY=VALUEThey can also be set using [TableOperations](https://static.javadoc.io/org.apache.accumulo/accumulo-core/2.1.3/org/apache/accumulo/core/client/admin/TableOperations.html) in the Java API:```javaclient.tableOperations().setProperty(&quot;mytable&quot;, &quot;table.durability&quot;, &quot;log&quot;);The java api also supports adding, modifying and removing multiple properties in a single operation:client.tableOperations().modifyProperties(&quot;mytable&quot;, properties -&amp;gt; {        properties.remove(&quot;table.file.max&quot;);        properties.put(&quot;table.bloom.enabled&quot;, &quot;true&quot;);        properties.put(&quot;table.bloom.error.rate&quot;, &quot;0.75&quot;);        properties.put(&quot;table.bloom.size&quot;, &quot;128000&quot;);        });Zookeeper ConsiderationsAny server properties that are set in Zookeeper should consider the limitations of Zookeeper itself with respect to thenumber of nodes and the size of the node data. Custom table properties and options for Iterators configured on tablesare two areas in which there aren’t any fail safes built into the API that can prevent the user from making this mistake.While these properties have the ability to add some much needed dynamic configuration tools, use cases which might fallinto these warnings should be reconsidered.Viewing Server ConfigurationAccumulo’s current configuration can be viewed in the shell using the config command.  config - view configuration for the entire system  config -ns &amp;lt;NAMESPACE&amp;gt; - view configuration for a specific namespace  config -t &amp;lt;TABLE&amp;gt; - view configuration for a specific tableBelow is an example shell output from viewing configuration for the table foo. Please note how table.compaction.major.ratiois set in multiple locations, but the value 1.6 set in the table scope is used because it has the highest precedence.root@accumulo-instance&amp;gt; config -t foo---------+---------------------------------------------+-----------------------SCOPE    | NAME                                        | VALUE---------+---------------------------------------------+-----------------------default  | table.bloom.enabled ....................... | falsedefault  | table.bloom.error.rate .................... | 0.5%default  | table.bloom.hash.type ..................... | murmurdefault  | table.bloom.load.threshold ................ | 1default  | table.bloom.size .......................... | 1048576default  | table.cache.block.enable .................. | falsedefault  | table.cache.index.enable .................. | falsedefault  | table.compaction.major.everything.at ...... | 19700101000000GMTdefault  | table.compaction.major.everything.idle .... | 1hdefault  | table.compaction.major.ratio .............. | 1.3site     |    @override .............................. | 1.4system   |    @override .............................. | 1.5table    |    @override .............................. | 1.6default  | table.compaction.minor.idle ............... | 5mdefault  | table.compaction.minor.logs.threshold ..... | 3default  | table.failures.ignore ..................... | false",
      "url": " /docs/2.x/configuration/overview",
      "categories": "configuration"
    },
  
    "docs-2-x-configuration-server-properties": {
      "title": "Server Properties (2.x)",
      "content": "Below are properties set in accumulo.properties or the Accumulo shell that configure Accumulo servers (i.e. tablet server, manager, etc). Properties labeled ‘Experimental’ should not be considered stable and have a higher risk of changing in the future.            Property      Description                   compaction.coordinator.*      ExperimentalAvailable since: 2.1.0Properties in this category affect the behavior of the accumulo compaction coordinator server.               compaction.coordinator.compaction.finalizer.check.interval      ExperimentalAvailable since: 2.1.0The interval at which to check for external compaction final state markers in the metadata table.type: TIMEDURATION, zk mutable: no, default value: 60s               compaction.coordinator.compaction.finalizer.threads.maximum      ExperimentalAvailable since: 2.1.0The maximum number of threads to use for notifying tablet servers that an external compaction has completed.type: COUNT, zk mutable: no, default value: 5               compaction.coordinator.compactor.dead.check.interval      ExperimentalAvailable since: 2.1.0The interval at which to check for dead compactors.type: TIMEDURATION, zk mutable: no, default value: 5m               compaction.coordinator.port.client      ExperimentalAvailable since: 2.1.0The port used for handling Thrift client connections on the compaction coordinator server.type: PORT, zk mutable: no, default value: 9132               compaction.coordinator.port.search      ExperimentalAvailable since: 2.1.0If the ports above are in use, search higher ports until one is available.type: BOOLEAN, zk mutable: no, default value: false               compaction.coordinator.threadcheck.time      ExperimentalAvailable since: 2.1.0The time between adjustments of the server thread pool.type: TIMEDURATION, zk mutable: no, default value: 1s               compaction.coordinator.threads.minimum      ExperimentalAvailable since: 2.1.0The minimum number of threads to use to handle incoming requests.type: COUNT, zk mutable: no, default value: 1               compaction.coordinator.threads.timeout      ExperimentalAvailable since: 2.1.0The time after which incoming request threads terminate with no work available.  Zero (0) will keep the threads alive indefinitely.type: TIMEDURATION, zk mutable: no, default value: 0s               compaction.coordinator.tserver.check.interval      ExperimentalAvailable since: 2.1.0The interval at which to check the tservers for external compactions.type: TIMEDURATION, zk mutable: no, default value: 1m               compactor.*      ExperimentalAvailable since: 2.1.0Properties in this category affect the behavior of the accumulo compactor server.               compactor.port.client      ExperimentalAvailable since: 2.1.0The port used for handling client connections on the compactor servers.type: PORT, zk mutable: no, default value: 9133               compactor.port.search      ExperimentalAvailable since: 2.1.0If the compactor.port.client is in use, search higher ports until one is available.type: BOOLEAN, zk mutable: no, default value: false               compactor.threadcheck.time      ExperimentalAvailable since: 2.1.0The time between adjustments of the server thread pool.type: TIMEDURATION, zk mutable: no, default value: 1s               compactor.threads.minimum      ExperimentalAvailable since: 2.1.0The minimum number of threads to use to handle incoming requests.type: COUNT, zk mutable: no, default value: 1               compactor.threads.timeout      ExperimentalAvailable since: 2.1.0The time after which incoming request threads terminate with no work available.  Zero (0) will keep the threads alive indefinitely.type: TIMEDURATION, zk mutable: no, default value: 0s               compactor.wait.time.job.max      ExperimentalAvailable since: 2.1.3Compactors do exponential backoff when their request for work repeatedly come back empty. This is the maximum amount of time to wait between checks for the next compaction job.type: TIMEDURATION, zk mutable: no, default value: 5m               compactor.wait.time.job.min      ExperimentalAvailable since: 2.1.3The minimum amount of time to wait between checks for the next compaction job, backing offexponentially until COMPACTOR_MAX_JOB_WAIT_TIME is reached.type: TIMEDURATION, zk mutable: no, default value: 1s               gc.*      Available since: 1.3.5Properties in this category affect the behavior of the accumulo garbage collector.               gc.candidate.batch.size      Available since: 2.1.0The amount of memory used as the batch size for garbage collection.type: MEMORY, zk mutable: yes, default value: 50%               gc.cycle.delay      Available since: 1.3.5Time between garbage collection cycles. In each cycle, old RFiles or write-ahead logs no longer in use are removed from the filesystem.type: TIMEDURATION, zk mutable: yes, default value: 5m               gc.cycle.start      Available since: 1.3.5Time to wait before attempting to garbage collect any old RFiles or write-ahead logs.type: TIMEDURATION, zk mutable: yes, default value: 30s               gc.port.client      Available since: 1.3.5The listening port for the garbage collector’s monitor service.type: PORT, zk mutable: yes but requires restart of the gc, default value: 9998               gc.post.metadata.action      Available since: 1.10.0When the gc runs it can make a lot of changes to the metadata, on completion,  to force the changes to be written to disk, the metadata and root tables can be flushed and possibly compacted. Legal values are: compact - which both flushes and compacts the metadata; flush - which flushes only (compactions may be triggered if required); or none.type: GC_POST_ACTION, zk mutable: yes, default value: flush               gc.remove.in.use.candidates      ExperimentalAvailable since: 2.1.3GC will remove deletion candidates that are in-use from the metadata location. This is expected to increase the speed of subsequent GC runs.type: BOOLEAN, zk mutable: yes, default value: false               gc.safemode      Available since: 2.1.0Provides listing of files to be deleted but does not delete any files.type: BOOLEAN, zk mutable: yes, default value: false               gc.threads.delete      Available since: 1.3.5The number of threads used to delete RFiles and write-ahead logs.type: COUNT, zk mutable: yes, default value: 16               gc.trace.percent      Available since: 1.7.0Deprecated since: 2.1.0Percent of gc cycles to trace.type: FRACTION, zk mutable: yes, default value: 0.01               gc.trash.ignore      Available since: 1.5.0Deprecated since: 2.1.1Do not use the Trash, even if it is configured.type: BOOLEAN, zk mutable: yes, default value: false               general.*      Available since: 1.3.5Properties in this category affect the behavior of accumulo overall, but do not have to be consistent throughout a cloud.               general.classpaths      Available since: 1.3.5Deprecated since: 2.0.0The class path should instead be configured by the launch environment (for example, accumulo-env.sh). A list of all of the places to look for a class. Order does matter, as it will look for the jar starting in the first location to the last. Supports full regex on filename alone.type: STRING, zk mutable: no, default value: empty               general.context.class.loader.factory      Available since: 2.1.0Name of classloader factory to be used to create classloaders for named contexts, such as per-table contexts set by table.class.loader.context.type: CLASSNAME, zk mutable: no, default value: empty               general.custom.*      Available since: 2.0.0Prefix to be used for user defined system-wide properties. This may be particularly useful for system-wide configuration for various user-implementations of pluggable Accumulo features, such as the balancer or volume chooser.               general.delegation.token.lifetime      Available since: 1.7.0The length of time that delegation tokens and secret keys are valid.type: TIMEDURATION, zk mutable: no, default value: 7d               general.delegation.token.update.interval      Available since: 1.7.0The length of time between generation of new secret keys.type: TIMEDURATION, zk mutable: no, default value: 1d               general.dynamic.classpaths      Available since: 1.3.5Deprecated since: 2.0.0A list of all of the places where changes in jars or classes will force a reload of the classloader. Built-in dynamic class loading will be removed in a future version. If this is needed, consider overriding the Java system class loader with one that has this feature (https://docs.oracle.com/javase/8/docs/api/java/lang/ClassLoader.html#getSystemClassLoader–). Additionally, this property no longer does property interpolation of environment variables, such as ‘$ACCUMULO_HOME’. Use commons-configuration syntax,’${env:ACCUMULO_HOME}’ instead.type: STRING, zk mutable: no, default value: empty               general.file.name.allocation.batch.size.max      Available since: 2.1.3The maximum number of filenames that will be allocated from ZooKeeper at a time.type: COUNT, zk mutable: yes, default value: 200               general.file.name.allocation.batch.size.min      Available since: 2.1.3The minimum number of filenames that will be allocated from ZooKeeper at a time.type: COUNT, zk mutable: yes, default value: 100               general.kerberos.keytab      Available since: 1.4.1Path to the kerberos keytab to use. Leave blank if not using kerberoized hdfs.type: PATH, zk mutable: no, default value: empty               general.kerberos.principal      Available since: 1.4.1Name of the kerberos principal to use. _HOST will automatically be replaced by the machines hostname in the hostname portion of the principal. Leave blank if not using kerberoized hdfs.type: STRING, zk mutable: no, default value: empty               general.kerberos.renewal.period      Available since: 1.6.5The amount of time between attempts to perform Kerberos ticket renewals. This does not equate to how often tickets are actually renewed (which is performed at 80% of the ticket lifetime).type: TIMEDURATION, zk mutable: no, default value: 30s               general.max.scanner.retry.period      Available since: 1.7.3The maximum amount of time that a Scanner should wait before retrying a failed RPC.type: TIMEDURATION, zk mutable: no, default value: 5s               general.metrics.process.idle      Available since: 2.1.3Amount of time a process must be idle before it is considered to be idle by the metrics system.type: TIMEDURATION, zk mutable: no, default value: 5m               general.micrometer.enabled      Available since: 2.1.0Enables metrics functionality using Micrometer.type: BOOLEAN, zk mutable: no, default value: false               general.micrometer.factory      Available since: 2.1.0A comma separated list of one or more class names that implements org.apache.accumulo.core.spi.metrics.MeterRegistryFactory. Prior to 2.1.3 this was a single value and the default was an empty string.  In 2.1.3 the default was changed and it now can accept multiple class names. The metrics spi was introduced in 2.1.3, the deprecated factory is org.apache.accumulo.core.metrics.MeterRegistryFactory.type: CLASSNAMELIST, zk mutable: no, default value: org.apache.accumulo.core.spi.metrics.LoggingMeterRegistryFactory               general.micrometer.jvm.metrics.enabled      Available since: 2.1.0Enables JVM metrics functionality using Micrometer.type: BOOLEAN, zk mutable: no, default value: false               general.opentelemetry.enabled      ExperimentalAvailable since: 2.1.0Enables tracing functionality using OpenTelemetry (assuming OpenTelemetry is configured).type: BOOLEAN, zk mutable: no, default value: false               general.rpc.server.type      ExperimentalAvailable since: 1.7.0Type of Thrift server to instantiate, see org.apache.accumulo.server.rpc.ThriftServerType for more information. Only useful for benchmarking thrift servers.type: STRING, zk mutable: no, default value: empty               general.rpc.timeout      Available since: 1.3.5Time to wait on I/O for simple, short RPC calls.type: TIMEDURATION, zk mutable: no, default value: 120s               general.security.credential.provider.paths      Available since: 1.6.1Comma-separated list of paths to CredentialProviders.type: STRING, zk mutable: no, default value: empty               general.server.message.size.max      Available since: 1.5.0Deprecated since: 2.1.3Replaced by: rpc.message.size.maxThe maximum size of a message that can be sent to a server.type: BYTES, zk mutable: no, default value: 1G               general.server.simpletimer.threadpool.size      Available since: 1.7.0Deprecated since: 2.1.0Replaced by: general.server.threadpool.sizeThe number of threads to use for server-internal scheduled tasks.type: COUNT, zk mutable: no, default value: 1               general.server.threadpool.size      Available since: 2.1.0The number of threads to use for server-internal scheduled tasks.type: COUNT, zk mutable: no, default value: 1               general.vfs.cache.dir      Available since: 1.5.0Deprecated since: 2.1.0The base directory to use for the vfs cache. The actual cached files will be located in a subdirectory, accumulo-vfs-cache-&amp;lt;jvmProcessName&amp;gt;-${user.name}, where &amp;lt;jvmProcessName&amp;gt; is determined by the JVM’s internal management engine. The cache will keep a soft reference to all of the classes loaded in the VM. This should be on local disk on each node with sufficient space.type: ABSOLUTEPATH, zk mutable: no, default value: ${java.io.tmpdir}               general.vfs.classpaths      Available since: 1.5.0Deprecated since: 2.1.0Configuration for a system level vfs classloader. Accumulo jar can be configured here and loaded out of HDFS.type: STRING, zk mutable: no, default value: empty               general.vfs.context.classpath.*      Available since: 1.5.0Deprecated since: 2.1.0Properties in this category are define a classpath. These properties start  with the category prefix, followed by a context name. The value is a comma separated list of URIs. Supports full regex on filename alone. For example, general.vfs.context.classpath.cx1=hdfs://nn1:9902/mylibdir/*.jar. You can enable post delegation for a context, which will load classes from the context first instead of the parent first. Do this by setting general.vfs.context.classpath.&amp;lt;name&amp;gt;.delegation=post, where &amp;lt;name&amp;gt; is your context name. If delegation is not specified, it defaults to loading from parent classloader first.               general.volume.chooser      ExperimentalAvailable since: 1.6.0The class that will be used to select which volume will be used to create new files.type: CLASSNAME, zk mutable: no, default value: org.apache.accumulo.core.spi.fs.RandomVolumeChooser               instance.*      Available since: 1.3.5Properties in this category must be consistent throughout a cloud. This is enforced and servers won’t be able to communicate if these differ.               instance.crypto.opts.*      ExperimentalAvailable since: 2.0.0Properties related to on-disk file encryption.               instance.crypto.opts.factory      ExperimentalAvailable since: 2.1.0The class which provides crypto services for on-disk file encryption. The default does nothing. To enable encryption, replace this classname with an implementation of theorg.apache.accumulo.core.spi.crypto.CryptoFactory interface.type: CLASSNAME, zk mutable: no, default value: org.apache.accumulo.core.spi.crypto.NoCryptoServiceFactory               instance.crypto.opts.sensitive.*      ExperimentalAvailable since: 2.0.0Sensitive properties related to on-disk file encryption.               instance.dfs.dir      Available since: 1.3.5Deprecated since: 1.6.0Replaced by: instance.volumesHDFS directory in which accumulo instance will run. Do not change after accumulo is initialized.type: ABSOLUTEPATH, zk mutable: no, default value: /accumulo               instance.dfs.uri      Available since: 1.4.0Deprecated since: 1.6.0Replaced by: instance.volumesA url accumulo should use to connect to DFS. If this is empty, accumulo will obtain this information from the hadoop configuration. This property will only be used when creating new files if instance.volumes is empty. After an upgrade to 1.6.0 Accumulo will start using absolute paths to reference files. Files created before a 1.6.0 upgrade are referenced via relative paths. Relative paths will always be resolved using this config (if empty using the hadoop config).type: URI, zk mutable: no, default value: empty               instance.rpc.sasl.allowed.host.impersonation      Available since: 1.7.1One-line configuration property controlling the network locations (hostnames) that are allowed to impersonate other users.type: STRING, zk mutable: no, default value: empty               instance.rpc.sasl.allowed.user.impersonation      Available since: 1.7.1One-line configuration property controlling what users are allowed to impersonate other users.type: STRING, zk mutable: no, default value: empty               instance.rpc.sasl.enabled      Available since: 1.7.0Configures Thrift RPCs to require SASL with GSSAPI which supports Kerberos authentication. Mutually exclusive with SSL RPC configuration.type: BOOLEAN, zk mutable: no, default value: false               instance.rpc.ssl.clientAuth      Available since: 1.6.0Require clients to present certs signed by a trusted root.type: BOOLEAN, zk mutable: no, default value: false               instance.rpc.ssl.enabled      Available since: 1.6.0Use SSL for socket connections from clients and among accumulo services. Mutually exclusive with SASL RPC configuration.type: BOOLEAN, zk mutable: no, default value: false               instance.secret      Available since: 1.3.5A secret unique to a given instance that all servers must know in order to communicate with one another. It should be changed prior to the initialization of Accumulo. To change it after Accumulo has been initialized, use the ChangeSecret tool and then update accumulo.properties everywhere. Before using the ChangeSecret tool, make sure Accumulo is not running and you are logged in as the user that controls Accumulo files in HDFS. To use the ChangeSecret tool, run the command: ./bin/accumulo org.apache.accumulo.server.util.ChangeSecret.type: STRING, zk mutable: no, default value: DEFAULT               instance.security.authenticator      ExperimentalAvailable since: 1.5.0The authenticator class that accumulo will use to determine if a user has privilege to perform an action.type: CLASSNAME, zk mutable: no, default value: org.apache.accumulo.server.security.handler.ZKAuthenticator               instance.security.authorizor      ExperimentalAvailable since: 1.5.0The authorizor class that accumulo will use to determine what labels a user has privilege to see.type: CLASSNAME, zk mutable: no, default value: org.apache.accumulo.server.security.handler.ZKAuthorizor               instance.security.permissionHandler      ExperimentalAvailable since: 1.5.0The permission handler class that accumulo will use to determine if a user has privilege to perform an action.type: CLASSNAME, zk mutable: no, default value: org.apache.accumulo.server.security.handler.ZKPermHandler               instance.volume.config.*      Available since: 2.1.1Properties in this category are used to provide volume specific overrides to the general filesystem client configuration. Properties using this prefix should be in the form ‘instance.volume.config..=. An example: &#39;instance.volume.config.hdfs://namespace-a:8020/accumulo.dfs.client.hedged.read.threadpool.size=10&#39;. Note that when specifying property names that contain colons in the properties files that the colons need to be escaped with a backslash.               instance.volumes      Available since: 1.6.0A comma separated list of dfs uris to use. Files will be stored across these filesystems. In some situations, the first volume in this list may be treated differently, such as being preferred for writing out temporary files (for example, when creating a pre-split table). After adding uris to this list, run ‘accumulo init –add-volume’ and then restart tservers. If entries are removed from this list then tservers will need to be restarted. After a uri is removed from the list Accumulo will not create new files in that location, however Accumulo can still reference files created at that location before the config change. To use a comma or other reserved characters in a URI use standard URI hex encoding. For example replace commas with %2C.type: STRING, zk mutable: no, default value: empty               instance.volumes.replacements      Available since: 1.6.0Since accumulo stores absolute URIs changing the location of a namenode could prevent Accumulo from starting. The property helps deal with that situation. Provide a comma separated list of uri replacement pairs here if a namenode location changes. Each pair should be separated with a space. For example, if hdfs://nn1 was replaced with hdfs://nnA and hdfs://nn2 was replaced with hdfs://nnB, then set this property to ‘hdfs://nn1 hdfs://nnA,hdfs://nn2 hdfs://nnB’ Replacements must be configured for use. To see which volumes are currently in use, run ‘accumulo admin volumes -l’. To use a comma or other reserved characters in a URI use standard URI hex encoding. For example replace commas with %2C.type: STRING, zk mutable: no, default value: empty               instance.volumes.upgrade.relative      Available since: 2.1.0The volume dfs uri containing relative tablet file paths. Relative paths may exist in the metadata from versions prior to 1.6. This property is only required if a relative path is detected during the upgrade process and will only be used once.type: STRING, zk mutable: no, default value: empty               instance.zookeeper.host      Available since: 1.3.5Comma separated list of zookeeper servers.type: HOSTLIST, zk mutable: no, default value: localhost:2181               instance.zookeeper.timeout      Available since: 1.3.5Zookeeper session timeout; max value when represented as milliseconds should be no larger than 2147483647.type: TIMEDURATION, zk mutable: no, default value: 30s               manager.*      Available since: 2.1.0Properties in this category affect the behavior of the manager server. Since 2.1.0, all properties in this category replace the old master.* names.               manager.bulk.rename.threadpool.size      Available since: 2.1.0 (since 1.7.0 as master.bulk.rename.threadpool.size)Deprecated since: 2.1.0Replaced by: manager.rename.threadpool.sizeThe number of threads to use when moving user files to bulk ingest directories under accumulo control.type: COUNT, zk mutable: yes, default value: 20               manager.bulk.retries      Available since: 2.1.0 (since 1.4.0 as master.bulk.retries)The number of attempts to bulk import a RFile before giving up.type: COUNT, zk mutable: yes, default value: 3               manager.bulk.threadpool.size      Available since: 2.1.0 (since 1.4.0 as master.bulk.threadpool.size)The number of threads to use when coordinating a bulk import.type: COUNT, zk mutable: yes, default value: 5               manager.bulk.threadpool.timeout      Available since: 2.1.0The time after which bulk import threads terminate with no work available.  Zero (0) will keep the threads alive indefinitely.type: TIMEDURATION, zk mutable: yes, default value: 0s               manager.bulk.timeout      Available since: 2.1.0 (since 1.4.3 as master.bulk.timeout)The time to wait for a tablet server to process a bulk import request.type: TIMEDURATION, zk mutable: yes, default value: 5m               manager.bulk.tserver.regex      Available since: 2.1.0 (since 2.0.0 as master.bulk.tserver.regex)Regular expression that defines the set of Tablet Servers that will perform bulk imports.type: STRING, zk mutable: yes, default value: empty               manager.fate.metrics.min.update.interval      Available since: 2.1.0 (since 1.9.3 as master.fate.metrics.min.update.interval)Limit calls from metric sinks to zookeeper to update interval.type: TIMEDURATION, zk mutable: yes, default value: 60s               manager.fate.threadpool.size      Available since: 2.1.0 (since 1.4.3 as master.fate.threadpool.size)The number of threads used to run fault-tolerant executions (FATE). These are primarily table operations like merge.type: COUNT, zk mutable: yes, default value: 4               manager.lease.recovery.interval      Available since: 2.1.0 (since 1.5.0 as master.lease.recovery.interval)The amount of time to wait after requesting a write-ahead log to be recovered.type: TIMEDURATION, zk mutable: yes, default value: 5s               manager.metadata.suspendable      Available since: 2.1.0 (since 1.8.0 as master.metadata.suspendable)Allow tablets for the accumulo.metadata table to be suspended via table.suspend.duration.type: BOOLEAN, zk mutable: yes, default value: false               manager.port.client      Available since: 2.1.0 (since 1.3.5 as master.port.client)The port used for handling client connections on the manager.type: PORT, zk mutable: yes but requires restart of the manager, default value: 9999               manager.recovery.delay      Available since: 2.1.0 (since 1.5.0 as master.recovery.delay)When a tablet server’s lock is deleted, it takes time for it to completely quit. This delay gives it time before log recoveries begin.type: TIMEDURATION, zk mutable: yes, default value: 10s               manager.recovery.wal.cache.time      Available since: 2.1.2Amount of time that the existence of recovery write-ahead logs is cached.type: TIMEDURATION, zk mutable: yes but requires restart of the manager, default value: 15s               manager.rename.threadpool.size      Available since: 2.1.0The number of threads to use when renaming user files during table import or bulk ingest.type: COUNT, zk mutable: yes, default value: 20               manager.replication.coordinator.minthreads      Available since: 2.1.0 (since 1.7.0 as master.replication.coordinator.minthreads)Deprecated since: 2.1.0Minimum number of threads dedicated to answering coordinator requests.type: COUNT, zk mutable: yes, default value: 4               manager.replication.coordinator.port      Available since: 2.1.0 (since 1.7.0 as master.replication.coordinator.port)Deprecated since: 2.1.0Port for the replication coordinator service.type: PORT, zk mutable: yes, default value: 10001               manager.replication.coordinator.threadcheck.time      Available since: 2.1.0 (since 1.7.0 as master.replication.coordinator.threadcheck.time)Deprecated since: 2.1.0The time between adjustments of the coordinator thread pool.type: TIMEDURATION, zk mutable: yes, default value: 5s               manager.replication.status.scan.interval      Available since: 2.1.0 (since 1.7.0 as master.replication.status.scan.interval)Deprecated since: 2.1.0Amount of time to sleep before scanning the status section of the replication table for new data.type: TIMEDURATION, zk mutable: yes, default value: 30s               manager.server.threadcheck.time      Available since: 2.1.0 (since 1.4.0 as master.server.threadcheck.time)The time between adjustments of the server thread pool.type: TIMEDURATION, zk mutable: yes, default value: 1s               manager.server.threads.minimum      Available since: 2.1.0 (since 1.4.0 as master.server.threads.minimum)The minimum number of threads to use to handle incoming requests.type: COUNT, zk mutable: yes but requires restart of the manager, default value: 20               manager.server.threads.timeout      Available since: 2.1.0The time after which incoming request threads terminate with no work available.  Zero (0) will keep the threads alive indefinitely.type: TIMEDURATION, zk mutable: yes but requires restart of the manager, default value: 0s               manager.startup.tserver.avail.max.wait      Available since: 2.1.0 (since 1.10.0 as master.startup.tserver.avail.max.wait)Maximum time manager will wait for tserver available threshold to be reached before continuing. When set to 0 or less, will block indefinitely. Default is 0 to block indefinitely. Only valid when tserver available threshold is set greater than 0.type: TIMEDURATION, zk mutable: yes, default value: 0               manager.startup.tserver.avail.min.count      Available since: 2.1.0 (since 1.10.0 as master.startup.tserver.avail.min.count)Minimum number of tservers that need to be registered before manager will start tablet assignment - checked at manager initialization, when manager gets lock.  When set to 0 or less, no blocking occurs. Default is 0 (disabled) to keep original  behaviour.type: COUNT, zk mutable: yes, default value: 0               manager.status.threadpool.size      Available since: 2.1.0 (since 1.8.0 as master.status.threadpool.size)The number of threads to use when fetching the tablet server status for balancing.  Zero indicates an unlimited number of threads will be used.type: COUNT, zk mutable: yes, default value: 0               manager.tablet.balancer      Available since: 2.1.0 (since 1.3.5 as master.tablet.balancer)The balancer class that accumulo will use to make tablet assignment and migration decisions.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.spi.balancer.TableLoadBalancer               manager.tablet.watcher.interval      Available since: 2.1.2Time to wait between scanning tablet states to identify tablets that need to be assigned, un-assigned, migrated, etc.type: TIMEDURATION, zk mutable: yes, default value: 60s               manager.wal.closer.implementation      Available since: 2.1.0A class that implements a mechanism to steal write access to a write-ahead log.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.server.manager.recovery.HadoopLogCloser               manager.walog.closer.implementation      Available since: 2.1.0 (since 1.5.0 as master.walog.closer.implementation)Deprecated since: 2.1.0Replaced by: manager.wal.closer.implementationA class that implements a mechanism to steal write access to a write-ahead log.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.server.manager.recovery.HadoopLogCloser               master.*      Available since: 1.3.5Deprecated since: 2.1.0Replaced by: manager.Properties in this category affect the behavior of the manager (formerly named master) server. Since 2.1.0, all properties in this category are deprecated and replaced with corresponding manager.* properties. The old master.* names can still be used until at release 3.0, but a warning will be emitted. Configuration files should be updated to use the new property names.               monitor.*      Available since: 1.3.5Properties in this category affect the behavior of the monitor web server.               monitor.lock.check.interval      Available since: 1.5.1The amount of time to sleep between checking for the Monitor ZooKeeper lock.type: TIMEDURATION, zk mutable: no, default value: 5s               monitor.port.client      Available since: 1.3.5The listening port for the monitor’s http service.type: PORT, zk mutable: no, default value: 9995               monitor.resources.external      Available since: 2.0.0A JSON Map of Strings. Each String should be an HTML tag of an external resource (JS or CSS) to be imported by the Monitor. Be sure to wrap with CDATA tags. If this value is set, all of the external resources in the &amp;lt;head&amp;gt; tag of the Monitor will be replaced with the tags set here. Be sure the jquery tag is first since other scripts will depend on it. The resources that are used by default can be seen in accumulo/server/monitor/src/main/resources/templates/default.ftl.type: JSON, zk mutable: no, default value: empty               monitor.ssl.exclude.ciphers      Available since: 1.6.1A comma-separated list of disallowed SSL Ciphers, see monitor.ssl.include.ciphers to allow ciphers.type: STRING, zk mutable: no, default value: empty               monitor.ssl.include.ciphers      Available since: 1.6.1A comma-separated list of allows SSL Ciphers, see monitor.ssl.exclude.ciphers to disallow ciphers.type: STRING, zk mutable: no, default value: empty               monitor.ssl.include.protocols      Available since: 1.5.3A comma-separate list of allowed SSL protocols.type: STRING, zk mutable: no, default value: TLSv1.2               monitor.ssl.keyPassword      Available since: 1.9.3Optional: the password for the private key in the keyStore. When not provided, this defaults to the keystore password.type: STRING, zk mutable: no, default value: empty               monitor.ssl.keyStore      Available since: 1.5.0The keystore for enabling monitor SSL.type: PATH, zk mutable: no, default value: empty               monitor.ssl.keyStorePassword      Available since: 1.5.0The keystore password for enabling monitor SSL.type: STRING, zk mutable: no, default value: empty               monitor.ssl.keyStoreType      Available since: 1.7.0Type of SSL keystore.type: STRING, zk mutable: no, default value: jks               monitor.ssl.trustStore      Available since: 1.5.0The truststore for enabling monitor SSL.type: PATH, zk mutable: no, default value: empty               monitor.ssl.trustStorePassword      Available since: 1.5.0The truststore password for enabling monitor SSL.type: STRING, zk mutable: no, default value: empty               monitor.ssl.trustStoreType      Available since: 1.7.0Type of SSL truststore.type: STRING, zk mutable: no, default value: jks               replication.*      Available since: 1.7.0Deprecated since: 2.1.0Properties in this category affect the replication of data to other Accumulo instances.               replication.driver.delay      Available since: 1.7.0Deprecated since: 2.1.0Amount of time to wait before the replication work loop begins in the manager.type: TIMEDURATION, zk mutable: yes, default value: 0s               replication.max.unit.size      Available since: 1.7.0Deprecated since: 2.1.0Maximum size of data to send in a replication message.type: BYTES, zk mutable: yes, default value: 64M               replication.max.work.queue      Available since: 1.7.0Deprecated since: 2.1.0Upper bound of the number of files queued for replication.type: COUNT, zk mutable: yes, default value: 1000               replication.name      Available since: 1.7.0Deprecated since: 2.1.0Name of this cluster with respect to replication. Used to identify this instance from other peers.type: STRING, zk mutable: yes, default value: empty               replication.peer.*      Available since: 1.7.0Deprecated since: 2.1.0Properties in this category control what systems data can be replicated to.               replication.peer.keytab.*      Available since: 1.7.0Deprecated since: 2.1.0The keytab to use when authenticating with the given peer.               replication.peer.password.*      Available since: 1.7.0Deprecated since: 2.1.0The password to provide when authenticating with the given peer.               replication.peer.user.*      Available since: 1.7.0Deprecated since: 2.1.0The username to provide when authenticating with the given peer.               replication.receipt.service.port      Available since: 1.7.0Deprecated since: 2.1.0Listen port used by thrift service in tserver listening for replication.type: PORT, zk mutable: yes, default value: 10002               replication.receiver.min.threads      Available since: 1.7.0Deprecated since: 2.1.0Minimum number of threads for replication.type: COUNT, zk mutable: yes, default value: 1               replication.receiver.threadcheck.time      Available since: 1.7.0Deprecated since: 2.1.0The time between adjustments of the replication thread pool.type: TIMEDURATION, zk mutable: yes, default value: 30s               replication.rpc.timeout      Available since: 1.7.4Deprecated since: 2.1.0Amount of time for a single replication RPC call to last before failing the attempt. See replication.work.attempts.type: TIMEDURATION, zk mutable: yes, default value: 2m               replication.trace.percent      Available since: 1.7.0Deprecated since: 2.1.0The sampling percentage to use for replication traces.type: FRACTION, zk mutable: yes, default value: 0.1               replication.work.assigner      Available since: 1.7.0Deprecated since: 2.1.0Replication WorkAssigner implementation to use.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.manager.replication.UnorderedWorkAssigner               replication.work.assignment.sleep      Available since: 1.7.0Deprecated since: 2.1.0Amount of time to sleep between replication work assignment.type: TIMEDURATION, zk mutable: yes, default value: 30s               replication.work.attempts      Available since: 1.7.0Deprecated since: 2.1.0Number of attempts to try to replicate some data before giving up and letting it naturally be retried later.type: COUNT, zk mutable: yes, default value: 10               replication.work.processor.delay      Available since: 1.7.0Deprecated since: 2.1.0Amount of time to wait before first checking for replication work, not useful outside of tests.type: TIMEDURATION, zk mutable: yes, default value: 0s               replication.work.processor.period      Available since: 1.7.0Deprecated since: 2.1.0Amount of time to wait before re-checking for replication work, not useful outside of tests.type: TIMEDURATION, zk mutable: yes, default value: 0s               replication.worker.threads      Available since: 1.7.0Deprecated since: 2.1.0Size of the threadpool that each tabletserver devotes to replicating data.type: COUNT, zk mutable: yes, default value: 4               rpc.*      Available since: 1.6.0Properties in this category related to the configuration of SSL keys for RPC. See also instance.ssl.enabled.               rpc.backlog      Available since: 2.1.3Configures the TCP backlog for the server side sockets created by Thrift. This property is not used for SSL type server sockets. A value of zero will use the Thrift default value.type: COUNT, zk mutable: no, default value: 50               rpc.javax.net.ssl.keyStore      Available since: 1.6.0Path of the keystore file for the server’s private SSL key.type: PATH, zk mutable: no, default value: empty               rpc.javax.net.ssl.keyStorePassword      Available since: 1.6.0Password used to encrypt the SSL private keystore. Leave blank to use the Accumulo instance secret.type: STRING, zk mutable: no, default value: empty               rpc.javax.net.ssl.keyStoreType      Available since: 1.6.0Type of SSL keystore.type: STRING, zk mutable: no, default value: jks               rpc.javax.net.ssl.trustStore      Available since: 1.6.0Path of the truststore file for the root cert.type: PATH, zk mutable: no, default value: empty               rpc.javax.net.ssl.trustStorePassword      Available since: 1.6.0Password used to encrypt the SSL truststore. Leave blank to use no password.type: STRING, zk mutable: no, default value: empty               rpc.javax.net.ssl.trustStoreType      Available since: 1.6.0Type of SSL truststore.type: STRING, zk mutable: no, default value: jks               rpc.message.size.max      Available since: 2.1.3The maximum size of a message that can be received by a server.type: BYTES, zk mutable: no, default value: 2147483647               rpc.sasl.qop      Available since: 1.7.0The quality of protection to be used with SASL. Valid values are ‘auth’, ‘auth-int’, and ‘auth-conf’.type: STRING, zk mutable: no, default value: auth               rpc.ssl.cipher.suites      Available since: 1.6.1Comma separated list of cipher suites that can be used by accepted connections.type: STRING, zk mutable: no, default value: empty               rpc.ssl.client.protocol      Available since: 1.6.2The protocol used to connect to a secure server. Must be in the list of enabled protocols on the server side rpc.ssl.server.enabled.protocols.type: STRING, zk mutable: no, default value: TLSv1.2               rpc.ssl.server.enabled.protocols      Available since: 1.6.2Comma separated list of protocols that can be used to accept connections.type: STRING, zk mutable: no, default value: TLSv1.2               rpc.useJsse      Available since: 1.6.0Use JSSE system properties to configure SSL rather than the rpc.javax.net.ssl.* Accumulo properties.type: BOOLEAN, zk mutable: no, default value: false               sserver.*      ExperimentalAvailable since: 2.1.0Properties in this category affect the behavior of the scan servers.               sserver.cache.data.size      ExperimentalAvailable since: 2.1.0Specifies the size of the cache for RFile data blocks on each scan server.type: MEMORY, zk mutable: yes but requires restart of the sserver, default value: 10%               sserver.cache.index.size      ExperimentalAvailable since: 2.1.0Specifies the size of the cache for RFile index blocks on each scan server.type: MEMORY, zk mutable: yes but requires restart of the sserver, default value: 25%               sserver.cache.metadata.expiration      ExperimentalAvailable since: 2.1.0The time after which cached tablet metadata will be expired if not previously refreshed.type: TIMEDURATION, zk mutable: yes but requires restart of the sserver, default value: 5m               sserver.cache.metadata.refresh.percent      ExperimentalAvailable since: 2.1.3The time after which cached tablet metadata will be refreshed, expressed as a percentage of the expiration time. Cache hits after this time, but before the expiration time, will trigger a background refresh for future hits. Value must be less than 100%. Set to 0 will disable refresh.type: FRACTION, zk mutable: yes, default value: .75               sserver.cache.summary.size      ExperimentalAvailable since: 2.1.0Specifies the size of the cache for summary data on each scan server.type: MEMORY, zk mutable: yes but requires restart of the sserver, default value: 10%               sserver.default.blocksize      ExperimentalAvailable since: 2.1.0Specifies a default blocksize for the scan server caches.type: BYTES, zk mutable: yes but requires restart of the sserver, default value: 1M               sserver.port.client      ExperimentalAvailable since: 2.1.0The port used for handling client connections on the tablet servers.type: PORT, zk mutable: yes but requires restart of the sserver, default value: 9996               sserver.port.search      ExperimentalAvailable since: 2.1.0if the ports above are in use, search higher ports until one is available.type: BOOLEAN, zk mutable: yes but requires restart of the sserver, default value: true               sserver.scan.executors.*      ExperimentalAvailable since: 2.1.0Prefix for defining executors to service scans. See scan executors for an overview of why and how to use this property. For each executor the number of threads, thread priority, and an optional prioritizer can be configured. To configure a new executor, set sserver.scan.executors.&amp;lt;name&amp;gt;.threads=&amp;lt;number&amp;gt;.  Optionally, can also set sserver.scan.executors.&amp;lt;name&amp;gt;.priority=&amp;lt;number 1 to 10&amp;gt;, sserver.scan.executors.&amp;lt;name&amp;gt;.prioritizer=&amp;lt;class name&amp;gt;, and sserver.scan.executors.&amp;lt;name&amp;gt;.prioritizer.opts.&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;.               sserver.scan.executors.default.prioritizer      ExperimentalAvailable since: 2.1.0Prioritizer for the default scan executor.  Defaults to none which results in FIFO priority.  Set to a class that implements org.apache.accumulo.core.spi.scan.ScanPrioritizer to configure one.type: STRING, zk mutable: yes, default value: empty               sserver.scan.executors.default.threads      ExperimentalAvailable since: 2.1.0The number of threads for the scan executor that tables use by default.type: COUNT, zk mutable: yes, default value: 16               sserver.scan.executors.meta.threads      ExperimentalAvailable since: 2.1.0The number of threads for the metadata table scan executor.type: COUNT, zk mutable: yes, default value: 8               sserver.scan.reference.expiration      ExperimentalAvailable since: 2.1.0The amount of time a scan reference is unused before its deleted from metadata table.type: TIMEDURATION, zk mutable: yes but requires restart of the sserver, default value: 5m               sserver.server.threadcheck.time      ExperimentalAvailable since: 2.1.0The time between adjustments of the thrift server thread pool.type: TIMEDURATION, zk mutable: yes, default value: 1s               sserver.server.threads.minimum      ExperimentalAvailable since: 2.1.0The minimum number of threads to use to handle incoming requests.type: COUNT, zk mutable: yes but requires restart of the sserver, default value: 2               sserver.server.threads.timeout      ExperimentalAvailable since: 2.1.0The time after which incoming request threads terminate with no work available.  Zero (0) will keep the threads alive indefinitely.type: TIMEDURATION, zk mutable: yes but requires restart of the sserver, default value: 0s               table.*      Available since: 1.3.5Properties in this category affect tablet server treatment of tablets, but can be configured on a per-table basis. Setting these properties in accumulo.properties will override the default globally for all tables and not any specific table. However, both the default and the global setting can be overridden per table using the table operations API or in the shell, which sets the overridden value in zookeeper. Restarting accumulo tablet servers after setting these properties in accumulo.properties will cause the global setting to take effect. However, you must use the API or the shell to change properties in zookeeper that are set on a table.               table.balancer      Available since: 1.3.5This property can be set to allow the LoadBalanceByTable load balancer to change the called Load Balancer for this table.type: STRING, zk mutable: yes, default value: org.apache.accumulo.core.spi.balancer.SimpleLoadBalancer               table.bloom.enabled      Available since: 1.3.5Use bloom filters on this table.type: BOOLEAN, zk mutable: yes, default value: false               table.bloom.error.rate      Available since: 1.3.5Bloom filter error rate.type: FRACTION, zk mutable: yes, default value: 0.5%               table.bloom.hash.type      Available since: 1.3.5The bloom filter hash type.type: STRING, zk mutable: yes, default value: murmur               table.bloom.key.functor      Available since: 1.3.5A function that can transform the key prior to insertion and check of bloom filter. org.apache.accumulo.core.file.keyfunctor.RowFunctor, org.apache.accumulo.core.file.keyfunctor.ColumnFamilyFunctor, and org.apache.accumulo.core.file.keyfunctor.ColumnQualifierFunctor are allowable values. One can extend any of the above mentioned classes to perform specialized parsing of the key.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.file.keyfunctor.RowFunctor               table.bloom.load.threshold      Available since: 1.3.5This number of seeks that would actually use a bloom filter must occur before a RFile’s bloom filter is loaded. Set this to zero to initiate loading of bloom filters when a RFile is opened.type: COUNT, zk mutable: yes, default value: 1               table.bloom.size      Available since: 1.3.5Bloom filter size, as number of keys.type: COUNT, zk mutable: yes, default value: 1048576               table.bulk.max.tablets      Available since: 2.1.0The maximum number of tablets allowed for one bulk import file. Value of 0 is Unlimited. This property is only enforced in the new bulk import API.type: COUNT, zk mutable: yes, default value: 0               table.cache.block.enable      Available since: 1.3.5Determines whether data block cache is enabled for a table.type: BOOLEAN, zk mutable: yes, default value: false               table.cache.index.enable      Available since: 1.3.5Determines whether index block cache is enabled for a table.type: BOOLEAN, zk mutable: yes, default value: true               table.class.loader.context      Available since: 2.1.0The context to use for loading per-table resources, such as iterators from the configured factory in general.context.class.loader.factory.type: STRING, zk mutable: yes, default value: empty               table.classpath.context      Available since: 1.5.0Deprecated since: 2.1.0Replaced by: table.class.loader.contextPer table classpath context.type: STRING, zk mutable: yes, default value: empty               table.compaction.configurer      Available since: 2.1.0A plugin that can dynamically configure compaction output files based on input files.type: CLASSNAME, zk mutable: yes, default value: empty               table.compaction.configurer.opts.*      Available since: 2.1.0Options for the table compaction configuror.               table.compaction.dispatcher      Available since: 2.1.0A configurable dispatcher that decides what compaction service a table should use.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.spi.compaction.SimpleCompactionDispatcher               table.compaction.dispatcher.opts.*      Available since: 2.1.0Options for the table compaction dispatcher.               table.compaction.major.everything.idle      Available since: 1.3.5Deprecated since: 2.1.0After a tablet has been idle (no mutations) for this time period it may have all of its RFiles compacted into one. There is no guarantee an idle tablet will be compacted. Compactions of idle tablets are only started when regular compactions are not running. Idle compactions only take place for tablets that have one or more RFiles.type: TIMEDURATION, zk mutable: yes, default value: 1h               table.compaction.major.output.drop.cache      Available since: 2.1.1Setting this property to true will callFSDataOutputStream.setDropBehind(true) on the major compaction output stream.type: BOOLEAN, zk mutable: yes, default value: false               table.compaction.major.ratio      Available since: 1.3.5Minimum ratio of total input size to maximum input RFile size for running a major compaction.type: FRACTION, zk mutable: yes, default value: 3               table.compaction.minor.idle      Available since: 1.3.5After a tablet has been idle (no mutations) for this time period it may have its in-memory map flushed to disk in a minor compaction. There is no guarantee an idle tablet will be compacted.type: TIMEDURATION, zk mutable: yes, default value: 5m               table.compaction.minor.logs.threshold      Available since: 1.3.5Deprecated since: 2.0.0Replaced by: tserver.wal.max.referencedThis property is deprecated and replaced.type: COUNT, zk mutable: yes, default value: 3               table.compaction.minor.output.drop.cache      Available since: 2.1.1Setting this property to true will callFSDataOutputStream.setDropBehind(true) on the minor compaction output stream.type: BOOLEAN, zk mutable: yes, default value: false               table.compaction.selection.expiration.ms      Available since: 2.1.0User compactions select files and are then queued for compaction, preventing these files from being used in system compactions.  This timeout allows system compactions to cancel the hold queued user compactions have on files, when its queued for more than the specified time.  If a system compaction cancels a hold and runs, then the user compaction can reselect and hold files after the system compaction runs.type: TIMEDURATION, zk mutable: yes, default value: 2m               table.compaction.selector      Available since: 2.1.0A configurable selector for a table that can periodically select file for mandatory compaction, even if the files do not meet the compaction ratio.type: CLASSNAME, zk mutable: yes, default value: empty               table.compaction.selector.opts.*      Available since: 2.1.0Options for the table compaction dispatcher.               table.constraint.*      Available since: 1.3.5Properties in this category are per-table properties that add constraints to a table. These properties start with the category prefix, followed by a number, and their values correspond to a fully qualified Java class that implements the Constraint interface.For example:table.constraint.1 = org.apache.accumulo.core.constraints.MyCustomConstraintand: table.constraint.2 = my.package.constraints.MySecondConstraint.               table.crypto.opts.*      ExperimentalAvailable since: 2.1.0Properties related to on-disk file encryption.               table.crypto.opts.sensitive.*      ExperimentalAvailable since: 2.1.0Sensitive properties related to on-disk file encryption.               table.custom.*      Available since: 1.7.0Prefix to be used for user defined arbitrary properties.               table.delete.behavior      ExperimentalAvailable since: 2.0.0This determines what action to take when a delete marker is seen. Valid values are process and fail with process being the default.  When set to process, deletes will suppress data.  When set to fail, any deletes seen will cause an exception. The purpose of fail is to support tables that never delete data and need fast seeks within the timestamp range of a column. When setting this to fail, also consider configuring the org.apache.accumulo.core.data.constraints.NoDeleteConstraint constraint.type: STRING, zk mutable: yes, default value: process               table.durability      Available since: 1.7.0The durability used to write to the write-ahead log. Legal values are: none, which skips the write-ahead log; log, which sends the data to the write-ahead log, but does nothing to make it durable; flush, which pushes data to the file system; and sync, which ensures the data is written to disk.type: DURABILITY, zk mutable: yes, default value: sync               table.failures.ignore      Available since: 1.3.5If you want queries for your table to hang or fail when data is missing from the system, then set this to false. When this set to true missing data will be reported but queries will still run possibly returning a subset of the data.type: BOOLEAN, zk mutable: yes, default value: false               table.file.blocksize      Available since: 1.3.5The HDFS block size used when writing RFiles. When set to 0B, the value/defaults of HDFS property ‘dfs.block.size’ will be used.type: BYTES, zk mutable: yes, default value: 0B               table.file.compress.blocksize      Available since: 1.3.5The maximum size of data blocks in RFiles before they are compressed and written.type: BYTES, zk mutable: yes, default value: 100k               table.file.compress.blocksize.index      Available since: 1.4.0The maximum size of index blocks in RFiles before they are compressed and written.type: BYTES, zk mutable: yes, default value: 128k               table.file.compress.type      Available since: 1.3.5Compression algorithm used on index and data blocks before they are written. Possible values: zstd, gz, snappy, bzip2, lzo, lz4, none.type: STRING, zk mutable: yes, default value: gz               table.file.max      Available since: 1.4.0The maximum number of RFiles each tablet in a table can have. When adjusting this property you may want to consider adjusting table.compaction.major.ratio also. Setting this property to 0 will make it default to tserver.scan.files.open.max-1, this will prevent a tablet from having more RFiles than can be opened. Prior to 2.1.0 this property was used to trigger merging minor compactions, but merging minor compactions were removed in 2.1.0. Now this property is only used by the DefaultCompactionStrategy and the DefaultCompactionPlanner. The DefaultCompactionPlanner started using this property in 2.1.3, before that it did not use the property.type: COUNT, zk mutable: yes, default value: 15               table.file.replication      Available since: 1.3.5The number of replicas for a table’s RFiles in HDFS. When set to 0, HDFS defaults are used.type: COUNT, zk mutable: yes, default value: 0               table.file.summary.maxSize      Available since: 2.0.0The maximum size summary that will be stored. The number of RFiles that had summary data exceeding this threshold is reported by Summary.getFileStatistics().getLarge(). When adjusting this consider the expected number RFiles with summaries on each tablet server and the summary cache size.type: BYTES, zk mutable: yes, default value: 256k               table.file.type      Available since: 1.3.5Change the type of file a table writes.type: FILENAME_EXT, zk mutable: yes, default value: rf               table.formatter      Available since: 1.4.0The Formatter class to apply on results in the shell.type: STRING, zk mutable: yes, default value: org.apache.accumulo.core.util.format.DefaultFormatter               table.group.*      Available since: 1.3.5Properties in this category are per-table properties that define locality groups in a table. These properties start with the category prefix, followed by a name, followed by a period, and followed by a property for that group.For example table.group.group1=x,y,z sets the column families for a group called group1. Once configured, group1 can be enabled by adding it to the list of groups in the table.groups.enabled property.Additional group options may be specified for a named group by setting table.group.&amp;lt;name&amp;gt;.opt.&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;.               table.groups.enabled      Available since: 1.3.5A comma separated list of locality group names to enable for this table.type: STRING, zk mutable: yes, default value: empty               table.interepreter      Available since: 1.5.0Deprecated since: 2.1.0The ScanInterpreter class to apply on scan arguments in the shell. Note that this property is deprecated and will be removed in a future version.type: STRING, zk mutable: yes, default value: org.apache.accumulo.core.util.interpret.DefaultScanInterpreter               table.iterator.*      Available since: 1.3.5Properties in this category specify iterators that are applied at various stages (scopes) of interaction with a table. These properties start with the category prefix, followed by a scope (minc, majc, scan, etc.), followed by a period, followed by a name, as in table.iterator.scan.vers, or table.iterator.scan.custom. The values for these properties are a number indicating the ordering in which it is applied, and a class name such as:table.iterator.scan.vers = 10,org.apache.accumulo.core.iterators.VersioningIteratorThese iterators can take options if additional properties are set that look like this property, but are suffixed with a period, followed by ‘opt’ followed by another period, and a property name.For example, table.iterator.minc.vers.opt.maxVersions = 3.               table.iterator.majc.*      Available since: 1.5.2Convenience prefix to find options for the majc iterator scope.               table.iterator.minc.*      Available since: 1.5.2Convenience prefix to find options for the minc iterator scope.               table.iterator.scan.*      Available since: 1.5.2Convenience prefix to find options for the scan iterator scope.               table.majc.compaction.strategy      Available since: 1.6.0Deprecated since: 2.1.0Replaced by: table.compaction.selectorSee org.apache.accumulo.core.spi.compaction.type: CLASSNAME, zk mutable: yes, default value: empty               table.majc.compaction.strategy.opts.*      Available since: 1.6.0Deprecated since: 2.1.0Replaced by: table.compaction.selector.opts.Properties in this category are used to configure the compaction strategy.               table.replication      Available since: 1.7.0Deprecated since: 2.1.0Is replication enabled for the given table.type: BOOLEAN, zk mutable: yes, default value: false               table.replication.target.*      Available since: 1.7.0Deprecated since: 2.1.0Enumerate a mapping of other systems which this table should replicate their data to. The key suffix is the identifying cluster name and the value is an identifier for a location on the target system, e.g. the ID of the table on the target to replicate to.               table.sampler      Available since: 1.8.0The name of a class that implements org.apache.accumulo.core.Sampler. Setting this option enables storing a sample of data which can be scanned. Always having a current sample can useful for query optimization and data comprehension. After enabling sampling for an existing table, a compaction is needed to compute the sample for existing data. The compact command in the shell has an option to only compact RFiles without sample data.type: CLASSNAME, zk mutable: yes, default value: empty               table.sampler.opt.*      Available since: 1.8.0The property is used to set options for a sampler. If a sample had two options like hasher and modulous, then the two properties table.sampler.opt.hasher=${hash algorithm} and table.sampler.opt.modulous=${mod} would be set.               table.scan.dispatcher      Available since: 2.0.0This class is used to dynamically dispatch scans to configured scan executors.  Configured classes must implement ScanDispatcher. See scan executors for an overview of why and how to use this property. This property is ignored for the root and metadata table.  The metadata table always dispatches to a scan executor named meta.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.spi.scan.SimpleScanDispatcher               table.scan.dispatcher.opts.*      Available since: 2.0.0Options for the table scan dispatcher.               table.scan.max.memory      Available since: 1.3.5The maximum amount of memory that will be used to cache results of a client query/scan. Once this limit is reached, the buffered data is sent to the client.type: BYTES, zk mutable: yes, default value: 512k               table.security.scan.visibility.default      Available since: 1.3.5The security label that will be assumed at scan time if an entry does not have a visibility expression.Note: An empty security label is displayed as []. The scan results will show an empty visibility even if the visibility from this setting is applied to the entry.CAUTION: If a particular key has an empty security label AND its table’s default visibility is also empty, access will ALWAYS be granted for users with permission to that table. Additionally, if this field is changed, all existing data with an empty visibility label will be interpreted with the new label on the next scan.type: STRING, zk mutable: yes, default value: empty               table.split.endrow.size.max      Available since: 1.7.0Maximum size of end row.type: BYTES, zk mutable: yes, default value: 10k               table.split.threshold      Available since: 1.3.5A tablet is split when the combined size of RFiles exceeds this amount.type: BYTES, zk mutable: yes, default value: 1G               table.summarizer.*      Available since: 2.0.0Prefix for configuring summarizers for a table. Using this prefix multiple summarizers can be configured with options for each one. Each summarizer configured should have a unique id, this id can be anything. To add a summarizer set table.summarizer.&amp;lt;unique id&amp;gt;=&amp;lt;summarizer class name&amp;gt;. If the summarizer has options, then for each option set table.summarizer.&amp;lt;unique id&amp;gt;.opt.&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;.               table.suspend.duration      Available since: 1.8.0For tablets belonging to this table: When a tablet server dies, allow the tablet server this duration to revive before reassigning its tablets to other tablet servers.type: TIMEDURATION, zk mutable: yes, default value: 0s               table.walog.enabled      Available since: 1.3.5Deprecated since: 1.7.0Replaced by: table.durabilityUse table.durability=none instead.type: BOOLEAN, zk mutable: yes, default value: true               trace.*      Available since: 1.3.5Deprecated since: 2.1.0Properties in this category affect the behavior of distributed tracing.               trace.password      Available since: 1.3.5Deprecated since: 2.1.0The password for the user used to store distributed traces.type: STRING, zk mutable: no, default value: secret               trace.port.client      Available since: 1.3.5Deprecated since: 2.1.0The listening port for the trace server.type: PORT, zk mutable: no, default value: 12234               trace.span.receiver.*      Available since: 1.7.0Deprecated since: 2.1.0Prefix for span receiver configuration properties.               trace.span.receivers      Available since: 1.7.0Deprecated since: 2.1.0A list of span receiver classes to send trace spans.type: CLASSNAMELIST, zk mutable: no, default value: org.apache.accumulo.tracer.ZooTraceClient               trace.table      Available since: 1.3.5Deprecated since: 2.1.0The name of the table to store distributed traces.type: STRING, zk mutable: no, default value: trace               trace.token.property.*      Available since: 1.5.0Deprecated since: 2.1.0The prefix used to create a token for storing distributed traces. For each property required by trace.token.type, place this prefix in front of it.               trace.token.type      Available since: 1.5.0Deprecated since: 2.1.0An AuthenticationToken type supported by the authorizer.type: CLASSNAME, zk mutable: no, default value: org.apache.accumulo.core.client.security.tokens.PasswordToken               trace.user      Available since: 1.3.5Deprecated since: 2.1.0The name of the user to store distributed traces.type: STRING, zk mutable: no, default value: root               trace.zookeeper.path      Available since: 1.7.0Deprecated since: 2.1.0The zookeeper node where tracers are registered.type: STRING, zk mutable: no, default value: /tracers               tserver.*      Available since: 1.3.5Properties in this category affect the behavior of the tablet servers.               tserver.assignment.concurrent.max      Available since: 1.7.0The number of threads available to load tablets. Recoveries are still performed serially.type: COUNT, zk mutable: yes, default value: 2               tserver.assignment.duration.warning      Available since: 1.6.2The amount of time an assignment can run before the server will print a warning along with the current stack trace. Meant to help debug stuck assignments.type: TIMEDURATION, zk mutable: yes, default value: 10m               tserver.bloom.load.concurrent.max      Available since: 1.3.5The number of concurrent threads that will load bloom filters in the background. Setting this to zero will make bloom filters load in the foreground.type: COUNT, zk mutable: yes, default value: 4               tserver.bulk.assign.threads      Available since: 1.4.0The manager delegates bulk import RFile processing and assignment to tablet servers. After file has been processed, the tablet server will assign the file to the appropriate tablets on all servers. This property controls the number of threads used to communicate to the other servers.type: COUNT, zk mutable: yes, default value: 1               tserver.bulk.process.threads      Available since: 1.4.0The manager will task a tablet server with pre-processing a bulk import RFile prior to assigning it to the appropriate tablet servers. This configuration value controls the number of threads used to process the files.type: COUNT, zk mutable: yes, default value: 1               tserver.bulk.retry.max      Available since: 1.4.0The number of times the tablet server will attempt to assign a RFile to a tablet as it migrates and splits.type: COUNT, zk mutable: yes, default value: 5               tserver.bulk.timeout      Available since: 1.4.3The time to wait for a tablet server to process a bulk import request.type: TIMEDURATION, zk mutable: yes, default value: 5m               tserver.cache.data.size      Available since: 1.3.5Specifies the size of the cache for RFile data blocks.type: MEMORY, zk mutable: yes but requires restart of the tserver, default value: 10%               tserver.cache.index.size      Available since: 1.3.5Specifies the size of the cache for RFile index blocks.type: MEMORY, zk mutable: yes but requires restart of the tserver, default value: 25%               tserver.cache.manager.class      Available since: 2.0.0Specifies the class name of the block cache factory implementation. Alternative implementation is org.apache.accumulo.core.file.blockfile.cache.tinylfu.TinyLfuBlockCacheManager.type: STRING, zk mutable: yes but requires restart of the tserver, default value: org.apache.accumulo.core.file.blockfile.cache.lru.LruBlockCacheManager               tserver.cache.summary.size      Available since: 2.0.0Specifies the size of the cache for summary data on each tablet server.type: MEMORY, zk mutable: yes but requires restart of the tserver, default value: 10%               tserver.client.timeout      Available since: 1.3.5Time to wait for clients to continue scans before closing a session.type: TIMEDURATION, zk mutable: yes, default value: 3s               tserver.compaction.major.concurrent.max      Available since: 1.3.5Deprecated since: 2.1.0Replaced by: tserver.compaction.major.service.default.planner.opts.executorsThe maximum number of concurrent major compactions for a tablet server.type: COUNT, zk mutable: yes, default value: 3               tserver.compaction.major.delay      Available since: 1.3.5Time a tablet server will sleep between checking which tablets need compaction.type: TIMEDURATION, zk mutable: yes, default value: 30s               tserver.compaction.major.service.*      Available since: 2.1.0Prefix for compaction services.               tserver.compaction.major.service.default.planner      Available since: 2.1.0Planner for default compaction service.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner               tserver.compaction.major.service.default.planner.opts.executors      Available since: 2.1.0See org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner.type: STRING, zk mutable: yes, default value: [{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;32M&quot;,&quot;numThreads&quot;:2},{&quot;name&quot;:&quot;medium&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;128M&quot;,&quot;numThreads&quot;:2},{&quot;name&quot;:&quot;large&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;numThreads&quot;:2}]               tserver.compaction.major.service.default.planner.opts.maxOpen      Available since: 2.1.0The maximum number of files a compaction will open.type: COUNT, zk mutable: yes, default value: 10               tserver.compaction.major.service.default.rate.limit      Available since: 2.1.0Maximum number of bytes to read or write per second over all major compactions in this compaction service, or 0B for unlimited.type: BYTES, zk mutable: yes, default value: 0B               tserver.compaction.major.service.meta.planner      Available since: 2.1.0Compaction planner for metadata table.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner               tserver.compaction.major.service.meta.planner.opts.executors      Available since: 2.1.0See org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner.type: JSON, zk mutable: yes, default value: [{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;32M&quot;,&quot;numThreads&quot;:2},{&quot;name&quot;:&quot;huge&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;numThreads&quot;:2}]               tserver.compaction.major.service.meta.planner.opts.maxOpen      Available since: 2.1.0The maximum number of files a compaction will open.type: COUNT, zk mutable: yes, default value: 30               tserver.compaction.major.service.meta.rate.limit      Available since: 2.1.0Maximum number of bytes to read or write per second over all major compactions in this compaction service, or 0B for unlimited.type: BYTES, zk mutable: yes, default value: 0B               tserver.compaction.major.service.root.planner      Available since: 2.1.0Compaction planner for root tablet service.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner               tserver.compaction.major.service.root.planner.opts.executors      Available since: 2.1.0See org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner.type: STRING, zk mutable: yes, default value: [{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;32M&quot;,&quot;numThreads&quot;:1},{&quot;name&quot;:&quot;huge&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;numThreads&quot;:1}]               tserver.compaction.major.service.root.planner.opts.maxOpen      Available since: 2.1.0The maximum number of files a compaction will open.type: COUNT, zk mutable: yes, default value: 30               tserver.compaction.major.service.root.rate.limit      Available since: 2.1.0Maximum number of bytes to read or write per second over all major compactions in this compaction service, or 0B for unlimited.type: BYTES, zk mutable: yes, default value: 0B               tserver.compaction.major.thread.files.open.max      Available since: 1.4.0Deprecated since: 2.1.0Replaced by: tserver.compaction.major.service.default.planner.opts.maxOpenMax number of RFiles a major compaction thread can open at once.type: COUNT, zk mutable: yes, default value: 10               tserver.compaction.major.throughput      Available since: 1.8.0Deprecated since: 2.1.0Replaced by: tserver.compaction.major.service.default.rate.limitMaximum number of bytes to read or write per second over all major compactions within each compaction service, or 0B for unlimited.type: BYTES, zk mutable: yes, default value: 0B               tserver.compaction.major.trace.percent      Available since: 1.7.0Deprecated since: 2.1.0The percent of major compactions to trace.type: FRACTION, zk mutable: yes, default value: 0.1               tserver.compaction.minor.concurrent.max      Available since: 1.3.5The maximum number of concurrent minor compactions for a tablet server.type: COUNT, zk mutable: yes, default value: 4               tserver.compaction.minor.trace.percent      Available since: 1.7.0Deprecated since: 2.1.0The percent of minor compactions to trace.type: FRACTION, zk mutable: yes, default value: 0.1               tserver.compaction.warn.time      Available since: 1.6.0When a compaction has not made progress for this time period, a warning will be logged.type: TIMEDURATION, zk mutable: yes, default value: 10m               tserver.default.blocksize      Available since: 1.3.5Specifies a default blocksize for the tserver caches.type: BYTES, zk mutable: yes but requires restart of the tserver, default value: 1M               tserver.dir.memdump      Available since: 1.3.5A long running scan could possibly hold memory that has been minor compacted. To prevent this, the in memory map is dumped to a local file and the scan is switched to that local file. We can not switch to the minor compacted file because it may have been modified by iterators. The file dumped to the local dir is an exact copy of what was in memory.type: PATH, zk mutable: yes, default value: /tmp               tserver.files.open.idle      Available since: 1.3.5Tablet servers leave previously used RFiles open for future queries. This setting determines how much time an unused RFile should be kept open until it is closed.type: TIMEDURATION, zk mutable: yes, default value: 1m               tserver.health.check.interval      Available since: 2.1.0The time between tablet server health checks.type: TIMEDURATION, zk mutable: yes, default value: 30m               tserver.hold.time.max      Available since: 1.4.0The maximum time for a tablet server to be in the “memory full” state. If the tablet server cannot write out memory in this much time, it will assume there is some failure local to its node, and quit. A value of zero is equivalent to forever.type: TIMEDURATION, zk mutable: yes, default value: 5m               tserver.last.location.mode      Available since: 2.1.1Describes how the system will record the ‘last’ location for tablets, which can be used for assigning them when a cluster restarts. If ‘compaction’ is the mode, then the system will record the location where the tablet’s most recent compaction occurred. If ‘assignment’ is the mode, then the most recently assigned location will be recorded. The manager.startup.tserver properties might also need to be set to ensure the tserver is available before tablets are initially assigned if the ‘last’ location is to be used.type: LAST_LOCATION_MODE, zk mutable: yes, default value: compaction               tserver.log.busy.tablets.count      Available since: 1.10.0Number of busiest tablets to log. Logged at interval controlled by tserver.log.busy.tablets.interval. If &amp;lt;= 0, logging of busy tablets is disabled.type: COUNT, zk mutable: yes, default value: 0               tserver.log.busy.tablets.interval      Available since: 1.10.0Time interval between logging out busy tablets information.type: TIMEDURATION, zk mutable: yes, default value: 1h               tserver.memory.maps.max      Available since: 1.3.5Maximum amount of memory that can be used to buffer data written to a tablet server. There are two other properties that can effectively limit memory usage table.compaction.minor.logs.threshold and tserver.wal.max.size. Ensure that table.compaction.minor.logs.threshold * tserver.wal.max.size &amp;gt;= this property.type: MEMORY, zk mutable: yes, default value: 33%               tserver.memory.maps.native.enabled      Available since: 1.3.5An in-memory data store for accumulo implemented in c++ that increases the amount of data accumulo can hold in memory and avoids Java GC pauses.type: BOOLEAN, zk mutable: yes but requires restart of the tserver, default value: true               tserver.metadata.readahead.concurrent.max      Available since: 1.3.5Deprecated since: 2.0.0Replaced by: tserver.scan.executors.meta.threadsThe maximum number of concurrent metadata read ahead that will execute.type: COUNT, zk mutable: yes, default value: 8               tserver.migrations.concurrent.max      Available since: 1.3.5The maximum number of concurrent tablet migrations for a tablet server.type: COUNT, zk mutable: yes, default value: 1               tserver.monitor.fs      Available since: 1.3.5Deprecated since: 2.1.0When enabled the tserver will monitor file systems and kill itself when one switches from rw to ro. This is usually and indication that Linux has detected a bad disk.type: BOOLEAN, zk mutable: yes, default value: false               tserver.port.client      Available since: 1.3.5The port used for handling client connections on the tablet servers.type: PORT, zk mutable: yes but requires restart of the tserver, default value: 9997               tserver.port.search      Available since: 1.3.5if the ports above are in use, search higher ports until one is available.type: BOOLEAN, zk mutable: yes but requires restart of the tserver, default value: false               tserver.readahead.concurrent.max      Available since: 1.3.5Deprecated since: 2.0.0Replaced by: tserver.scan.executors.default.threadsThe maximum number of concurrent read ahead that will execute. This effectively limits the number of long running scans that can run concurrently per tserver.type: COUNT, zk mutable: yes, default value: 16               tserver.recovery.concurrent.max      Available since: 1.5.0Deprecated since: 2.1.0Replaced by: tserver.wal.sort.concurrent.maxThe maximum number of threads to use to sort logs during recovery.type: COUNT, zk mutable: yes, default value: 2               tserver.replication.batchwriter.replayer.memory      Available since: 1.7.0Deprecated since: 2.1.0Memory to provide to batchwriter to replay mutations for replication.type: BYTES, zk mutable: yes, default value: 50M               tserver.replication.default.replayer      Available since: 1.7.0Deprecated since: 2.1.0Default AccumuloReplicationReplayer implementation.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.tserver.replication.BatchWriterReplicationReplayer               tserver.replication.replayer.*      Available since: 1.7.0Deprecated since: 2.1.0Allows configuration of implementation used to apply replicated data.               tserver.scan.executors.*      Available since: 2.0.0Prefix for defining executors to service scans. See scan executors for an overview of why and how to use this property. For each executor the number of threads, thread priority, and an optional prioritizer can be configured. To configure a new executor, set tserver.scan.executors.&amp;lt;name&amp;gt;.threads=&amp;lt;number&amp;gt;.  Optionally, can also set tserver.scan.executors.&amp;lt;name&amp;gt;.priority=&amp;lt;number 1 to 10&amp;gt;, tserver.scan.executors.&amp;lt;name&amp;gt;.prioritizer=&amp;lt;class name&amp;gt;, and tserver.scan.executors.&amp;lt;name&amp;gt;.prioritizer.opts.&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;.               tserver.scan.executors.default.prioritizer      Available since: 2.0.0Prioritizer for the default scan executor.  Defaults to none which results in FIFO priority.  Set to a class that implements org.apache.accumulo.core.spi.scan.ScanPrioritizer to configure one.type: STRING, zk mutable: yes, default value: empty               tserver.scan.executors.default.threads      Available since: 2.0.0The number of threads for the scan executor that tables use by default.type: COUNT, zk mutable: yes, default value: 16               tserver.scan.executors.meta.threads      Available since: 2.0.0The number of threads for the metadata table scan executor.type: COUNT, zk mutable: yes, default value: 8               tserver.scan.files.open.max      Available since: 1.4.0Maximum total RFiles that all tablets in a tablet server can open for scans.type: COUNT, zk mutable: yes but requires restart of the tserver, default value: 100               tserver.scan.results.max.timeout      Available since: 2.1.0Max time for the thrift client handler to wait for scan results before timing out.type: TIMEDURATION, zk mutable: yes, default value: 1s               tserver.server.message.size.max      Available since: 1.6.0Deprecated since: 2.1.3Replaced by: rpc.message.size.maxThe maximum size of a message that can be sent to a tablet server.type: BYTES, zk mutable: yes but requires restart of the tserver, default value: 1G               tserver.server.threadcheck.time      Available since: 1.4.0The time between adjustments of the server thread pool.type: TIMEDURATION, zk mutable: yes, default value: 1s               tserver.server.threads.minimum      Available since: 1.4.0The minimum number of threads to use to handle incoming requests.type: COUNT, zk mutable: yes but requires restart of the tserver, default value: 20               tserver.server.threads.timeout      Available since: 2.1.0The time after which incoming request threads terminate with no work available.  Zero (0) will keep the threads alive indefinitely.type: TIMEDURATION, zk mutable: yes but requires restart of the tserver, default value: 0s               tserver.session.idle.max      Available since: 1.3.5When a tablet server’s SimpleTimer thread triggers to check idle sessions, this configurable option will be used to evaluate scan sessions to determine if they can be closed due to inactivity.type: TIMEDURATION, zk mutable: yes, default value: 1m               tserver.session.update.idle.max      Available since: 1.6.5When a tablet server’s SimpleTimer thread triggers to check idle sessions, this configurable option will be used to evaluate update sessions to determine if they can be closed due to inactivity.type: TIMEDURATION, zk mutable: yes, default value: 1m               tserver.slow.filepermit.time      Available since: 1.9.3If a thread blocks more than this period of time waiting to get file permits, debugging information will be written.type: TIMEDURATION, zk mutable: yes, default value: 100ms               tserver.slow.flush.time      Available since: 1.8.0If a flush to the write-ahead log takes longer than this period of time, debugging information will written, and may result in a log rollover.type: TIMEDURATION, zk mutable: yes, default value: 100ms               tserver.sort.buffer.size      Available since: 1.5.0Deprecated since: 2.1.0Replaced by: tserver.wal.sort.buffer.sizeThe amount of memory to use when sorting logs during recovery.type: MEMORY, zk mutable: yes, default value: 10%               tserver.summary.partition.threads      Available since: 2.0.0Summary data must be retrieved from RFiles. For a large number of RFiles, the files are broken into partitions of 100k files. This setting determines how many of these groups of 100k RFiles will be processed concurrently.type: COUNT, zk mutable: yes, default value: 10               tserver.summary.remote.threads      Available since: 2.0.0For a partitioned group of 100k RFiles, those files are grouped by tablet server. Then a remote tablet server is asked to gather summary data. This setting determines how many concurrent request are made per partition.type: COUNT, zk mutable: yes, default value: 128               tserver.summary.retrieval.threads      Available since: 2.0.0The number of threads on each tablet server available to retrieve summary data, that is not currently in cache, from RFiles.type: COUNT, zk mutable: yes, default value: 10               tserver.tablet.split.midpoint.files.max      Available since: 1.3.5To find a tablets split points, all RFiles are opened and their indexes are read. This setting determines how many RFiles can be opened at once. When there are more RFiles than this setting multiple passes must be made, which is slower. However opening too many RFiles at once can cause problems.type: COUNT, zk mutable: yes, default value: 300               tserver.total.mutation.queue.max      Available since: 1.7.0The amount of memory used to store write-ahead-log mutations before flushing them.type: MEMORY, zk mutable: yes, default value: 5%               tserver.wal.blocksize      Available since: 1.5.0The size of the HDFS blocks used to write to the Write-Ahead log. If zero, it will be 110% of tserver.wal.max.size (that is, try to use just one block).type: BYTES, zk mutable: yes, default value: 0               tserver.wal.max.age      Available since: 2.1.0The maximum age for each write-ahead log.type: TIMEDURATION, zk mutable: yes, default value: 24h               tserver.wal.max.referenced      Available since: 2.1.0When a tablet server has more than this many write ahead logs, any tablet referencing older logs over this threshold is minor compacted.  Also any tablet referencing this many logs or more will be compacted.type: COUNT, zk mutable: yes, default value: 3               tserver.wal.max.size      Available since: 2.1.0The maximum size for each write-ahead log. See comment for property tserver.memory.maps.max.type: BYTES, zk mutable: yes, default value: 1G               tserver.wal.maximum.wait.duration      Available since: 2.1.0The maximum amount of time to wait after a failure to create or write a write-ahead log.type: TIMEDURATION, zk mutable: yes, default value: 5m               tserver.wal.replication      Available since: 1.5.0The replication to use when writing the Write-Ahead log to HDFS. If zero, it will use the HDFS default replication setting.type: COUNT, zk mutable: yes, default value: 0               tserver.wal.sort.buffer.size      Available since: 2.1.0The amount of memory to use when sorting logs during recovery.type: MEMORY, zk mutable: yes, default value: 10%               tserver.wal.sort.concurrent.max      Available since: 2.1.0The maximum number of threads to use to sort logs during recovery.type: COUNT, zk mutable: yes, default value: 2               tserver.wal.sort.file.*      Available since: 2.1.0The rfile properties to use when sorting logs during recovery. Most of the properties that begin with ‘table.file’ can be used here. For example, to set the compression of the sorted recovery files to snappy use ‘tserver.wal.sort.file.compress.type=snappy’.               tserver.wal.sync      Available since: 1.5.0Use the SYNC_BLOCK create flag to sync WAL writes to disk. Prevents problems recovering from sudden system resets.type: BOOLEAN, zk mutable: yes, default value: true               tserver.wal.sync.method      Available since: 1.5.2Deprecated since: 1.7.0Replaced by: table.durabilityUse table.durability instead.type: STRING, zk mutable: yes, default value: hsync               tserver.wal.tolerated.creation.failures      Available since: 2.1.0The maximum number of failures tolerated when creating a new write-ahead log. Negative values will allow unlimited creation failures. Exceeding this number of failures consecutively trying to create a new write-ahead log causes the TabletServer to exit.type: COUNT, zk mutable: yes, default value: 50               tserver.wal.tolerated.wait.increment      Available since: 2.1.0The amount of time to wait between failures to create or write a write-ahead log.type: TIMEDURATION, zk mutable: yes, default value: 1000ms               tserver.walog.max.age      Available since: 1.6.6Deprecated since: 2.1.0Replaced by: tserver.wal.max.ageThe maximum age for each write-ahead log.type: TIMEDURATION, zk mutable: yes, default value: 24h               tserver.walog.max.referenced      Available since: 2.0.0Deprecated since: 2.1.0Replaced by: tserver.wal.max.referencedWhen a tablet server has more than this many write ahead logs, any tablet referencing older logs over this threshold is minor compacted.  Also any tablet referencing this many logs or more will be compacted.type: COUNT, zk mutable: yes, default value: 3               tserver.walog.max.size      Available since: 1.3.5Deprecated since: 2.1.0Replaced by: tserver.wal.max.sizeThe maximum size for each write-ahead log. See comment for property tserver.memory.maps.max.type: BYTES, zk mutable: yes, default value: 1G               tserver.walog.maximum.wait.duration      Available since: 1.7.1Deprecated since: 2.1.0Replaced by: tserver.wal.maximum.wait.durationThe maximum amount of time to wait after a failure to create or write a write-ahead log.type: TIMEDURATION, zk mutable: yes, default value: 5m               tserver.walog.tolerated.creation.failures      Available since: 1.7.1Deprecated since: 2.1.0Replaced by: tserver.wal.tolerated.creation.failuresThe maximum number of failures tolerated when creating a new write-ahead log. Negative values will allow unlimited creation failures. Exceeding this number of failures consecutively trying to create a new write-ahead log causes the TabletServer to exit.type: COUNT, zk mutable: yes, default value: 50               tserver.walog.tolerated.wait.increment      Available since: 1.7.1Deprecated since: 2.1.0Replaced by: tserver.wal.tolerated.wait.incrementThe amount of time to wait between failures to create or write a write-ahead log.type: TIMEDURATION, zk mutable: yes, default value: 1000ms               tserver.workq.threads      Available since: 1.4.2Deprecated since: 2.1.3The number of threads for the distributed work queue. These threads are used for copying failed bulk import RFiles. This property will be removed when bulk import V1 is removed.type: COUNT, zk mutable: yes, default value: 2      Property Types            Type      Description                  duration      A non-negative integer optionally followed by a unit of time (whitespace disallowed), as in 30s.If no unit of time is specified, seconds are assumed. Valid units are ‘ms’, ‘s’, ‘m’, ‘h’ for milliseconds, seconds, minutes, and hours.Examples of valid durations are ‘600’, ’30s’, ‘45m’, ‘30000ms’, ‘3d’, and ‘1h’.Examples of invalid durations are ‘1w’, ‘1h30m’, ‘1s 200ms’, ‘ms’, ‘’, and ‘a’.Unless otherwise stated, the max value for the duration represented in milliseconds is 9223372036854775807              bytes      A positive integer optionally followed by a unit of memory (whitespace disallowed).If no unit is specified, bytes are assumed. Valid units are ‘B’, ‘K’, ‘M’ or ‘G’ for bytes, kilobytes, megabytes, gigabytes.Examples of valid memories are ‘1024’, ‘20B’, ‘100K’, ‘1500M’, ‘2G’, ‘20%’.Examples of invalid memories are ‘1M500K’, ‘1M 2K’, ‘1MB’, ‘1.5G’, ‘1,024K’, ‘’, and ‘a’.Unless otherwise stated, the max value for the memory represented in bytes is 9223372036854775807              memory      A positive integer optionally followed by a unit of memory or a percentage (whitespace disallowed).If a percentage is specified, memory will be a percentage of the max memory allocated to a Java process (set by the JVM option -Xmx).If no unit is specified, bytes are assumed. Valid units are ‘B’, ‘K’, ‘M’, ‘G’, ‘%’ for bytes, kilobytes, megabytes, gigabytes, and percentage.Examples of valid memories are ‘1024’, ‘20B’, ‘100K’, ‘1500M’, ‘2G’, ‘20%’.Examples of invalid memories are ‘1M500K’, ‘1M 2K’, ‘1MB’, ‘1.5G’, ‘1,024K’, ‘’, and ‘a’.Unless otherwise stated, the max value for the memory represented in bytes is 9223372036854775807              host list      A comma-separated list of hostnames or ip addresses, with optional port numbers.Examples of valid host lists are ‘localhost:2000,www.example.com,10.10.1.1:500’ and ‘localhost’.Examples of invalid host lists are ‘’, ‘:1000’, and ‘localhost:80000’              port      An positive integer in the range 1024-65535 (not already in use or specified elsewhere in the configuration),zero to indicate any open ephemeral port, or a range of positive integers specified as M-N              count      A non-negative integer in the range of 0-2147483647              fraction/percentage      A floating point number that represents either a fraction or, if suffixed with the ‘%’ character, a percentage.Examples of valid fractions/percentages are ‘10’, ‘1000%’, ‘0.05’, ‘5%’, ‘0.2%’, ‘0.0005’.Examples of invalid fractions/percentages are ‘’, ‘10 percent’, ‘Hulk Hogan’              path      A string that represents a filesystem path, which can be either relative or absolute to some directory. The filesystem depends on the property. Substitutions of the ACCUMULO_HOME environment variable can be done in the system config file using ‘${env:ACCUMULO_HOME}’ or similar.              absolute path      An absolute filesystem path. The filesystem depends on the property. This is the same as path, but enforces that its root is explicitly specified.              java class      A fully qualified java class name representing a class on the classpath.An example is ‘java.lang.String’, rather than ‘String’              java class list      A list of fully qualified java class names representing classes on the classpath.An example is ‘java.lang.String’, rather than ‘String’              durability      One of ‘none’, ‘log’, ‘flush’ or ‘sync’.              gc_post_action      One of ‘none’, ‘flush’, or ‘compact’.              last_location_mode      Defines how to update the last location.  One of ‘assignment’, or ‘compaction’.              string      An arbitrary string of characters whose format is unspecified and interpreted based on the context of the property to which it applies.              json      An arbitrary string that is represents a valid, parsable generic json object.The validity of the json object in the context of the property usage is not checked by this type.              boolean      Has a value of either ‘true’ or ‘false’ (case-insensitive)              uri      A valid URI              file name extension      One of the currently supported filename extensions for storing table data files. Currently, only rf is supported.      ",
      "url": " /docs/2.x/configuration/server-properties",
      "categories": "configuration"
    },
  
    "docs-2-x-configuration-server-properties3": {
      "title": "Server Properties (3.x)",
      "content": "Below are properties set in accumulo.properties or the Accumulo shell that configure Accumulo servers (i.e. tablet server, manager, etc). Properties labeled ‘Experimental’ should not be considered stable and have a higher risk of changing in the future.            Property      Description                   compaction.coordinator.*      ExperimentalAvailable since: 2.1.0Properties in this category affect the behavior of the accumulo compaction coordinator server.               compaction.coordinator.compaction.finalizer.check.interval      ExperimentalAvailable since: 2.1.0The interval at which to check for external compaction final state markers in the metadata table.type: TIMEDURATION, zk mutable: no, default value: 60s               compaction.coordinator.compaction.finalizer.threads.maximum      ExperimentalAvailable since: 2.1.0The maximum number of threads to use for notifying tablet servers that an external compaction has completed.type: COUNT, zk mutable: no, default value: 5               compaction.coordinator.compactor.dead.check.interval      ExperimentalAvailable since: 2.1.0The interval at which to check for dead compactors.type: TIMEDURATION, zk mutable: no, default value: 5m               compaction.coordinator.message.size.max      ExperimentalAvailable since: 2.1.0The maximum size of a message that can be sent to a tablet server.type: BYTES, zk mutable: no, default value: 10M               compaction.coordinator.port.client      ExperimentalAvailable since: 2.1.0The port used for handling Thrift client connections on the compaction coordinator servertype: PORT, zk mutable: no, default value: 9132               compaction.coordinator.port.search      ExperimentalAvailable since: 2.1.0If the ports above are in use, search higher ports until one is availabletype: BOOLEAN, zk mutable: no, default value: false               compaction.coordinator.threadcheck.time      ExperimentalAvailable since: 2.1.0The time between adjustments of the server thread pool.type: TIMEDURATION, zk mutable: no, default value: 1s               compaction.coordinator.threads.minimum      ExperimentalAvailable since: 2.1.0The minimum number of threads to use to handle incoming requests.type: COUNT, zk mutable: no, default value: 1               compaction.coordinator.threads.timeout      ExperimentalAvailable since: 2.1.0The time after which incoming request threads terminate with no work available.  Zero (0) will keep the threads alive indefinitely.type: TIMEDURATION, zk mutable: no, default value: 0s               compaction.coordinator.tserver.check.interval      ExperimentalAvailable since: 2.1.0The interval at which to check the tservers for external compactions.type: TIMEDURATION, zk mutable: no, default value: 1m               compactor.*      ExperimentalAvailable since: 2.1.0Properties in this category affect the behavior of the accumulo compactor server.               compactor.message.size.max      ExperimentalAvailable since: 2.1.0The maximum size of a message that can be sent to a tablet server.type: BYTES, zk mutable: no, default value: 10M               compactor.port.client      ExperimentalAvailable since: 2.1.0The port used for handling client connections on the compactor serverstype: PORT, zk mutable: no, default value: 9133               compactor.port.search      ExperimentalAvailable since: 2.1.0If the compactor.port.client is in use, search higher ports until one is availabletype: BOOLEAN, zk mutable: no, default value: false               compactor.queue      ExperimentalAvailable since: 3.0.0The queue for which this Compactor will perform compactionstype: STRING, zk mutable: no, default value: empty               compactor.threadcheck.time      ExperimentalAvailable since: 2.1.0The time between adjustments of the server thread pool.type: TIMEDURATION, zk mutable: no, default value: 1s               compactor.threads.minimum      ExperimentalAvailable since: 2.1.0The minimum number of threads to use to handle incoming requests.type: COUNT, zk mutable: no, default value: 1               compactor.threads.timeout      ExperimentalAvailable since: 2.1.0The time after which incoming request threads terminate with no work available.  Zero (0) will keep the threads alive indefinitely.type: TIMEDURATION, zk mutable: no, default value: 0s               gc.*      Available since: 1.3.5Properties in this category affect the behavior of the accumulo garbage collector.               gc.candidate.batch.size      Available since: 2.1.0The batch size used for garbage collection.type: BYTES, zk mutable: yes, default value: 8M               gc.cycle.delay      Available since: 1.3.5Time between garbage collection cycles. In each cycle, old RFiles or write-ahead logs no longer in use are removed from the filesystem.type: TIMEDURATION, zk mutable: yes, default value: 5m               gc.cycle.start      Available since: 1.3.5Time to wait before attempting to garbage collect any old RFiles or write-ahead logs.type: TIMEDURATION, zk mutable: yes, default value: 30s               gc.port.client      Available since: 1.3.5The listening port for the garbage collector’s monitor servicetype: PORT, zk mutable: yes but requires restart of the gc, default value: 9998               gc.post.metadata.action      Available since: 1.10.0When the gc runs it can make a lot of changes to the metadata, on completion,  to force the changes to be written to disk, the metadata and root tables can be flushed and possibly compacted. Legal values are: compact - which both flushes and compacts the metadata; flush - which flushes only (compactions may be triggered if required); or nonetype: GC_POST_ACTION, zk mutable: yes, default value: flush               gc.safemode      Available since: 2.1.0Provides listing of files to be deleted but does not delete any filestype: BOOLEAN, zk mutable: yes, default value: false               gc.threads.delete      Available since: 1.3.5The number of threads used to delete RFiles and write-ahead logstype: COUNT, zk mutable: yes, default value: 16               general.*      Available since: 1.3.5Properties in this category affect the behavior of accumulo overall, but do not have to be consistent throughout a cloud.               general.context.class.loader.factory      Available since: 2.1.0Name of classloader factory to be used to create classloaders for named contexts, such as per-table contexts set by table.class.loader.context.type: CLASSNAME, zk mutable: no, default value: empty               general.custom.*      Available since: 2.0.0Prefix to be used for user defined system-wide properties. This may be particularly useful for system-wide configuration for various user-implementations of pluggable Accumulo features, such as the balancer or volume chooser.               general.delegation.token.lifetime      Available since: 1.7.0The length of time that delegation tokens and secret keys are validtype: TIMEDURATION, zk mutable: no, default value: 7d               general.delegation.token.update.interval      Available since: 1.7.0The length of time between generation of new secret keystype: TIMEDURATION, zk mutable: no, default value: 1d               general.kerberos.keytab      Available since: 1.4.1Path to the kerberos keytab to use. Leave blank if not using kerberoized hdfstype: PATH, zk mutable: no, default value: empty               general.kerberos.principal      Available since: 1.4.1Name of the kerberos principal to use. _HOST will automatically be replaced by the machines hostname in the hostname portion of the principal. Leave blank if not using kerberoized hdfstype: STRING, zk mutable: no, default value: empty               general.kerberos.renewal.period      Available since: 1.6.5The amount of time between attempts to perform Kerberos ticket renewals. This does not equate to how often tickets are actually renewed (which is performed at 80% of the ticket lifetime).type: TIMEDURATION, zk mutable: no, default value: 30s               general.low.mem.detector.interval      Available since: 3.0.0The time interval between low memory checkstype: TIMEDURATION, zk mutable: no, default value: 5s               general.low.mem.detector.threshold      Available since: 3.0.0The LowMemoryDetector will report when free memory drops below this percentage of total memorytype: FRACTION, zk mutable: no, default value: 0.05               general.low.mem.protection.compaction.majc      Available since: 3.0.0Major compactions may be paused when the server is low on memory and this property is set to true. Enabling this property will incur a slight compaction performance penalty when the server is not low on memorytype: BOOLEAN, zk mutable: no, default value: false               general.low.mem.protection.compaction.minc      Available since: 3.0.0Minor compactions may be paused when the server is low on memory and this property is set to true. Enabling this property will incur a slight compaction performance penalty when the server is not low on memorytype: BOOLEAN, zk mutable: no, default value: false               general.low.mem.protection.scan      Available since: 3.0.0Scans may be paused or return results early when the server is low on memory and this property is set to true. Enabling this property will incur a slight scan performance penalty when the server is not low on memorytype: BOOLEAN, zk mutable: no, default value: false               general.max.scanner.retry.period      Available since: 1.7.3The maximum amount of time that a Scanner should wait before retrying a failed RPCtype: TIMEDURATION, zk mutable: no, default value: 5s               general.micrometer.enabled      Available since: 2.1.0Enables metrics functionality using Micrometertype: BOOLEAN, zk mutable: no, default value: false               general.micrometer.factory      Available since: 2.1.0Name of class that implements MeterRegistryFactorytype: CLASSNAME, zk mutable: no, default value: empty               general.micrometer.jvm.metrics.enabled      Available since: 2.1.0Enables JVM metrics functionality using Micrometertype: BOOLEAN, zk mutable: no, default value: false               general.opentelemetry.enabled      ExperimentalAvailable since: 2.1.0Enables tracing functionality using OpenTelemetry (assuming OpenTelemetry is configured).type: BOOLEAN, zk mutable: no, default value: false               general.process.bind.addr      Available since: 3.0.0The local IP address to which this server should bind for sending and receiving network traffictype: STRING, zk mutable: no, default value: 0.0.0.0               general.rpc.server.type      ExperimentalAvailable since: 1.7.0Type of Thrift server to instantiate, see org.apache.accumulo.server.rpc.ThriftServerType for more information. Only useful for benchmarking thrift serverstype: STRING, zk mutable: no, default value: empty               general.rpc.timeout      Available since: 1.3.5Time to wait on I/O for simple, short RPC callstype: TIMEDURATION, zk mutable: no, default value: 120s               general.security.credential.provider.paths      Available since: 1.6.1Comma-separated list of paths to CredentialProviderstype: STRING, zk mutable: no, default value: empty               general.server.message.size.max      Available since: 1.5.0The maximum size of a message that can be sent to a server.type: BYTES, zk mutable: no, default value: 1G               general.server.threadpool.size      Available since: 2.1.0The number of threads to use for server-internal scheduled taskstype: COUNT, zk mutable: no, default value: 1               general.volume.chooser      ExperimentalAvailable since: 1.6.0The class that will be used to select which volume will be used to create new files.type: CLASSNAME, zk mutable: no, default value: org.apache.accumulo.core.spi.fs.RandomVolumeChooser               instance.*      Available since: 1.3.5Properties in this category must be consistent throughout a cloud. This is enforced and servers won’t be able to communicate if these differ.               instance.crypto.opts.*      ExperimentalAvailable since: 2.0.0Properties related to on-disk file encryption.               instance.crypto.opts.factory      ExperimentalAvailable since: 2.1.0The class which provides crypto services for on-disk file encryption. The default does nothing. To enable encryption, replace this classname with an implementation of theorg.apache.accumulo.core.spi.crypto.CryptoFactory interface.type: CLASSNAME, zk mutable: no, default value: org.apache.accumulo.core.spi.crypto.NoCryptoServiceFactory               instance.crypto.opts.sensitive.*      ExperimentalAvailable since: 2.0.0Sensitive properties related to on-disk file encryption.               instance.rpc.sasl.allowed.host.impersonation      Available since: 1.7.1One-line configuration property controlling the network locations (hostnames) that are allowed to impersonate other userstype: STRING, zk mutable: no, default value: empty               instance.rpc.sasl.allowed.user.impersonation      Available since: 1.7.1One-line configuration property controlling what users are allowed to impersonate other userstype: STRING, zk mutable: no, default value: empty               instance.rpc.sasl.enabled      Available since: 1.7.0Configures Thrift RPCs to require SASL with GSSAPI which supports Kerberos authentication. Mutually exclusive with SSL RPC configuration.type: BOOLEAN, zk mutable: no, default value: false               instance.rpc.ssl.clientAuth      Available since: 1.6.0Require clients to present certs signed by a trusted roottype: BOOLEAN, zk mutable: no, default value: false               instance.rpc.ssl.enabled      Available since: 1.6.0Use SSL for socket connections from clients and among accumulo services. Mutually exclusive with SASL RPC configuration.type: BOOLEAN, zk mutable: no, default value: false               instance.secret      Available since: 1.3.5A secret unique to a given instance that all servers must know in order to communicate with one another. It should be changed prior to the initialization of Accumulo. To change it after Accumulo has been initialized, use the ChangeSecret tool and then update accumulo.properties everywhere. Before using the ChangeSecret tool, make sure Accumulo is not running and you are logged in as the user that controls Accumulo files in HDFS. To use the ChangeSecret tool, run the command: ./bin/accumulo org.apache.accumulo.server.util.ChangeSecrettype: STRING, zk mutable: no, default value: DEFAULT               instance.security.authenticator      ExperimentalAvailable since: 1.5.0The authenticator class that accumulo will use to determine if a user has privilege to perform an actiontype: CLASSNAME, zk mutable: no, default value: org.apache.accumulo.server.security.handler.ZKAuthenticator               instance.security.authorizor      ExperimentalAvailable since: 1.5.0The authorizor class that accumulo will use to determine what labels a user has privilege to seetype: CLASSNAME, zk mutable: no, default value: org.apache.accumulo.server.security.handler.ZKAuthorizor               instance.security.permissionHandler      ExperimentalAvailable since: 1.5.0The permission handler class that accumulo will use to determine if a user has privilege to perform an actiontype: CLASSNAME, zk mutable: no, default value: org.apache.accumulo.server.security.handler.ZKPermHandler               instance.volume.config.*      Available since: 2.1.1Properties in this category are used to provide volume specific overrides to the general filesystem client configuration. Properties using this prefix should be in the form ‘instance.volume.config..=. An example: &#39;instance.volume.config.hdfs://namespace-a:8020/accumulo.dfs.client.hedged.read.threadpool.size=10&#39;. Note that when specifying property names that contain colons in the properties files that the colons need to be escaped with a backslash.               instance.volumes      Available since: 1.6.0A comma separated list of dfs uris to use. Files will be stored across these filesystems. In some situations, the first volume in this list may be treated differently, such as being preferred for writing out temporary files (for example, when creating a pre-split table). After adding uris to this list, run ‘accumulo init –add-volume’ and then restart tservers. If entries are removed from this list then tservers will need to be restarted. After a uri is removed from the list Accumulo will not create new files in that location, however Accumulo can still reference files created at that location before the config change. To use a comma or other reserved characters in a URI use standard URI hex encoding. For example replace commas with %2C.type: STRING, zk mutable: no, default value: empty               instance.volumes.replacements      Available since: 1.6.0Since accumulo stores absolute URIs changing the location of a namenode could prevent Accumulo from starting. The property helps deal with that situation. Provide a comma separated list of uri replacement pairs here if a namenode location changes. Each pair should be separated with a space. For example, if hdfs://nn1 was replaced with hdfs://nnA and hdfs://nn2 was replaced with hdfs://nnB, then set this property to ‘hdfs://nn1 hdfs://nnA,hdfs://nn2 hdfs://nnB’ Replacements must be configured for use. To see which volumes are currently in use, run ‘accumulo admin volumes -l’. To use a comma or other reserved characters in a URI use standard URI hex encoding. For example replace commas with %2C.type: STRING, zk mutable: no, default value: empty               instance.volumes.upgrade.relative      Available since: 2.1.0The volume dfs uri containing relative tablet file paths. Relative paths may exist in the metadata from versions prior to 1.6. This property is only required if a relative path is detected during the upgrade process and will only be used once.type: STRING, zk mutable: no, default value: empty               instance.zookeeper.host      Available since: 1.3.5Comma separated list of zookeeper serverstype: HOSTLIST, zk mutable: no, default value: localhost:2181               instance.zookeeper.timeout      Available since: 1.3.5Zookeeper session timeout; max value when represented as milliseconds should be no larger than 2147483647type: TIMEDURATION, zk mutable: no, default value: 30s               manager.*      Available since: 2.1.0Properties in this category affect the behavior of the manager server.               manager.bulk.timeout      Available since: 2.1.0 (formerly master.bulk.timeout since 1.4.3)The time to wait for a tablet server to process a bulk import requesttype: TIMEDURATION, zk mutable: yes, default value: 5m               manager.fate.metrics.min.update.interval      Available since: 2.1.0 (formerly master.fate.metrics.min.update.interval since 1.9.3)Limit calls from metric sinks to zookeeper to update intervaltype: TIMEDURATION, zk mutable: yes, default value: 60s               manager.fate.threadpool.size      Available since: 2.1.0 (formerly master.fate.threadpool.size since 1.4.3)The number of threads used to run fault-tolerant executions (FATE). These are primarily table operations like merge.type: COUNT, zk mutable: yes, default value: 4               manager.lease.recovery.interval      Available since: 2.1.0 (formerly master.lease.recovery.interval since 1.5.0)The amount of time to wait after requesting a write-ahead log to be recoveredtype: TIMEDURATION, zk mutable: yes, default value: 5s               manager.metadata.suspendable      Available since: 2.1.0 (formerly master.metadata.suspendable since 1.8.0)Allow tablets for the accumulo.metadata table to be suspended via table.suspend.duration.type: BOOLEAN, zk mutable: yes, default value: false               manager.port.client      Available since: 2.1.0 (formerly master.port.client since 1.3.5)The port used for handling client connections on the managertype: PORT, zk mutable: yes but requires restart of the manager, default value: 9999               manager.recovery.delay      Available since: 2.1.0 (formerly master.recovery.delay since 1.5.0)When a tablet server’s lock is deleted, it takes time for it to completely quit. This delay gives it time before log recoveries begin.type: TIMEDURATION, zk mutable: yes, default value: 10s               manager.recovery.wal.cache.time      Available since: 2.1.2Amount of time that the existence of recovery write-ahead logs is cached.type: TIMEDURATION, zk mutable: yes but requires restart of the manager, default value: 15s               manager.rename.threadpool.size      Available since: 2.1.0The number of threads to use when renaming user files during table import or bulk ingest.type: COUNT, zk mutable: yes, default value: 20               manager.server.threadcheck.time      Available since: 2.1.0 (formerly master.server.threadcheck.time since 1.4.0)The time between adjustments of the server thread pool.type: TIMEDURATION, zk mutable: yes, default value: 1s               manager.server.threads.minimum      Available since: 2.1.0 (formerly master.server.threads.minimum since 1.4.0)The minimum number of threads to use to handle incoming requests.type: COUNT, zk mutable: yes, default value: 20               manager.server.threads.timeout      Available since: 2.1.0The time after which incoming request threads terminate with no work available.  Zero (0) will keep the threads alive indefinitely.type: TIMEDURATION, zk mutable: yes, default value: 0s               manager.startup.tserver.avail.max.wait      Available since: 2.1.0 (formerly master.startup.tserver.avail.max.wait since 1.10.0)Maximum time manager will wait for tserver available threshold to be reached before continuing. When set to 0 or less, will block indefinitely. Default is 0 to block indefinitely. Only valid when tserver available threshold is set greater than 0. Added with version 1.10type: TIMEDURATION, zk mutable: yes, default value: 0               manager.startup.tserver.avail.min.count      Available since: 2.1.0 (formerly master.startup.tserver.avail.min.count since 1.10.0)Minimum number of tservers that need to be registered before manager will start tablet assignment - checked at manager initialization, when manager gets lock.  When set to 0 or less, no blocking occurs. Default is 0 (disabled) to keep original  behaviour. Added with version 1.10type: COUNT, zk mutable: yes, default value: 0               manager.status.threadpool.size      Available since: 2.1.0 (formerly master.status.threadpool.size since 1.8.0)The number of threads to use when fetching the tablet server status for balancing.  Zero indicates an unlimited number of threads will be used.type: COUNT, zk mutable: yes, default value: 0               manager.tablet.balancer      Available since: 2.1.0 (formerly master.tablet.balancer since 1.3.5)The balancer class that accumulo will use to make tablet assignment and migration decisions.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.spi.balancer.TableLoadBalancer               manager.tablet.watcher.interval      Available since: 2.1.2Time to wait between scanning tablet states to identify tablets that need to be assigned, un-assigned, migrated, etc.type: TIMEDURATION, zk mutable: yes, default value: 60s               manager.wal.closer.implementation      Available since: 2.1.0A class that implements a mechanism to steal write access to a write-ahead logtype: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.server.manager.recovery.HadoopLogCloser               monitor.*      Available since: 1.3.5Properties in this category affect the behavior of the monitor web server.               monitor.lock.check.interval      Available since: 1.5.1The amount of time to sleep between checking for the Monitor ZooKeeper locktype: TIMEDURATION, zk mutable: no, default value: 5s               monitor.port.client      Available since: 1.3.5The listening port for the monitor’s http servicetype: PORT, zk mutable: no, default value: 9995               monitor.resources.external      Available since: 2.0.0A JSON Map of Strings. Each String should be an HTML tag of an external resource (JS or CSS) to be imported by the Monitor. Be sure to wrap with CDATA tags. If this value is set, all of the external resources in the &amp;lt;head&amp;gt; tag of the Monitor will be replaced with the tags set here. Be sure the jquery tag is first since other scripts will depend on it. The resources that are used by default can be seen in accumulo/server/monitor/src/main/resources/templates/default.ftltype: STRING, zk mutable: no, default value: empty               monitor.ssl.exclude.ciphers      Available since: 1.6.1A comma-separated list of disallowed SSL Ciphers, see monitor.ssl.include.ciphers to allow cipherstype: STRING, zk mutable: no, default value: empty               monitor.ssl.include.ciphers      Available since: 1.6.1A comma-separated list of allows SSL Ciphers, see monitor.ssl.exclude.ciphers to disallow cipherstype: STRING, zk mutable: no, default value: empty               monitor.ssl.include.protocols      Available since: 1.5.3A comma-separate list of allowed SSL protocolstype: STRING, zk mutable: no, default value: TLSv1.2               monitor.ssl.keyPassword      Available since: 1.9.3Optional: the password for the private key in the keyStore. When not provided, this defaults to the keystore password.type: STRING, zk mutable: no, default value: empty               monitor.ssl.keyStore      Available since: 1.5.0The keystore for enabling monitor SSL.type: PATH, zk mutable: no, default value: empty               monitor.ssl.keyStorePassword      Available since: 1.5.0The keystore password for enabling monitor SSL.type: STRING, zk mutable: no, default value: empty               monitor.ssl.keyStoreType      Available since: 1.7.0Type of SSL keystoretype: STRING, zk mutable: no, default value: jks               monitor.ssl.trustStore      Available since: 1.5.0The truststore for enabling monitor SSL.type: PATH, zk mutable: no, default value: empty               monitor.ssl.trustStorePassword      Available since: 1.5.0The truststore password for enabling monitor SSL.type: STRING, zk mutable: no, default value: empty               monitor.ssl.trustStoreType      Available since: 1.7.0Type of SSL truststoretype: STRING, zk mutable: no, default value: jks               rpc.*      Available since: 1.6.0Properties in this category related to the configuration of SSL keys for RPC. See also instance.ssl.enabled               rpc.javax.net.ssl.keyStore      Available since: 1.6.0Path of the keystore file for the server’s private SSL keytype: PATH, zk mutable: no, default value: empty               rpc.javax.net.ssl.keyStorePassword      Available since: 1.6.0Password used to encrypt the SSL private keystore. Leave blank to use the Accumulo instance secrettype: STRING, zk mutable: no, default value: empty               rpc.javax.net.ssl.keyStoreType      Available since: 1.6.0Type of SSL keystoretype: STRING, zk mutable: no, default value: jks               rpc.javax.net.ssl.trustStore      Available since: 1.6.0Path of the truststore file for the root certtype: PATH, zk mutable: no, default value: empty               rpc.javax.net.ssl.trustStorePassword      Available since: 1.6.0Password used to encrypt the SSL truststore. Leave blank to use no passwordtype: STRING, zk mutable: no, default value: empty               rpc.javax.net.ssl.trustStoreType      Available since: 1.6.0Type of SSL truststoretype: STRING, zk mutable: no, default value: jks               rpc.sasl.qop      Available since: 1.7.0The quality of protection to be used with SASL. Valid values are ‘auth’, ‘auth-int’, and ‘auth-conf’type: STRING, zk mutable: no, default value: auth               rpc.ssl.cipher.suites      Available since: 1.6.1Comma separated list of cipher suites that can be used by accepted connectionstype: STRING, zk mutable: no, default value: empty               rpc.ssl.client.protocol      Available since: 1.6.2The protocol used to connect to a secure server, must be in the list of enabled protocols on the server side (rpc.ssl.server.enabled.protocols)type: STRING, zk mutable: no, default value: TLSv1.2               rpc.ssl.server.enabled.protocols      Available since: 1.6.2Comma separated list of protocols that can be used to accept connectionstype: STRING, zk mutable: no, default value: TLSv1.2               rpc.useJsse      Available since: 1.6.0Use JSSE system properties to configure SSL rather than the rpc.javax.net.ssl.* Accumulo propertiestype: BOOLEAN, zk mutable: no, default value: false               sserver.*      ExperimentalAvailable since: 2.1.0Properties in this category affect the behavior of the scan servers               sserver.cache.data.size      ExperimentalAvailable since: 2.1.0Specifies the size of the cache for RFile data blocks on each scan server.type: MEMORY, zk mutable: no, default value: 10%               sserver.cache.index.size      ExperimentalAvailable since: 2.1.0Specifies the size of the cache for RFile index blocks on each scan server.type: MEMORY, zk mutable: no, default value: 25%               sserver.cache.metadata.expiration      ExperimentalAvailable since: 2.1.0The time after which cached tablet metadata will be refreshed.type: TIMEDURATION, zk mutable: no, default value: 5m               sserver.cache.summary.size      ExperimentalAvailable since: 2.1.0Specifies the size of the cache for summary data on each scan server.type: MEMORY, zk mutable: no, default value: 10%               sserver.default.blocksize      ExperimentalAvailable since: 2.1.0Specifies a default blocksize for the scan server cachestype: BYTES, zk mutable: no, default value: 1M               sserver.group      ExperimentalAvailable since: 3.0.0Optional group name that will be made available to the ScanServerSelector client plugin. Groups support at least two use cases: dedicating resources to scans and/or using different hardware for scans.type: STRING, zk mutable: no, default value: default               sserver.port.client      ExperimentalAvailable since: 2.1.0The port used for handling client connections on the tablet serverstype: PORT, zk mutable: no, default value: 9996               sserver.port.search      ExperimentalAvailable since: 2.1.0if the ports above are in use, search higher ports until one is availabletype: BOOLEAN, zk mutable: no, default value: true               sserver.scan.executors.*      ExperimentalAvailable since: 2.1.0Prefix for defining executors to service scans. See scan executors for an overview of why and how to use this property. For each executor the number of threads, thread priority, and an optional prioritizer can be configured. To configure a new executor, set sserver.scan.executors.&amp;lt;name&amp;gt;.threads=&amp;lt;number&amp;gt;.  Optionally, can also set sserver.scan.executors.&amp;lt;name&amp;gt;.priority=&amp;lt;number 1 to 10&amp;gt;, sserver.scan.executors.&amp;lt;name&amp;gt;.prioritizer=&amp;lt;class name&amp;gt;, and sserver.scan.executors.&amp;lt;name&amp;gt;.prioritizer.opts.&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;               sserver.scan.executors.default.prioritizer      ExperimentalAvailable since: 2.1.0Prioritizer for the default scan executor.  Defaults to none which results in FIFO priority.  Set to a class that implements org.apache.accumulo.core.spi.scan.ScanPrioritizer to configure one.type: STRING, zk mutable: no, default value: empty               sserver.scan.executors.default.threads      ExperimentalAvailable since: 2.1.0The number of threads for the scan executor that tables use by default.type: COUNT, zk mutable: no, default value: 16               sserver.scan.executors.meta.threads      ExperimentalAvailable since: 2.1.0The number of threads for the metadata table scan executor.type: COUNT, zk mutable: no, default value: 8               sserver.scan.reference.expiration      ExperimentalAvailable since: 2.1.0The amount of time a scan reference is unused before its deleted from metadata table type: TIMEDURATION, zk mutable: no, default value: 5m               sserver.server.message.size.max      ExperimentalAvailable since: 2.1.0The maximum size of a message that can be sent to a scan server.type: BYTES, zk mutable: no, default value: 1G               sserver.server.threadcheck.time      ExperimentalAvailable since: 2.1.0The time between adjustments of the thrift server thread pool.type: TIMEDURATION, zk mutable: no, default value: 1s               sserver.server.threads.minimum      ExperimentalAvailable since: 2.1.0The minimum number of threads to use to handle incoming requests.type: COUNT, zk mutable: no, default value: 2               sserver.server.threads.timeout      ExperimentalAvailable since: 2.1.0The time after which incoming request threads terminate with no work available.  Zero (0) will keep the threads alive indefinitely.type: TIMEDURATION, zk mutable: no, default value: 0s               table.*      Available since: 1.3.5Properties in this category affect tablet server treatment of tablets, but can be configured on a per-table basis. Setting these properties in accumulo.properties will override the default globally for all tables and not any specific table. However, both the default and the global setting can be overridden per table using the table operations API or in the shell, which sets the overridden value in zookeeper. Restarting accumulo tablet servers after setting these properties in accumulo.properties will cause the global setting to take effect. However, you must use the API or the shell to change properties in zookeeper that are set on a table.               table.balancer      Available since: 1.3.5This property can be set to allow the LoadBalanceByTable load balancer to change the called Load Balancer for this tabletype: STRING, zk mutable: yes, default value: org.apache.accumulo.core.spi.balancer.SimpleLoadBalancer               table.bloom.enabled      Available since: 1.3.5Use bloom filters on this table.type: BOOLEAN, zk mutable: yes, default value: false               table.bloom.error.rate      Available since: 1.3.5Bloom filter error rate.type: FRACTION, zk mutable: yes, default value: 0.5%               table.bloom.hash.type      Available since: 1.3.5The bloom filter hash typetype: STRING, zk mutable: yes, default value: murmur               table.bloom.key.functor      Available since: 1.3.5A function that can transform the key prior to insertion and check of bloom filter. org.apache.accumulo.core.file.keyfunctor.RowFunctor, org.apache.accumulo.core.file.keyfunctor.ColumnFamilyFunctor, and org.apache.accumulo.core.file.keyfunctor.ColumnQualifierFunctor are allowable values. One can extend any of the above mentioned classes to perform specialized parsing of the key. type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.file.keyfunctor.RowFunctor               table.bloom.load.threshold      Available since: 1.3.5This number of seeks that would actually use a bloom filter must occur before a RFile’s bloom filter is loaded. Set this to zero to initiate loading of bloom filters when a RFile is opened.type: COUNT, zk mutable: yes, default value: 1               table.bloom.size      Available since: 1.3.5Bloom filter size, as number of keys.type: COUNT, zk mutable: yes, default value: 1048576               table.bulk.max.tablets      Available since: 2.1.0The maximum number of tablets allowed for one bulk import file. Value of 0 is Unlimited. This property is only enforced in the new bulk import APItype: COUNT, zk mutable: yes, default value: 0               table.cache.block.enable      Available since: 1.3.5Determines whether data block cache is enabled for a table.type: BOOLEAN, zk mutable: yes, default value: false               table.cache.index.enable      Available since: 1.3.5Determines whether index block cache is enabled for a table.type: BOOLEAN, zk mutable: yes, default value: true               table.class.loader.context      Available since: 2.1.0The context to use for loading per-table resources, such as iterators from the configured factory in general.context.class.loader.factory.type: STRING, zk mutable: yes, default value: empty               table.compaction.configurer      Available since: 2.1.0A plugin that can dynamically configure compaction output files based on input files.type: CLASSNAME, zk mutable: yes, default value: empty               table.compaction.configurer.opts.*      Available since: 2.1.0Options for the table compaction configuror               table.compaction.dispatcher      Available since: 2.1.0A configurable dispatcher that decides what compaction service a table should use.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.spi.compaction.SimpleCompactionDispatcher               table.compaction.dispatcher.opts.*      Available since: 2.1.0Options for the table compaction dispatcher               table.compaction.major.output.drop.cache      Available since: 2.1.1Setting this property to true will callFSDataOutputStream.setDropBehind(true) on the major compaction output stream.type: BOOLEAN, zk mutable: yes, default value: false               table.compaction.major.ratio      Available since: 1.3.5Minimum ratio of total input size to maximum input RFile size for running a major compaction. type: FRACTION, zk mutable: yes, default value: 3               table.compaction.minor.idle      Available since: 1.3.5After a tablet has been idle (no mutations) for this time period it may have its in-memory map flushed to disk in a minor compaction. There is no guarantee an idle tablet will be compacted.type: TIMEDURATION, zk mutable: yes, default value: 5m               table.compaction.minor.output.drop.cache      Available since: 2.1.1Setting this property to true will callFSDataOutputStream.setDropBehind(true) on the minor compaction output stream.type: BOOLEAN, zk mutable: yes, default value: false               table.compaction.selection.expiration.ms      Available since: 2.1.0User compactions select files and are then queued for compaction, preventing these files from being used in system compactions.  This timeout allows system compactions to cancel the hold queued user compactions have on files, when its queued for more than the specified time.  If a system compaction cancels a hold and runs, then the user compaction can reselect and hold files after the system compaction runs.type: TIMEDURATION, zk mutable: yes, default value: 2m               table.compaction.selector      Available since: 2.1.0A configurable selector for a table that can periodically select file for mandatory compaction, even if the files do not meet the compaction ratio.type: CLASSNAME, zk mutable: yes, default value: empty               table.compaction.selector.opts.*      Available since: 2.1.0Options for the table compaction dispatcher               table.constraint.*      Available since: 1.3.5Properties in this category are per-table properties that add constraints to a table. These properties start with the category prefix, followed by a number, and their values correspond to a fully qualified Java class that implements the Constraint interface.For example:table.constraint.1 = org.apache.accumulo.core.constraints.MyCustomConstraintand: table.constraint.2 = my.package.constraints.MySecondConstraint               table.crypto.opts.*      ExperimentalAvailable since: 2.1.0Properties related to on-disk file encryption.               table.crypto.opts.sensitive.*      ExperimentalAvailable since: 2.1.0Sensitive properties related to on-disk file encryption.               table.custom.*      Available since: 1.7.0Prefix to be used for user defined arbitrary properties.               table.delete.behavior      ExperimentalAvailable since: 2.0.0This determines what action to take when a delete marker is seen. Valid values are process and fail with process being the default.  When set to process, deletes will suppress data.  When set to fail, any deletes seen will cause an exception. The purpose of fail is to support tables that never delete data and need fast seeks within the timestamp range of a column. When setting this to fail, also consider configuring the org.apache.accumulo.core.data.constraints.NoDeleteConstraint constraint.type: STRING, zk mutable: yes, default value: process               table.durability      Available since: 1.7.0The durability used to write to the write-ahead log. Legal values are: none, which skips the write-ahead log; log, which sends the data to the write-ahead log, but does nothing to make it durable; flush, which pushes data to the file system; and sync, which ensures the data is written to disk.type: DURABILITY, zk mutable: yes, default value: sync               table.failures.ignore      Available since: 1.3.5If you want queries for your table to hang or fail when data is missing from the system, then set this to false. When this set to true missing data will be reported but queries will still run possibly returning a subset of the data.type: BOOLEAN, zk mutable: yes, default value: false               table.file.blocksize      Available since: 1.3.5The HDFS block size used when writing RFiles. When set to 0B, the value/defaults of HDFS property ‘dfs.block.size’ will be used.type: BYTES, zk mutable: yes, default value: 0B               table.file.compress.blocksize      Available since: 1.3.5The maximum size of data blocks in RFiles before they are compressed and written.type: BYTES, zk mutable: yes, default value: 100k               table.file.compress.blocksize.index      Available since: 1.4.0The maximum size of index blocks in RFiles before they are compressed and written.type: BYTES, zk mutable: yes, default value: 128k               table.file.compress.type      Available since: 1.3.5Compression algorithm used on index and data blocks before they are written. Possible values: zstd, gz, snappy, bzip2, lzo, lz4, nonetype: STRING, zk mutable: yes, default value: gz               table.file.max      Available since: 1.4.0The maximum number of RFiles each tablet in a table can have. When adjusting this property you may want to consider adjusting table.compaction.major.ratio also. Setting this property to 0 will make it default to tserver.scan.files.open.max-1, this will prevent a tablet from having more RFiles than can be opened. Setting this property low may throttle ingest and increase query performance.type: COUNT, zk mutable: yes, default value: 15               table.file.replication      Available since: 1.3.5The number of replicas for a table’s RFiles in HDFS. When set to 0, HDFS defaults are used.type: COUNT, zk mutable: yes, default value: 0               table.file.summary.maxSize      Available since: 2.0.0The maximum size summary that will be stored. The number of RFiles that had summary data exceeding this threshold is reported by Summary.getFileStatistics().getLarge(). When adjusting this consider the expected number RFiles with summaries on each tablet server and the summary cache size.type: BYTES, zk mutable: yes, default value: 256k               table.file.type      Available since: 1.3.5Change the type of file a table writestype: FILENAME_EXT, zk mutable: yes, default value: rf               table.formatter      Available since: 1.4.0The Formatter class to apply on results in the shelltype: STRING, zk mutable: yes, default value: org.apache.accumulo.core.util.format.DefaultFormatter               table.group.*      Available since: 1.3.5Properties in this category are per-table properties that define locality groups in a table. These properties start with the category prefix, followed by a name, followed by a period, and followed by a property for that group.For example table.group.group1=x,y,z sets the column families for a group called group1. Once configured, group1 can be enabled by adding it to the list of groups in the table.groups.enabled property.Additional group options may be specified for a named group by setting table.group.&amp;lt;name&amp;gt;.opt.&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;.               table.groups.enabled      Available since: 1.3.5A comma separated list of locality group names to enable for this table.type: STRING, zk mutable: yes, default value: empty               table.iterator.*      Available since: 1.3.5Properties in this category specify iterators that are applied at various stages (scopes) of interaction with a table. These properties start with the category prefix, followed by a scope (minc, majc, scan, etc.), followed by a period, followed by a name, as in table.iterator.scan.vers, or table.iterator.scan.custom. The values for these properties are a number indicating the ordering in which it is applied, and a class name such as:table.iterator.scan.vers = 10,org.apache.accumulo.core.iterators.VersioningIteratorThese iterators can take options if additional properties are set that look like this property, but are suffixed with a period, followed by ‘opt’ followed by another period, and a property name.For example, table.iterator.minc.vers.opt.maxVersions = 3               table.iterator.majc.*      Available since: 1.5.2Convenience prefix to find options for the majc iterator scope               table.iterator.minc.*      Available since: 1.5.2Convenience prefix to find options for the minc iterator scope               table.iterator.scan.*      Available since: 1.5.2Convenience prefix to find options for the scan iterator scope               table.sampler      Available since: 1.8.0The name of a class that implements org.apache.accumulo.core.Sampler. Setting this option enables storing a sample of data which can be scanned. Always having a current sample can useful for query optimization and data comprehension. After enabling sampling for an existing table, a compaction is needed to compute the sample for existing data. The compact command in the shell has an option to only compact RFiles without sample data.type: CLASSNAME, zk mutable: yes, default value: empty               table.sampler.opt.*      Available since: 1.8.0The property is used to set options for a sampler. If a sample had two options like hasher and modulous, then the two properties table.sampler.opt.hasher=${hash algorithm} and table.sampler.opt.modulous=${mod} would be set.               table.scan.dispatcher      Available since: 2.0.0This class is used to dynamically dispatch scans to configured scan executors.  Configured classes must implement ScanDispatcher See scan executors for an overview of why and how to use this property. This property is ignored for the root and metadata table.  The metadata table always dispatches to a scan executor named meta.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.spi.scan.SimpleScanDispatcher               table.scan.dispatcher.opts.*      Available since: 2.0.0Options for the table scan dispatcher               table.scan.max.memory      Available since: 1.3.5The maximum amount of memory that will be used to cache results of a client query/scan. Once this limit is reached, the buffered data is sent to the client.type: BYTES, zk mutable: yes, default value: 512k               table.security.scan.visibility.default      Available since: 1.3.5The security label that will be assumed at scan time if an entry does not have a visibility expression.Note: An empty security label is displayed as []. The scan results will show an empty visibility even if the visibility from this setting is applied to the entry.CAUTION: If a particular key has an empty security label AND its table’s default visibility is also empty, access will ALWAYS be granted for users with permission to that table. Additionally, if this field is changed, all existing data with an empty visibility label will be interpreted with the new label on the next scan.type: STRING, zk mutable: yes, default value: empty               table.split.endrow.size.max      Available since: 1.7.0Maximum size of end rowtype: BYTES, zk mutable: yes, default value: 10k               table.split.threshold      Available since: 1.3.5A tablet is split when the combined size of RFiles exceeds this amount.type: BYTES, zk mutable: yes, default value: 1G               table.summarizer.*      Available since: 2.0.0Prefix for configuring summarizers for a table. Using this prefix multiple summarizers can be configured with options for each one. Each summarizer configured should have a unique id, this id can be anything. To add a summarizer set table.summarizer.&amp;lt;unique id&amp;gt;=&amp;lt;summarizer class name&amp;gt;. If the summarizer has options, then for each option set table.summarizer.&amp;lt;unique id&amp;gt;.opt.&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;.               table.suspend.duration      Available since: 1.8.0For tablets belonging to this table: When a tablet server dies, allow the tablet server this duration to revive before reassigning its tablets to other tablet servers.type: TIMEDURATION, zk mutable: yes, default value: 0s               tserver.*      Available since: 1.3.5Properties in this category affect the behavior of the tablet servers               tserver.assignment.concurrent.max      Available since: 1.7.0The number of threads available to load tablets. Recoveries are still performed serially.type: COUNT, zk mutable: yes, default value: 2               tserver.assignment.duration.warning      Available since: 1.6.2The amount of time an assignment can run before the server will print a warning along with the current stack trace. Meant to help debug stuck assignmentstype: TIMEDURATION, zk mutable: yes, default value: 10m               tserver.bloom.load.concurrent.max      Available since: 1.3.5The number of concurrent threads that will load bloom filters in the background. Setting this to zero will make bloom filters load in the foreground.type: COUNT, zk mutable: yes, default value: 4               tserver.cache.data.size      Available since: 1.3.5Specifies the size of the cache for RFile data blocks.type: MEMORY, zk mutable: yes but requires restart of the tserver, default value: 10%               tserver.cache.index.size      Available since: 1.3.5Specifies the size of the cache for RFile index blocks.type: MEMORY, zk mutable: yes but requires restart of the tserver, default value: 25%               tserver.cache.manager.class      Available since: 2.0.0Specifies the class name of the block cache factory implementation. Alternative implementation is org.apache.accumulo.core.file.blockfile.cache.tinylfu.TinyLfuBlockCacheManagertype: STRING, zk mutable: yes but requires restart of the tserver, default value: org.apache.accumulo.core.file.blockfile.cache.lru.LruBlockCacheManager               tserver.cache.summary.size      Available since: 2.0.0Specifies the size of the cache for summary data on each tablet server.type: MEMORY, zk mutable: yes but requires restart of the tserver, default value: 10%               tserver.client.timeout      Available since: 1.3.5Time to wait for clients to continue scans before closing a session.type: TIMEDURATION, zk mutable: yes, default value: 3s               tserver.compaction.major.delay      Available since: 1.3.5Time a tablet server will sleep between checking which tablets need compaction.type: TIMEDURATION, zk mutable: yes, default value: 30s               tserver.compaction.major.service.*      Available since: 2.1.0Prefix for compaction services.               tserver.compaction.major.service.default.planner      Available since: 2.1.0Planner for default compaction service.type: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner               tserver.compaction.major.service.default.planner.opts.executors      Available since: 2.1.0See org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner type: STRING, zk mutable: yes, default value: [{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;32M&quot;,&quot;numThreads&quot;:2},{&quot;name&quot;:&quot;medium&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;128M&quot;,&quot;numThreads&quot;:2},{&quot;name&quot;:&quot;large&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;numThreads&quot;:2}]               tserver.compaction.major.service.default.planner.opts.maxOpen      Available since: 2.1.0The maximum number of files a compaction will opentype: COUNT, zk mutable: yes, default value: 10               tserver.compaction.major.service.default.rate.limit      Available since: 2.1.0Maximum number of bytes to read or write per second over all major compactions in this compaction service, or 0B for unlimited.type: BYTES, zk mutable: yes, default value: 0B               tserver.compaction.major.service.meta.planner      Available since: 2.1.0Compaction planner for metadata tabletype: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner               tserver.compaction.major.service.meta.planner.opts.executors      Available since: 2.1.0See org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner type: STRING, zk mutable: yes, default value: [{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;32M&quot;,&quot;numThreads&quot;:2},{&quot;name&quot;:&quot;huge&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;numThreads&quot;:2}]               tserver.compaction.major.service.meta.planner.opts.maxOpen      Available since: 2.1.0The maximum number of files a compaction will opentype: COUNT, zk mutable: yes, default value: 30               tserver.compaction.major.service.meta.rate.limit      Available since: 2.1.0Maximum number of bytes to read or write per second over all major compactions in this compaction service, or 0B for unlimited.type: BYTES, zk mutable: yes, default value: 0B               tserver.compaction.major.service.root.planner      Available since: 2.1.0Compaction planner for root tablet servicetype: CLASSNAME, zk mutable: yes, default value: org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner               tserver.compaction.major.service.root.planner.opts.executors      Available since: 2.1.0See org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner type: STRING, zk mutable: yes, default value: [{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;32M&quot;,&quot;numThreads&quot;:1},{&quot;name&quot;:&quot;huge&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;numThreads&quot;:1}]               tserver.compaction.major.service.root.planner.opts.maxOpen      Available since: 2.1.0The maximum number of files a compaction will opentype: COUNT, zk mutable: yes, default value: 30               tserver.compaction.major.service.root.rate.limit      Available since: 2.1.0Maximum number of bytes to read or write per second over all major compactions in this compaction service, or 0B for unlimited.type: BYTES, zk mutable: yes, default value: 0B               tserver.compaction.minor.concurrent.max      Available since: 1.3.5The maximum number of concurrent minor compactions for a tablet servertype: COUNT, zk mutable: yes, default value: 4               tserver.compaction.warn.time      Available since: 1.6.0When a compaction has not made progress for this time period, a warning will be loggedtype: TIMEDURATION, zk mutable: yes, default value: 10m               tserver.default.blocksize      Available since: 1.3.5Specifies a default blocksize for the tserver cachestype: BYTES, zk mutable: yes, default value: 1M               tserver.dir.memdump      Available since: 1.3.5A long running scan could possibly hold memory that has been minor compacted. To prevent this, the in memory map is dumped to a local file and the scan is switched to that local file. We can not switch to the minor compacted file because it may have been modified by iterators. The file dumped to the local dir is an exact copy of what was in memory.type: PATH, zk mutable: yes, default value: /tmp               tserver.files.open.idle      Available since: 1.3.5Tablet servers leave previously used RFiles open for future queries. This setting determines how much time an unused RFile should be kept open until it is closed.type: TIMEDURATION, zk mutable: yes, default value: 1m               tserver.health.check.interval      Available since: 2.1.0The time between tablet server health checks.type: TIMEDURATION, zk mutable: yes, default value: 30m               tserver.hold.time.max      Available since: 1.4.0The maximum time for a tablet server to be in the “memory full” state. If the tablet server cannot write out memory in this much time, it will assume there is some failure local to its node, and quit. A value of zero is equivalent to forever.type: TIMEDURATION, zk mutable: yes, default value: 5m               tserver.last.location.mode      Available since: 2.1.1Describes how the system will record the ‘last’ location for tablets, which can be used for assigning them when a cluster restarts. If ‘compaction’ is the mode, then the system will record the location where the tablet’s most recent compaction occurred. If ‘assignment’ is the mode, then the most recently assigned location will be recorded. The manager.startup.tserver properties might also need to be set to ensure the tserver is available before tablets are initially assigned if the ‘last’ location is to be used.type: LAST_LOCATION_MODE, zk mutable: yes, default value: compaction               tserver.log.busy.tablets.count      Available since: 1.10.0Number of busiest tablets to log. Logged at interval controlled by tserver.log.busy.tablets.interval. If &amp;lt;= 0, logging of busy tablets is disabledtype: COUNT, zk mutable: yes, default value: 0               tserver.log.busy.tablets.interval      Available since: 1.10.0Time interval between logging out busy tablets information.type: TIMEDURATION, zk mutable: yes, default value: 1h               tserver.memory.maps.max      Available since: 1.3.5Maximum amount of memory that can be used to buffer data written to a tablet server. There are two other properties that can effectively limit memory usage table.compaction.minor.logs.threshold and tserver.wal.max.size. Ensure that table.compaction.minor.logs.threshold * tserver.wal.max.size &amp;gt;= this property.type: MEMORY, zk mutable: yes, default value: 33%               tserver.memory.maps.native.enabled      Available since: 1.3.5An in-memory data store for accumulo implemented in c++ that increases the amount of data accumulo can hold in memory and avoids Java GC pauses.type: BOOLEAN, zk mutable: yes but requires restart of the tserver, default value: true               tserver.migrations.concurrent.max      Available since: 1.3.5The maximum number of concurrent tablet migrations for a tablet servertype: COUNT, zk mutable: yes, default value: 1               tserver.port.client      Available since: 1.3.5The port used for handling client connections on the tablet serverstype: PORT, zk mutable: yes but requires restart of the tserver, default value: 9997               tserver.port.search      Available since: 1.3.5if the ports above are in use, search higher ports until one is availabletype: BOOLEAN, zk mutable: yes, default value: false               tserver.scan.executors.*      Available since: 2.0.0Prefix for defining executors to service scans. See scan executors for an overview of why and how to use this property. For each executor the number of threads, thread priority, and an optional prioritizer can be configured. To configure a new executor, set tserver.scan.executors.&amp;lt;name&amp;gt;.threads=&amp;lt;number&amp;gt;.  Optionally, can also set tserver.scan.executors.&amp;lt;name&amp;gt;.priority=&amp;lt;number 1 to 10&amp;gt;, tserver.scan.executors.&amp;lt;name&amp;gt;.prioritizer=&amp;lt;class name&amp;gt;, and tserver.scan.executors.&amp;lt;name&amp;gt;.prioritizer.opts.&amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;               tserver.scan.executors.default.prioritizer      Available since: 2.0.0Prioritizer for the default scan executor.  Defaults to none which results in FIFO priority.  Set to a class that implements org.apache.accumulo.core.spi.scan.ScanPrioritizer to configure one.type: STRING, zk mutable: yes, default value: empty               tserver.scan.executors.default.threads      Available since: 2.0.0The number of threads for the scan executor that tables use by default.type: COUNT, zk mutable: yes, default value: 16               tserver.scan.executors.meta.threads      Available since: 2.0.0The number of threads for the metadata table scan executor.type: COUNT, zk mutable: yes, default value: 8               tserver.scan.files.open.max      Available since: 1.4.0Maximum total RFiles that all tablets in a tablet server can open for scans. type: COUNT, zk mutable: yes but requires restart of the tserver, default value: 100               tserver.scan.results.max.timeout      Available since: 2.1.0Max time for the thrift client handler to wait for scan results before timing out.type: TIMEDURATION, zk mutable: yes, default value: 1s               tserver.server.message.size.max      Available since: 1.6.0The maximum size of a message that can be sent to a tablet server.type: BYTES, zk mutable: yes, default value: 1G               tserver.server.threadcheck.time      Available since: 1.4.0The time between adjustments of the server thread pool.type: TIMEDURATION, zk mutable: yes, default value: 1s               tserver.server.threads.minimum      Available since: 1.4.0The minimum number of threads to use to handle incoming requests.type: COUNT, zk mutable: yes, default value: 20               tserver.server.threads.timeout      Available since: 2.1.0The time after which incoming request threads terminate with no work available.  Zero (0) will keep the threads alive indefinitely.type: TIMEDURATION, zk mutable: yes, default value: 0s               tserver.session.idle.max      Available since: 1.3.5When a tablet server’s SimpleTimer thread triggers to check idle sessions, this configurable option will be used to evaluate scan sessions to determine if they can be closed due to inactivitytype: TIMEDURATION, zk mutable: yes, default value: 1m               tserver.session.update.idle.max      Available since: 1.6.5When a tablet server’s SimpleTimer thread triggers to check idle sessions, this configurable option will be used to evaluate update sessions to determine if they can be closed due to inactivitytype: TIMEDURATION, zk mutable: yes, default value: 1m               tserver.slow.filepermit.time      Available since: 1.9.3If a thread blocks more than this period of time waiting to get file permits, debugging information will be written.type: TIMEDURATION, zk mutable: yes, default value: 100ms               tserver.slow.flush.time      Available since: 1.8.0If a flush to the write-ahead log takes longer than this period of time, debugging information will written, and may result in a log rollover.type: TIMEDURATION, zk mutable: yes, default value: 100ms               tserver.summary.partition.threads      Available since: 2.0.0Summary data must be retrieved from RFiles. For a large number of RFiles, the files are broken into partitions of 100k files. This setting determines how many of these groups of 100k RFiles will be processed concurrently.type: COUNT, zk mutable: yes, default value: 10               tserver.summary.remote.threads      Available since: 2.0.0For a partitioned group of 100k RFiles, those files are grouped by tablet server. Then a remote tablet server is asked to gather summary data. This setting determines how many concurrent request are made per partition.type: COUNT, zk mutable: yes, default value: 128               tserver.summary.retrieval.threads      Available since: 2.0.0The number of threads on each tablet server available to retrieve summary data, that is not currently in cache, from RFiles.type: COUNT, zk mutable: yes, default value: 10               tserver.tablet.split.midpoint.files.max      Available since: 1.3.5To find a tablets split points, all RFiles are opened and their indexes are read. This setting determines how many RFiles can be opened at once. When there are more RFiles than this setting multiple passes must be made, which is slower. However opening too many RFiles at once can cause problems.type: COUNT, zk mutable: yes, default value: 300               tserver.total.mutation.queue.max      Available since: 1.7.0The amount of memory used to store write-ahead-log mutations before flushing them.type: MEMORY, zk mutable: yes, default value: 5%               tserver.wal.blocksize      Available since: 1.5.0The size of the HDFS blocks used to write to the Write-Ahead log. If zero, it will be 110% of tserver.wal.max.size (that is, try to use just one block)type: BYTES, zk mutable: yes, default value: 0               tserver.wal.max.age      Available since: 2.1.0The maximum age for each write-ahead log.type: TIMEDURATION, zk mutable: yes, default value: 24h               tserver.wal.max.referenced      Available since: 2.1.0When a tablet server has more than this many write ahead logs, any tablet referencing older logs over this threshold is minor compacted.  Also any tablet referencing this many logs or more will be compacted.type: COUNT, zk mutable: yes, default value: 3               tserver.wal.max.size      Available since: 2.1.0The maximum size for each write-ahead log. See comment for property tserver.memory.maps.maxtype: BYTES, zk mutable: yes, default value: 1G               tserver.wal.maximum.wait.duration      Available since: 2.1.0The maximum amount of time to wait after a failure to create or write a write-ahead log.type: TIMEDURATION, zk mutable: yes, default value: 5m               tserver.wal.replication      Available since: 1.5.0The replication to use when writing the Write-Ahead log to HDFS. If zero, it will use the HDFS default replication setting.type: COUNT, zk mutable: yes, default value: 0               tserver.wal.sort.buffer.size      Available since: 2.1.0The amount of memory to use when sorting logs during recovery.type: MEMORY, zk mutable: yes, default value: 10%               tserver.wal.sort.concurrent.max      Available since: 2.1.0The maximum number of threads to use to sort logs during recoverytype: COUNT, zk mutable: yes, default value: 2               tserver.wal.sort.file.*      Available since: 2.1.0The rfile properties to use when sorting logs during recovery. Most of the properties that begin with ‘table.file’ can be used here. For example, to set the compression of the sorted recovery files to snappy use ‘tserver.wal.sort.file.compress.type=snappy’               tserver.wal.sync      Available since: 1.5.0Use the SYNC_BLOCK create flag to sync WAL writes to disk. Prevents problems recovering from sudden system resets.type: BOOLEAN, zk mutable: yes, default value: true               tserver.wal.tolerated.creation.failures      Available since: 2.1.0The maximum number of failures tolerated when creating a new write-ahead log. Negative values will allow unlimited creation failures. Exceeding this number of failures consecutively trying to create a new write-ahead log causes the TabletServer to exit.type: COUNT, zk mutable: yes, default value: 50               tserver.wal.tolerated.wait.increment      Available since: 2.1.0The amount of time to wait between failures to create or write a write-ahead log.type: TIMEDURATION, zk mutable: yes, default value: 1000ms               tserver.workq.threads      Available since: 1.4.2The number of threads for the distributed work queue. These threads are used for copying failed bulk import RFiles.type: COUNT, zk mutable: yes, default value: 2      Property Types            Type      Description                  duration      A non-negative integer optionally followed by a unit of time (whitespace disallowed), as in 30s.If no unit of time is specified, seconds are assumed. Valid units are ‘ms’, ‘s’, ‘m’, ‘h’ for milliseconds, seconds, minutes, and hours.Examples of valid durations are ‘600’, ’30s’, ‘45m’, ‘30000ms’, ‘3d’, and ‘1h’.Examples of invalid durations are ‘1w’, ‘1h30m’, ‘1s 200ms’, ‘ms’, ‘’, and ‘a’.Unless otherwise stated, the max value for the duration represented in milliseconds is 9223372036854775807              bytes      A positive integer optionally followed by a unit of memory (whitespace disallowed).If no unit is specified, bytes are assumed. Valid units are ‘B’, ‘K’, ‘M’ or ‘G’ for bytes, kilobytes, megabytes, gigabytes.Examples of valid memories are ‘1024’, ‘20B’, ‘100K’, ‘1500M’, ‘2G’, ‘20%’.Examples of invalid memories are ‘1M500K’, ‘1M 2K’, ‘1MB’, ‘1.5G’, ‘1,024K’, ‘’, and ‘a’.Unless otherwise stated, the max value for the memory represented in bytes is 9223372036854775807              memory      A positive integer optionally followed by a unit of memory or a percentage (whitespace disallowed).If a percentage is specified, memory will be a percentage of the max memory allocated to a Java process (set by the JVM option -Xmx).If no unit is specified, bytes are assumed. Valid units are ‘B’, ‘K’, ‘M’, ‘G’, ‘%’ for bytes, kilobytes, megabytes, gigabytes, and percentage.Examples of valid memories are ‘1024’, ‘20B’, ‘100K’, ‘1500M’, ‘2G’, ‘20%’.Examples of invalid memories are ‘1M500K’, ‘1M 2K’, ‘1MB’, ‘1.5G’, ‘1,024K’, ‘’, and ‘a’.Unless otherwise stated, the max value for the memory represented in bytes is 9223372036854775807              host list      A comma-separated list of hostnames or ip addresses, with optional port numbers.Examples of valid host lists are ‘localhost:2000,www.example.com,10.10.1.1:500’ and ‘localhost’.Examples of invalid host lists are ‘’, ‘:1000’, and ‘localhost:80000’              port      An positive integer in the range 1024-65535 (not already in use or specified elsewhere in the configuration),zero to indicate any open ephemeral port, or a range of positive integers specified as M-N              count      A non-negative integer in the range of 0-2147483647              fraction/percentage      A floating point number that represents either a fraction or, if suffixed with the ‘%’ character, a percentage.Examples of valid fractions/percentages are ‘10’, ‘1000%’, ‘0.05’, ‘5%’, ‘0.2%’, ‘0.0005’.Examples of invalid fractions/percentages are ‘’, ‘10 percent’, ‘Hulk Hogan’              path      A string that represents a filesystem path, which can be either relative or absolute to some directory. The filesystem depends on the property. Substitutions of the ACCUMULO_HOME environment variable can be done in the system config file using ‘${env:ACCUMULO_HOME}’ or similar.              absolute path      An absolute filesystem path. The filesystem depends on the property. This is the same as path, but enforces that its root is explicitly specified.              java class      A fully qualified java class name representing a class on the classpath.An example is ‘java.lang.String’, rather than ‘String’              java class list      A list of fully qualified java class names representing classes on the classpath.An example is ‘java.lang.String’, rather than ‘String’              durability      One of ‘none’, ‘log’, ‘flush’ or ‘sync’.              gc_post_action      One of ‘none’, ‘flush’, or ‘compact’.              last_location_mode      Defines how to update the last location.  One of ‘assignment’, or ‘compaction’.              string      An arbitrary string of characters whose format is unspecified and interpreted based on the context of the property to which it applies.              boolean      Has a value of either ‘true’ or ‘false’ (case-insensitive)              uri      A valid URI              file name extension      One of the currently supported filename extensions for storing table data files. Currently, only rf is supported.      ",
      "url": " /docs/2.x/configuration/server-properties3",
      "categories": "configuration"
    },
  
    "docs-2-x-development-development-tools": {
      "title": "Development Tools",
      "content": "Accumulo has several tools that can help developers test their code.MiniAccumuloClusterMiniAccumuloCluster is a standalone instance of Apache Accumulo for testing. It willcreate Zookeeper and Accumulo processes that write all of their data to a single localdirectory. MiniAccumuloCluster makes it easy to code against a real Accumulo instance.Developers can write realistic-to-end integration tests that mimic the use of a normalAccumulo instance.MiniAccumuloCluster is published in a separate jar that should be added to your pom.xmlas a test dependency:&amp;lt;dependency&amp;gt;  &amp;lt;groupId&amp;gt;org.apache.accumulo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;accumulo-minicluster&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;${accumulo.version}&amp;lt;/version&amp;gt;  &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;To start it up, you will need to supply an empty directory and a root password as arguments:File tempDirectory = // JUnit and Guava supply mechanisms for creating temp directoriesMiniAccumuloCluster mac = new MiniAccumuloCluster(tempDirectory, &quot;password&quot;);mac.start();Once we have our mini cluster running, we will want to interact with the Accumulo client API:AccumuloClient client = mac.getAccumuloClient(&quot;root&quot;, new PasswordToken(&quot;password&quot;));Upon completion of our development code, we will want to shutdown our MiniAccumuloCluster:mac.stop();// delete your temporary folderIterator Test HarnessIterators, while extremely powerful, are notoriously difficult to test. While the API definesthe methods an Iterator must implement and each method’s functionality, the actual invocationof these methods by Accumulo TabletServers can be surprisingly difficult to mimic in unit tests.The Apache Accumulo “Iterator Test Harness” is designed to provide a generalized testing frameworkfor all Accumulo Iterators to leverage to identify common pitfalls in user-created Iterators.Framework UseThe Iterator Test Harness is published in a separate jar that should be added to your pom.xml asa test dependency:&amp;lt;dependency&amp;gt;  &amp;lt;groupId&amp;gt;org.apache.accumulo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;accumulo-iterator-test-harness&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;${accumulo.version}&amp;lt;/version&amp;gt;  &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&amp;lt;/dependency&amp;gt;To use the Iterator test harness, create a class that extends the IteratorTestBase classand defines the following:  A SortedMap of input data (Key-Value pairs)  A Range to use in tests  A Map of options (String to String pairs)  A SortedMap of output data (Key-Value pairs)  A list of IteratorTestCases (these can be automatically discovered)The majority of effort a user must make is in creating the input dataset and the expectedoutput dataset for the iterator being tested.Normal Test OutlineMost iterator tests will follow the given outline:import java.util.List;import java.util.SortedMap;import org.apache.accumulo.core.data.Key;import org.apache.accumulo.core.data.Range;import org.apache.accumulo.core.data.Value;import org.apache.accumulo.iteratortest.IteratorTestBase;import org.apache.accumulo.iteratortest.IteratorTestInput;import org.apache.accumulo.iteratortest.IteratorTestOutput;import org.apache.accumulo.iteratortest.IteratorTestParameters;public class MyIteratorTest extends IteratorTestBase {  @Override  protected Stream&amp;lt;IteratorTestParameters&amp;gt; parameters() {      var input = new IteratorTestInput(MyIterator.class, Map.of(), createRange(), INPUT_DATA);      var expectedOutput = new IteratorTestOutput(OUTPUT_DATA);      return builtinTestCases().map(test -&amp;gt; test.toParameters(input, expectedOutput));  }  private static SortedMap&amp;lt;Key,Value&amp;gt; INPUT_DATA = createInputData();  private static SortedMap&amp;lt;Key,Value&amp;gt; OUTPUT_DATA = createOutputData();  private static SortedMap&amp;lt;Key,Value&amp;gt; createInputData() {    // TODO -- implement this method  }  private static SortedMap&amp;lt;Key,Value&amp;gt; createOutputData() {    // TODO -- implement this method  }  private static Map&amp;lt;String,String&amp;gt; createOpts() {    IteratorSetting setting = new IteratorSetting(50, MyIterator.class);    // TODO -- add iterator specific options    return setting.getOptions();  }  private static Range createRange() {    // TODO -- implement this method  }}LimitationsWhile the classes that implement IteratorTestCases should exercise common edge-cases in user iterators,there are still many limitations to the existing test harness. Some of them are:  Can only specify a single iterator, not many (a “stack”)  No control over provided IteratorEnvironment for tests  Exercising delete keys (especially with major compactions that do not include all files)These are left as future improvements to the harness.",
      "url": " /docs/2.x/development/development_tools",
      "categories": "development"
    },
  
    "docs-2-x-development-high-speed-ingest": {
      "title": "High-Speed Ingest",
      "content": "Accumulo is often used as part of a larger data processing and storage system. Tomaximize the performance of a parallel system involving Accumulo, the ingestionand query components should be designed to provide enough parallelism andconcurrency to avoid creating bottlenecks for users and other systems writing toand reading from Accumulo. There are several ways to achieve high ingestperformance.Pre-Splitting New TablesNew tables consist of a single tablet by default. As mutations are applied, the tablegrows and splits into multiple tablets which are balanced by the Manager acrossTabletServers. This implies that the aggregate ingest rate will be limited to fewerservers than are available within the cluster until the table has reached the pointwhere there are tablets on every TabletServer.Pre-splitting a table ensures that there are as many tablets as desired availablebefore ingest begins to take advantage of all the parallelism possible with the clusterhardware. Tables can be split at any time by using the shell:user@myinstance mytable&amp;gt; addsplits -sf /local_splitfile -t mytableFor the purposes of providing parallelism to ingest, it is not necessary to create moretablets than there are physical machines within the cluster as the aggregate ingestrate is a function of the number of physical machines. Note that the aggregate ingestrate is still subject to the number of machines running ingest clients, and thedistribution of rowIDs across the table. The aggregation ingest rate will besuboptimal if there are many inserts into a small number of rowIDs.Multiple Ingest ClientsAccumulo is capable of scaling to very high rates of ingest, which is dependent uponnot just the number of TabletServers in operation but also the number of ingestclients. This is because a single client, while capable of batching mutations andsending them to all TabletServers, is ultimately limited by the amount of data thatcan be processed on a single machine. The aggregate ingest rate will scale linearlywith the number of clients up to the point at which either the aggregate I/O ofTabletServers or total network bandwidth capacity is reached.In operational settings where high rates of ingest are paramount, clusters are oftenconfigured to dedicate some number of machines solely to running Ingester Clients.The exact ratio of clients to TabletServers necessary for optimum ingestion rateswill vary according to the distribution of resources per machine and by data type.Bulk IngestAccumulo supports the ability to import files produced by an external process suchas MapReduce into an existing table. In some cases it may be faster to load data thisway rather than via ingesting through clients using BatchWriters. This allows a largenumber of machines to format data the way Accumulo expects. The new files canthen simply be introduced to Accumulo via a shell command.To configure MapReduce to format data in preparation for bulk loading, the jobshould be set to use a range partitioner instead of the default hash partitioner. Therange partitioner uses the split points of the Accumulo table that will receive thedata. The split points can be obtained from the shell and used by the MapReduceRangePartitioner. Note that this is only useful if the existing table is already splitinto multiple tablets.user@myinstance mytable&amp;gt; getsplitsaaabac...zxzyzzRun the MapReduce job, using the AccumuloFileOutputFormat to create the files tobe introduced to Accumulo. Once this is complete, the files can be added toAccumulo via the shell:user@myinstance mytable&amp;gt; importdirectory /files_dir /failuresNote that the paths referenced are directories within the same HDFS instance overwhich Accumulo is running. Accumulo places any files that failed to be added to thesecond directory specified.See the bulk ingest example for a complete example.Logical Time for Bulk IngestLogical time is important for bulk imported data, for which the client code maybe choosing a timestamp. At bulk import time, the user can choose to enablelogical time for the set of files being imported. When its enabled, Accumulouses a specialized system iterator to lazily set times in a bulk imported file.This mechanism guarantees that times set by unsynchronized multi-nodeapplications (such as those running on MapReduce) will maintain some semblanceof causal ordering. This mitigates the problem of the time being wrong on thesystem that created the file for bulk import. These times are not set when thefile is imported, but whenever it is read by scans or compactions. At import, atime is obtained and always used by the specialized system iterator to set thattime.The timestamp assigned by Accumulo will be the same for every key in the file.This could cause problems if the file contains multiple keys that are identicalexcept for the timestamp. In this case, the sort order of the keys will beundefined. This could occur if an insert and an update were in the same bulkimport file.MapReduce IngestIt is possible to efficiently write many mutations to Accumulo in parallel via aMapReduce job. Typically, a MapReduce job will process data that lives in HDFSand write mutations to Accumulo using AccumuloOutputFormat. For more informationon how use to use MapReduce with Accumulo, see the MapReduce documentation.",
      "url": " /docs/2.x/development/high_speed_ingest",
      "categories": "development"
    },
  
    "docs-2-x-development-iterators": {
      "title": "Iterators",
      "content": "Accumulo SortedKeyValueIterators, commonly referred to as Iterators for short, are server-side programming constructsthat allow users to implement custom retrieval or computational purpose within Accumulo TabletServers.  The name rightlybrings forward similarities to the Java Iterator interface; however, Accumulo Iterators are more complex than JavaIterators. Notably, in addition to the expected methods to retrieve the current element and advance to the next elementin the iteration, Accumulo Iterators must also support the ability to “move” (seek) to a specified point in theiteration (the Accumulo table). Accumulo Iterators are designed to be concatenated together, similar to applying aseries of transformations to a list of elements. Accumulo Iterators can duplicate their underlying source to createmultiple “pointers” over the same underlying data (which is extremely powerful since each stream is sorted) or they canmerge multiple Iterators into a single view. In this sense, a collection of Iterators operating in tandem is closer toa tree-structure than a list, but there is always a sense of a flow of Key-Value pairs through some Iterators. Iteratorsare not designed to act as triggers nor are they designed to operate outside of the purview of a single table.This guide aims to provide a more detailed description of how Iterators are invoked, some best practices and some commonpitfalls.InstantiationTo invoke an Accumulo Iterator inside of the TabletServer, the Iterator class must be on the classpath of everyTabletServer. It is common to place a JAR file which contains the Iterator in lib/. Accumulo references the Iterator class by name and uses Java reflection to instantiate the Iterator. This means that Iterators must have a public no-args constructor.InterfaceA normal implementation of the SortedKeyValueIterator defines functionality for the following methods:void init(SortedKeyValueIterator&amp;lt;Key,Value&amp;gt; source, Map&amp;lt;String,String&amp;gt; options, IteratorEnvironment env) throws IOException;boolean hasTop();void next() throws IOException;void seek(Range range, Collection&amp;lt;ByteSequence&amp;gt; columnFamilies, boolean inclusive) throws IOException;Key getTopKey();Value getTopValue();SortedKeyValueIterator&amp;lt;Key,Value&amp;gt; deepCopy(IteratorEnvironment env);initThe init method is called by the TabletServer after it constructs an instance of the Iterator.  This method shouldclear/reset any internal state in the Iterator and prepare it to process data.  The first argument, the source, is theIterator “below” this Iterator (where the client is at “top” and the Iterator for files in HDFS are at the “bottom”).The “source” Iterator provides the Key-Value pairs which this Iterator will operate upon.The second argument, a Map of options, is made up of options provided by the user, options set in the table’sconfiguration, and/or options set in the containing namespace’s configuration.These options allow for Iterators to dynamically configure themselves on the fly. If no options are used in the current context(a Scan or Compaction), the Map will be empty. An example of a configuration item for an Iterator could be a pattern used to filterKey-Value pairs in a regular expression Iterator.The third argument, the IteratorEnvironment, is a special object which provides information to this Iterator about thecontext in which it was invoked. Commonly, this information is not necessary to inspect. For example, if an Iteratorknows that it is running in the context of a full-major compaction (reading all of the data) as opposed to a user scan(which may strongly limit the number of columns), the Iterator might make different algorithmic decisions in an attempt tooptimize itself.seekThe purpose of the seek method is to advance the stream of Key-Value pairs to a certain point in the iteration (the Accumulo table). It is common that beforethe implementation of this method returns some additional processing is performed which may further advance the currentposition past the startKey of the Range. This, however, is dependent on the functionality the iterator provides. Forexample, a filtering iterator would consume a number Key-Value pairs which do not meet its criteria before seekreturns. The important condition for seek to meet is that this Iterator should be ready to return the first Key-Valuepair, or none if no such pair is available, when the method returns. The Key-Value pair would be returned by getTopKeyand getTopValue, respectively, and hasTop should return a boolean denoting whether or not there isa Key-Value pair to return.The arguments passed to seek are as follows:The TabletServer first provides a Range, an object which defines some collection of Accumulo Keys, which defines theKey-Value pairs that this Iterator should return. Each Range has a startKey and endKey with an inclusive flag forboth. While this Range is often similar to the Range(s) set by the client on a Scanner or BatchScanner, it is notguaranteed to be a Range that the client set. Accumulo will split up larger ranges and group them together based onTablet boundaries per TabletServer. Iterators should not attempt to implement any custom logic based on the Range(s)provided to seek and Iterators should not return any Keys that fall outside of the provided Range.The second argument, a Collection&amp;lt;ByteSequence&amp;gt;, is the set of column families which should be retained orexcluded by this Iterator. The third argument, a boolean, defines whether the collection of column familiesshould be treated as an inclusion collection (true) or an exclusion collection (false).It is likely that all implementations of seek will first make a call to the seek method on the“source” Iterator that was provided in the init method. The collection of column families andthe boolean include argument should be passed down as well as the Range. Somewhat commonly, the Iterator willalso implement some sort of additional logic to find or compute the first Key-Value pair in the providedRange. For example, a regular expression Iterator would consume all records which do not match the givenpattern before returning from seek.It is important to retain the original Range passed to this method to know when this Iterator should stopreading more Key-Value pairs. Ignoring this typically does not affect scans from a Scanner, but itwill result in duplicate keys emitting from a BatchScanner if the scanned table has more than one tablet.Best practice is to never emit entries outside the seek range.nextThe next method is analogous to the next method on a Java Iterator: this method should advancethe Iterator to the next Key-Value pair. For implementations that perform some filtering or complexlogic, this may result in more than one Key-Value pair being inspected. This method alterssome internal state that is exposed via the hasTop, getTopKey, and getTopValue methods.The result of this method is commonly caching a Key-Value pair which getTopKey and getTopValuecan later return. While there is another Key-Value pair to return, hasTop should return true.If there are no more Key-Value pairs to return from this Iterator since the last call toseek, hasTop should return false.hasTopThe hasTop method is similar to the hasNext method on a Java Iterator in that it informsthe caller if there is a Key-Value pair to be returned. If there is no pair to return, this methodshould return false. Like a Java Iterator, multiple calls to hasTop (without calling next) should notalter the internal state of the Iterator.getTopKey and getTopValueThese methods simply return the current Key-Value pair for this iterator. If hasTop returns true,both of these methods should return non-null objects. If hasTop returns false, it is undefinedwhat these methods should return. Like hasTop, multiple calls to these methods should not alterthe state of the Iterator.Users should take caution when either  caching the Key/Value from getTopKey/getTopValue, for use after calling next on the source iterator.In this case, the cached Key/Value object is aliased to the reference returned by the source iterator.Iterators may reuse the same Key/Value object in a next call for performance reasons, changing the datathat the cached Key/Value object references and resulting in a logic bug.  modifying the Key/Value from getTopKey/getTopValue. If the source iterator reuses data stored in the Key/Value,then the source iterator may use the modified data that the Key/Value references. This may/may not result in a logic bug.In both cases, copying the Key/Value’s data into a new object ensures iterator correctness. If neither case applies,it is safe to not copy the Key/Value.  The general guideline is to be aware of who else may use Key/Value objectsreturned from getTopKey/getTopValue.deepCopyThe deepCopy method is similar to the clone method from the Java Cloneable interface.Implementations of this method should return a new object of the same type as the Accumulo Iteratorinstance it was called on. Any internal state from the instance deepCopy was calledon should be carried over to the returned copy. The returned copy should be ready to haveseek called on it. The SortedKeyValueIterator interface guarantees that init will be called onan iterator before deepCopy and that init will not be called on the iterator returned bydeepCopy.Typically, implementations of deepCopy call a copy-constructor which will initializeinternal data structures. As with seek, it is common for the IteratorEnvironmentargument to be ignored as most Iterator implementations can be written without the explicitinformation the environment provides.In the analogy of a series of Iterators representing a tree, deepCopy can be thought of asearly programming assignments which implement their own tree data structures. deepCopy callscopy on its sources (the children), copies itself, attaches the copies of the children, andthen returns itself.Yielding InterfaceIf you have implemented an iterator with a next or seek call that can take a very long timeresulting in starving out other scans within the same thread pool, try implementing theoptional YieldingKeyValueIterator interface which SortedKeyValueIterator extends.default void enableYielding(YieldCallback callback) { }enableYieldingThe implementation of this method should simply cache the supplied callback as a member ofthe iterator. Then one can call the yield(Key key) method on the callback within a next orseek call when the iterator is to yield control.  The supplied key will be used as thestart key in a follow-on seek call’s range allowing the iterator to continue where it leftoff. Note when an iterator yields, the hasTop() method must return false.  Also note thatthe enableYielding method will not be called in isolation mode.TabletServer invocation of IteratorsThe following code is a general outline for how TabletServers invoke Iterators.List&amp;lt;KeyValue&amp;gt; batch;Range range = getRangeFromClient();while (!overSizeLimit(batch)) {    SortedKeyValueIterator source = getSystemIterator();    for (String clzName : getUserIterators()) {        Class&amp;lt;?&amp;gt; clz = Class.forName(clzName);        SortedKeyValueIterator iter = (SortedKeyValueIterator) clz.newInstance();        iter.init(source, opts, env);        source = iter;    }    // read a batch of data to return to client from    // the last iterator, the &quot;top&quot;    SortedKeyValueIterator topIter = source;    YieldCallback cb = new YieldCallback();    topIter.enableYielding(cb)    topIter.seek(range, ...)    while (topIter.hasTop() &amp;amp;&amp;amp; !overSizeLimit(batch)) {        key = topIter.getTopKey()        val = topIter.getTopValue()        batch.add(new KeyValue(key, val)        // remember the last key returned        setLastKeyReturned(key);        if (systemDataSourcesChanged()) {            // code does not show isolation case, which will            // keep using same data sources until a row boundary is hit            range = new Range(key, false, range.endKey(), range.endKeyInclusive());            break;        }        topIter.next()    }    if (cb.hasYielded()) {        // remember the yield key as the last key returned        setLastKeyReturned(cb.getKey());        break;    }}//return batch of key values to clientAdditionally, the obtuse “re-seek” case can be outlined as the following:// Given the aboveList&amp;lt;KeyValue&amp;gt; batch = getNextBatch();// thread goes away (client stops asking for the next batch).// Eventually client comes back// Setup as before...Range userRange = getRangeFromClient();Range actualRange = new Range(getLastKeyReturned(), false, userRange.getEndKey(), userRange.isEndKeyInclusive());// Use the actualRange, not the user provided onetopIter.seek(actualRange);IsolationAccumulo provides a feature which clients can enable to prevent the viewing of partiallyapplied mutations within the context of rows. If a client is submitting multiple columnupdates to rows at a time, isolation would ensure that a client would either see all ofupdates made to that row or none of the updates (until they are all applied).When using Isolation, there are additional concerns in iterator design. A scan time iterator in accumuloreads from a set of data sources. While an iterator is reading data it has an isolated view. However, after it returns akey/value it is possible that accumulo may switch data sources and re-seek the iterator. This is done so that resourcesmay be reclaimed. When the user does not request isolation this can occur after any key is returned. When a user enablesIsolation, this will only occur after a new row is returned, in which case it will re-seek to the very beginning of thenext possible row.Abstract IteratorsA number of Abstract implementations of Iterators are provided to allow for faster creationof common patterns. The most commonly used abstract implementations are the Filter andCombiner classes. When possible these classes should be used instead as they have beenthoroughly tested inside Accumulo itself.FilterThe Filter abstract Iterator provides a very simple implementation which allows implementationsto define whether or not a Key-Value pair should be returned via an accept(Key, Value) method.Filters are extremely simple to implement; however, when the implementation is filtering alarge percentage of Key-Value pairs with respect to the total number of pairs examined,it can be very inefficient. For example, if a Filter implementation can determine after examiningpart of the row that no other pairs in this row will be accepted, there is no mechanism toefficiently skip the remaining Key-Value pairs. Concretely, take a row which is comprised of1000 Key-Value pairs. After examining the first 10 Key-Value pairs, it is determinedthat no other Key-Value pairs in this row will be accepted. The Filter must still examine eachremaining 990 Key-Value pairs in this row. Another way to express this deficiency is thatFilters have no means to leverage the seek method to efficiently skip large portionsof Key-Value pairs.As such, the Filter class functions well for filtering small amounts of data, but isinefficient for filtering large amounts of data. The decision to use a Filter stronglydepends on the use case and distribution of data being filtered.CombinerThe Combiner class is another common abstract Iterator. Similar to the Combiner interfacedefine in Hadoop’s MapReduce framework, implementations of this abstract class reducemultiple Values for different versions of a Key (Keys which only differ by timestamps) into one Key-Value pair.Combiners provide a simple way to implement common operations like summation andaggregation without the need to implement the entire Accumulo Iterator interface.One important consideration when choosing to design a Combiner is that the “reduction” operationis often best represented when it is associative and commutative. Operations which do not meetthese criteria can be implemented; however, the implementation can be difficult.A second consideration is that a Combiner is not guaranteed to see every Key-Value pairwhich differ only by timestamp every time it is invoked. For example, if there are 5 Key-Valuepairs in a table which only differ by the timestamps 1, 2, 3, 4, and 5, it is not guaranteed thatevery invocation of the Combiner will see 5 timestamps. One invocation might see the Values forKeys with timestamp 1 and 4, while another invocation might see the Values for Keys with thetimestamps 1, 2, 4 and 5.Finally, when configuring an Accumulo table to use a Combiner, be sure to disable the Versioning Iterator or set theCombiner at a priority less than the Combiner (the Versioning Iterator is added at a priority of 20 by default). TheVersioning Iterator will filter out multiple Key-Value pairs that differ only by timestamp and return only the Key-Valuepair that has the largest timestamp.Combiner ApplicationsMany applications can benefit from the ability to aggregate values across commonkeys. This can be done via Combiner iterators and is similar to the Reduce step inMapReduce. This provides the ability to define online, incrementally updatedanalytics without the overhead or latency associated with batch-orientedMapReduce jobs.All that is needed to aggregate values of a table is to identify the fields over whichvalues will be grouped, insert mutations with those fields as the key, and configurethe table with a combining iterator that supports the summarizing operationdesired.The only restriction on a combining iterator is that the combiner developershould not assume that all values for a given key have been seen, since newmutations can be inserted at anytime. This precludes using the total number ofvalues in the aggregation such as when calculating an average, for example.An interesting use of combining iterators within an Accumulo table is to storefeature vectors for use in machine learning algorithms. For example, manyalgorithms such as k-means clustering, support vector machines, anomaly detection,etc. use the concept of a feature vector and the calculation of distance metrics tolearn a particular model. The columns in an Accumulo table can be used to efficientlystore sparse features and their weights to be incrementally updated via the use of acombining iterator.Best practicesBecause of the flexibility that the SortedKeyValueIterator interface provides, it doesn’t directly disallowmany implementations which are poor design decisions. The following are some common recommendations tofollow and pitfalls to avoid in Iterator implementations.Avoid special logic encoded in RangesCommonly, granular Ranges that a client passes to an Iterator from a Scanner or BatchScanner are unmodified.If a Range falls within the boundaries of a Tablet, an Iterator will often see that same Range in theseek method. However, there is no guarantee that the Range will remain unaltered from client to server. As such, Iteratorsshould never make assumptions about the current state/context based on the Range.The common failure condition is referred to as a “re-seek”. In the context of a Scan, TabletServers construct the“stack” of Iterators and batch up Key-Value pairs to send back to the client. When a sufficient number of Key-Valuepairs are collected, it is common for the Iterators to be “torn down” until the client asks for the next batch ofKey-Value pairs. This is done by the TabletServer to add fairness in ensuring one Scan does not monopolize the availableresources. When the client asks for the next batch, the implementation modifies the original Range so that servers knowthe point to resume the iteration (to avoid returning duplicate Key-Value pairs). Specifically, the new Range is createdfrom the original but is shortened by setting the startKey of the original Range to the Key last returned by the Scan,non-inclusive.seeking backwardsThe ability for an Iterator to “skip over” large blocks of Key-Value pairs is a major tenet behind Iterators.By seek‘ing when it is known that there is a collection of Key-Value pairs which can be ignored cangreatly increase the speed of a scan as many Key-Value pairs do not have to be deserialized and processed.While the seek method provides the Range that should be used to seek the underlying source Iterator,there is no guarantee that the implementing Iterator uses that Range to perform the seek on its“source” Iterator. As such, it is possible to seek to any Range and the interface has no assertionsto prevent this from happening.Since Iterators are allowed to seek to arbitrary Keys, it also allows Iterators to create infinite loopsinside Scans that will repeatedly read the same data without end. If an arbitrary Range is constructed, it shouldconstruct a completely new Range as it allows for bugs to be introduced which will break Accumulo.Thus, seek’s should always be thought of as making “forward progress” in the view of the total iteration. ThestartKey of a Range should always be greater than the current Key seen by the Iterator while the endKey of theRange should always retain the original endKey (and endKey inclusivity) of the last Range seen by yourIterator’s implementation of seek.Take caution in constructing new data in an IteratorImplementations of Iterator might be tempted to open BatchWriters inside of an Iterator as a meansto implement triggers for writing additional data outside of their client application. The lifecycle of an Iteratoris not managed in such a way that guarantees that this is safe nor efficient. Specifically, thereis no way to guarantee that the internal ThreadPool inside of the BatchWriter is closed (and the thread(s)are reaped) without calling the close() method. close‘ing and recreating a BatchWriter after everyKey-Value pair is also prohibitively performance limiting to be considered an option.The only safe way to generate additional data in an Iterator is to alter the current Key-Value pair.For example, the WholeRowIterator serializes the all of the Key-Values pairs that fall within eachrow. A safe way to generate more data in an Iterator would be to construct an Iterator that is“higher” (at a larger priority) than the WholeRowIterator, that is, the Iterator receives the Key-Value pairs which area serialization of many Key-Value pairs. The custom Iterator could deserialize the pairs, computesome function, and add a new Key-Value pair to the original collection, re-serializing the collectionof Key-Value pairs back into a single Key-Value pair.Any other situation is likely not guaranteed to ensure that the caller (a Scan or a Compaction) willalways see all intended data that is generated.Final things to rememberSome simple recommendations/points to keep in mind:Method call orderOn an instance of an Iterator: init is always called before seek, seek is always called before hasTop,getTopKey and getTopValue will not be called if hasTop returns false.TeardownInstances of iterators may be torn down inside the server transparently. When a complex collectionof iterators is performing advanced functionality, they will not be torn down until a Key-Value pairis returned out of the “stack” of iterators (and added into the batch of Key-Values to be returnedto the caller), or the iterator is yielded.When an iterator is torn down, the entire stack is dropped and no state is preserved. Only the lastkey returned (or the yielded position), original options, and seek range are retained. When the scanis continued, the iterator stack is rebuilt and re-initialized using the original options. The stackis then seeked with the original range, and the start key is replaced by the last key returned (orthe yielded position), non-inclusive.Compaction-time IteratorsWhen Iterators are configured to run during compactions, at the minc or majc scope, these Iterators sometimes needto make different assertions than those who only operate at scan time. Iterators won’t see the delete entries; however,Iterators will not necessarily see all of the Key-Value pairs in ever invocation. Because compactions often do not rewriteall files (only a subset of them), it is possible that the logic take this into consideration.For example, a Combiner that runs over data at during compactions, might not see all of the values for a given Key. TheCombiner must recognize this and not perform any function that would be incorrect dueto the missing values.TestingThe Iterator test harness is generalized testing framework for Accumulo Iterators that canidentify common pitfalls in user-created Iterators.",
      "url": " /docs/2.x/development/iterators",
      "categories": "development"
    },
  
    "docs-2-x-development-mapreduce": {
      "title": "MapReduce",
      "content": "Accumulo tables can be used as the source and destination of MapReduce jobs.General MapReduce configurationAdd Accumulo’s MapReduce API to your dependenciesIf you are using Maven, add the following dependency to your pom.xml to use Accumulo’s MapReduce API:&amp;lt;dependency&amp;gt;  &amp;lt;groupId&amp;gt;org.apache.accumulo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;accumulo-hadoop-mapreduce&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;2.1.3&amp;lt;/version&amp;gt;&amp;lt;/dependency&amp;gt;The MapReduce API consists of the following classes:  If using Hadoop’s mapreduce API:          org.apache.accumulo.hadoop.mapreduce.AccumuloInputFormat      org.apache.accumulo.hadoop.mapreduce.AccumuloOutputFormat      org.apache.accumulo.hadoop.mapreduce.AccumuloFileOutputFormat        If using Hadoop’s mapred API:          org.apache.accumulo.hadoop.mapred.AccumuloInputFormat      org.apache.accumulo.hadoop.mapred.AccumuloOutputFormat      org.apache.accumulo.hadoop.mapred.AccumuloFileOutputFormat      Before 2.0, the MapReduce API resided in the org.apache.accumulo.core.client package of the accumulo-core jar.While this old API still exists and can be used, it has been deprecated and will be removed eventually.Configure dependencies for your MapReduce jobBefore 2.0, Accumulo used the same versions for dependencies (such as Guava) as Hadoop. This allowedMapReduce jobs to run with both Accumulo’s &amp;amp; Hadoop’s dependencies on the classpath.Since 2.0, Accumulo no longer has the same versions for dependencies as Hadoop. While this allowsAccumulo to update its dependencies more frequently, it can cause problems if both Accumulo’s &amp;amp;Hadoop’s dependencies are on the classpath of the MapReduce job. When launching a MapReduce job thatuses Accumulo, you should build a shaded jar with all of your dependencies and complete the followingsteps so YARN only includes Hadoop code (and not all of Hadoop’s dependencies) when running your MapReduce job:      Set export HADOOP_USE_CLIENT_CLASSLOADER=true in your environment before submittingyour job with yarn command.        Set the following in your Job configuration.     job.getConfiguration().set(&quot;mapreduce.job.classloader&quot;, &quot;true&quot;);      Read input from an Accumulo tableFollow the steps below to create a MapReduce job that reads from an Accumulo table:      Create a Mapper with the following class parameterization.     class MyMapper extends Mapper&amp;lt;Key,Value,WritableComparable,Writable&amp;gt; {     public void map(Key k, Value v, Context c) {         // transform key and value data here     } }            Configure your MapReduce job to use AccumuloInputFormat.     Job job = Job.getInstance(); job.setInputFormatClass(AccumuloInputFormat.class); Properties props = Accumulo.newClientProperties().to(&quot;myinstance&quot;,&quot;zoo1,zoo2&quot;)                         .as(&quot;user&quot;, &quot;passwd&quot;).build(); AccumuloInputFormat.configure().clientProperties(props).table(table).store(job);        AccumuloInputFormat has optional settings.     List&amp;lt;Range&amp;gt; ranges = new ArrayList&amp;lt;Range&amp;gt;(); Collection&amp;lt;IteratorSetting.Column&amp;gt; columns = new ArrayList&amp;lt;IteratorSetting.Column&amp;gt;(); // populate ranges &amp;amp; columns IteratorSetting is = new IteratorSetting(30, RexExFilter.class); RegExFilter.setRegexs(is, &quot;.*suffix&quot;, null, null, null, true); AccumuloInputFormat.configure().clientProperties(props).table(table)     .auths(Authorizations.EMPTY) // optional: default to user&#39;s auths if not set     .ranges(ranges)              // optional: only read specified ranges     .fetchColumns(columns)       // optional: only read specified columns     .addIterator(is)             // optional: add iterator that matches row IDs     .store(job);        AccumuloInputFormat can also be configured to read from multiple Accumulo tables.     Job job = Job.getInstance(); job.setInputFormatClass(AccumuloInputFormat.class); Properties props = Accumulo.newClientProperties().to(&quot;myinstance&quot;,&quot;zoo1,zoo2&quot;)                         .as(&quot;user&quot;, &quot;passwd&quot;).build(); AccumuloInputFormat.configure().clientProperties(props)     .table(&quot;table1&quot;).auths(Authorizations.EMPTY).ranges(tableOneRanges)     .table(&quot;table2&quot;).auths(Authorizations.EMPTY).ranges(tableTwoRanges)     .store(job);        If reading from multiple tables, the table name can be retrieved from the input split:     class MyMapper extends Mapper&amp;lt;Key,Value,WritableComparable,Writable&amp;gt; {     public void map(Key k, Value v, Context c) {         RangeInputSplit split = (RangeInputSplit)c.getInputSplit();         String tableName = split.getTableName();         // do something with table name     } }      Write output to an Accumulo tableFollow the steps below to write to an Accumulo table from a MapReduce job.  Create a Reducer with the following class parameterization. The key emitted from the Reducer identifies the table to which the mutation is sent. This allows a single Reducer to write to more than one table if desired. A default table can be configured using the AccumuloOutputFormat, in which case the output table name does not have to be passed to the Context object within the Reducer.     class MyReducer extends Reducer&amp;lt;WritableComparable, Writable, Text, Mutation&amp;gt; {     public void reduce(WritableComparable key, Iterable&amp;lt;Text&amp;gt; values, Context c) {         Mutation m;         // create the mutation based on input key and value         c.write(new Text(&quot;output-table&quot;), m);     } }        The Text object passed as the output should contain the name of the table to which this mutation should be applied. The Text can be null in which case the mutation will be applied to the default table name specified in the AccumuloOutputFormat options.    Configure your MapReduce job to use AccumuloOutputFormat.     Job job = Job.getInstance(); job.setOutputFormatClass(AccumuloOutputFormat.class); Properties props = Accumulo.newClientProperties().to(&quot;myinstance&quot;,&quot;zoo1,zoo2&quot;)                         .as(&quot;user&quot;, &quot;passwd&quot;).build(); AccumuloOutputFormat.configure().clientProperties(props)     .defaultTable(&quot;mytable&quot;).store(job);      Write output to RFiles in HDFSFollow the steps below to have a MapReduce job output to RFiles in HDFS. These filescan then be bulk imported into Accumulo:  Create a Mapper or Reducer with Key &amp;amp; Value as output parameters.     class MyReducer extends Reducer&amp;lt;WritableComparable, Writable, Key, Value&amp;gt; {     public void reduce(WritableComparable key, Iterable&amp;lt;Text&amp;gt; values, Context c) {         Key key;         Value value;         // create Key &amp;amp; Value based on input         c.write(key, value);     } }        Configure your MapReduce job to use AccumuloFileOutputFormat.     Job job = Job.getInstance(); job.setOutputFormatClass(AccumuloFileOutputFormat.class); AccumuloFileOutputFormat.configure()     .outputPath(new Path(&quot;hdfs://localhost:8020/myoutput/&quot;)).store(job);      Example CodeThe Accumulo Examples repo has several MapReduce examples:  wordcount - Uses MapReduce and Accumulo to do a word count on text files  regex - Uses MapReduce and Accumulo to find data using regular expressions  rowhash - Uses MapReduce to read a table and write to a new column in the same table  tabletofile - Uses MapReduce to read a table and write one of its columns to a file in HDFS  uniquecols - Uses MapReduce to count unique columns in Accumulo",
      "url": " /docs/2.x/development/mapreduce",
      "categories": "development"
    },
  
    "docs-2-x-development-proxy": {
      "title": "Proxy",
      "content": "The Accumulo Proxy allows the interaction with Accumulo with languages other than Java.A proxy server is provided in the codebase and a client can further be generated.The proxy API can also be used instead of the traditional AccumuloClient class toprovide a single TCP port in which clients can be securely routed through a firewall,without requiring access to all tablet servers in the cluster.PrerequisitesThe proxy server can live on any node in which the basic client API would work. Thatmeans it must be able to communicate with the Manager, ZooKeepers, NameNode, and theDataNodes. A proxy client only needs the ability to communicate with the proxy server.Running the Proxy ServerTo run Accumulo Proxy server, first clone the repository:git clone https://github.com/apache/accumulo-proxyNext, follow the instructions in the Proxy README.md or use Uno to run the proxy.To run the Proxy using Uno, configure uno.conf to start the Proxy by setting theconfiguration below:export POST_RUN_PLUGINS=&quot;accumulo-proxy&quot;export PROXY_REPO=/path/to/accumulo-proxyProxy Client ExamplesThe following examples show proxy clients written in Java, Ruby, and Python.RubyThe Accumulo Proxy repo has an example ruby client along with instructions on howto run it.PythonThe Accumulo Proxy repo has two example Python scripts that can be run using these instructions:  basic client - creates a table, writes data to it, and then reads it  namespace client - shows how to manage Accumulo namespaces.JavaUsers may want to write a Java client to the proxy to restrict access to the cluster.",
      "url": " /docs/2.x/development/proxy",
      "categories": "development"
    },
  
    "docs-2-x-development-sampling": {
      "title": "Sampling",
      "content": "OverviewAccumulo has the ability to generate and scan a per table set of sample data.This sample data is kept up to date as a table is mutated.  What key values areplaced in the sample data is configurable per table.This feature can be used for query estimation and optimization.  For an exampleof estimation, assume an Accumulo table is configured to generate a samplecontaining one millionth of the table’s data. If a query is executed against thesample and returns one thousand results, then the same query against all thedata would probably return a billion results.  A nice property of havingAccumulo generate the sample is that its always up to date.  So estimationswill be accurate even when querying the most recently written data.An example of a query optimization is an iterator using sample data to get anestimate, and then making decisions based on the estimate.ConfiguringIn order to use sampling, an Accumulo table must be configured with a class thatimplements Sampler along with options for that class.  For guidance onimplementing a Sampler, see the Sampler interface javadoc. Accumulo provides a fewimplementations of Sampler out of the box. For information on how to use the samplers thatship with Accumulo, look in the package org.apache.accumulo.core.client.sampleand consult the javadoc of the classes there. See the sampling examplefor examples of how to configure a Sampler on a table.Once a table is configured with a Sampler, all writes after that point willgenerate sample data.  For data written before sampling was configured, sampledata will not be present.  A compaction can be initiated that only compacts thefiles in the table that do not have sample data.  The sampling exampleshows how to do this.If the sampling configuration of a table is changed, then Accumulo will startgenerating new sample data with the new configuration.   However, old data willstill have sample data generated with the previous configuration.  A selectivecompaction can also be issued in this case to regenerate the sample data.Scanning sample dataIn order to scan sample data, use setSamplerConfiguration(...) method ofScanner or BatchScanner.  Please consult the javadoc of this method for moreinformation.Sample data can also be scanned from within an Accumulo SortedKeyValueIterator.To see how to do this, look at the example iterator referenced in the sampling example.Also, consult the javadoc on IteratorEnvironment.cloneWithSamplingEnabled().MapReduce jobs using the AccumuloInputFormat can also read sample data.  See the javadocfor samplerConfiguration() in the configure() method of AccumuloInputFormat.Scans over sample data will throw a SampleNotPresentException in the following cases :  sample data is not present,  sample data is present but was generated with multiple configurations  sample data is partially presentSo a scan over sample data can only succeed if all data written has sample datagenerated with the same configuration.Bulk importWhen generating rfiles to bulk import into Accumulo, those rfiles can containsample data.  To use this feature, look at the javadoc of sampler() in the configure()method of AccumuloFileOutputFormat.",
      "url": " /docs/2.x/development/sampling",
      "categories": "development"
    },
  
    "docs-2-x-development-spark": {
      "title": "Spark",
      "content": "Apache Spark applications can read from and write to Accumulo tables.Before reading this documentation, it may help to review the MapReducedocumentation as API created for MapReduce jobs is used by Spark.This documentation references code from the Accumulo Spark example.General configuration      Create a shaded jar with your Spark code and all of your dependencies (excludingSpark and Hadoop). When creating the shaded jar, you should relocate Guavaas Accumulo uses a different version. The pom.xml in the Spark example isa good reference and can be used as a starting point for a Spark application.        Submit the job by running spark-submit with your shaded jar. You should passin the location of your accumulo-client.properties that will be used to connectto your Accumulo instance.     $SPARK_HOME/bin/spark-submit    --class com.my.spark.job.MainClass    --master yarn    --deploy-mode client    /path/to/spark-job-shaded.jar    /path/to/accumulo-client.properties      Reading from Accumulo tableApache Spark can read from an Accumulo table by using AccumuloInputFormat.Job job = Job.getInstance();AccumuloInputFormat.configure().clientProperties(props).table(inputTable).store(job);JavaPairRDD&amp;lt;Key,Value&amp;gt; data = sc.newAPIHadoopRDD(job.getConfiguration(),    AccumuloInputFormat.class, Key.class, Value.class);Writing to Accumulo tableThere are two ways to write to an Accumulo table in Spark applications.Use a BatchWriterWrite your data to Accumulo by creating an AccumuloClient for each partition and writing alldata in the partition using a BatchWriter.// Spark will automatically serialize this properties object and send it to each partitionProperties props = Accumulo.newClientProperties()                    .from(&quot;/path/to/accumulo-client.properties&quot;).build();JavaPairRDD&amp;lt;Key, Value&amp;gt; dataToWrite = ... ;dataToWrite.foreachPartition(iter -&amp;gt; {  // Create client inside partition so that Spark does not attempt to serialize it.  try (AccumuloClient client = Accumulo.newClient().from(props).build();       BatchWriter bw = client.createBatchWriter(outputTable)) {    iter.forEachRemaining(kv -&amp;gt; {      Key key = kv._1;      Value val = kv._2;      Mutation m = new Mutation(key.getRow());      m.at().family(key.getColumnFamily()).qualifier(key.getColumnQualifier())          .visibility(key.getColumnVisibility()).timestamp(key.getTimestamp()).put(val);      bw.addMutation(m);    });  }});Using Bulk ImportPartition your data and write it to RFiles. The AccumuloRangePartitioner found in the AccumuloSpark example can be used for partitioning data. After your data has been written to an outputdirectory using AccumuloFileOutputFormat as RFiles, bulk import this directory into Accumulo.// Write Spark output to HDFSJavaPairRDD&amp;lt;Key, Value&amp;gt; dataToWrite = ... ;Job job = Job.getInstance();AccumuloFileOutputFormat.configure().outputPath(outputDir).store(job);Partitioner partitioner = new AccumuloRangePartitioner(&quot;3&quot;, &quot;7&quot;);JavaPairRDD&amp;lt;Key, Value&amp;gt; partData = dataPlus5K.repartitionAndSortWithinPartitions(partitioner);partData.saveAsNewAPIHadoopFile(outputDir.toString(), Key.class, Value.class,    AccumuloFileOutputFormat.class);// Bulk import RFiles in HDFS into Accumulotry (AccumuloClient client = Accumulo.newClient().from(props).build()) {  client.tableOperations().importDirectory(outputDir.toString()).to(outputTable).load();}Reference  Spark example - Example Spark application that reads from and writes to Accumulo  MapReduce - Documentation on reading/writing to Accumulo using MapReduce  Apache Spark - Spark project website",
      "url": " /docs/2.x/development/spark",
      "categories": "development"
    },
  
    "docs-2-x-development-summaries": {
      "title": "Summary Statistics",
      "content": "OverviewAccumulo has the ability to generate summary statistics about data in a tableusing user defined functions.  Currently, these statistics are only generated fordata written to files.  Data recently written to Accumulo that is still inmemory will not contribute to summary statistics.This feature can be used to inform a user about what data is in their table.Summary statistics can also be used by compaction strategies to make decisionsabout which files to compact.Summary data is stored in each file Accumulo produces.  Accumulo can gathersummary information from across a cluster merging it along the way.  In orderfor this to be fast, the summary information should fit in cache.  There is adedicated cache for summary data on each tserver with a configurable size.  Inorder for summary data to fit in cache, it should probably be small.For information on writing a custom summarizer see the javadoc of the Summarizerclass. The package org.apache.accumulo.core.client.summary.summarizerscontains summarizer implementations that ship with Accumulo and can be configured for use.InaccuraciesSummary data can be inaccurate when files are missing summary data or whenfiles have extra summary data. Files can contain data outside of a tabletsboundaries. This can happen as result of bulk imported files and tablet splits.When this happens, those files could contain extra summary information.Accumulo offsets this some by storing summary information for multiple rowranges per a file.  However, the ranges are not granular enough to completelyoffset extra data.Any source of inaccuracies is reported when summary information is requested.In the shell examples below, this can be seen on the File Statistics line.For files missing summary information, the compact command in the shell has a--sf-no-summary option.  This options compacts files that do not have thesummary information configured for the table.  The compact command also has the--sf-extra-summary option which will compact files with extra summaryinformation.ConfiguringThe following tablet server and table properties configure summarization.  tserver.cache.summary.size  tserver.summary.partition.threads  tserver.summary.remote.threads  tserver.summary.retrieval.threads  table.summarizer.*)  table.file.summary.maxSizePermissionsBecause summary data may be derived from sensitive data, requesting summary datarequires a special permission.  Users must have the table permissionGET_SUMMARIES in order to retrieve summary data.Bulk importWhen generating RFiles to bulk import into Accumulo, those RFiles can containsummary data.  To use this feature, look at the javadoc of summarizers() in the configure() methodof AccumuloFileOutputFormat.  Also, the RFileclass has options for creating RFiles with embedded summary data.ExamplesThis example walks through using summarizers in the Accumulo shell.  Below, atable is created and some data is inserted to summarize.root@uno&amp;gt; createtable summary_testroot@uno summary_test&amp;gt; setauths -u root -s PI,GEO,TIMEroot@uno summary_test&amp;gt; insert 3b503bd name last Doeroot@uno summary_test&amp;gt; insert 3b503bd name first Johnroot@uno summary_test&amp;gt; insert 3b503bd contact address &quot;123 Park Ave, NY, NY&quot; -l PI&amp;amp;GEOroot@uno summary_test&amp;gt; insert 3b503bd date birth &quot;1/11/1942&quot; -l PI&amp;amp;TIMEroot@uno summary_test&amp;gt; insert 3b503bd date married &quot;5/11/1962&quot; -l PI&amp;amp;TIMEroot@uno summary_test&amp;gt; insert 3b503bd contact home_phone 1-123-456-7890 -l PIroot@uno summary_test&amp;gt; insert d5d18dd contact address &quot;50 Lake Shore Dr, Chicago, IL&quot; -l PI&amp;amp;GEOroot@uno summary_test&amp;gt; insert d5d18dd name first Janeroot@uno summary_test&amp;gt; insert d5d18dd name last Doeroot@uno summary_test&amp;gt; insert d5d18dd date birth 8/15/1969 -l PI&amp;amp;TIMEroot@uno summary_test&amp;gt; scan -s PI,GEO,TIME3b503bd contact:address [PI&amp;amp;GEO]    123 Park Ave, NY, NY3b503bd contact:home_phone [PI]    1-123-456-78903b503bd date:birth [PI&amp;amp;TIME]    1/11/19423b503bd date:married [PI&amp;amp;TIME]    5/11/19623b503bd name:first []    John3b503bd name:last []    Doed5d18dd contact:address [PI&amp;amp;GEO]    50 Lake Shore Dr, Chicago, ILd5d18dd date:birth [PI&amp;amp;TIME]    8/15/1969d5d18dd name:first []    Janed5d18dd name:last []    DoeAfter inserting the data, summaries are requested below.  No summaries are returned.root@uno summary_test&amp;gt; summariesThe visibility summarizer is configured below and the table is flushed.Flushing the table creates a file creating summary data in the process. Thesummary data returned counts how many times each column visibility occurred.The statistics with a c: prefix are visibilities.  The others are genericstatistics created by the CountingSummarizer that VisibilitySummarizer extends.root@uno summary_test&amp;gt; config -t summary_test -s table.summarizer.vis=org.apache.accumulo.core.client.summary.summarizers.VisibilitySummarizerroot@uno summary_test&amp;gt; summariesroot@uno summary_test&amp;gt; flush -w2017-02-24 19:54:46,090 [shell.Shell] INFO : Flush of table summary_test completed.root@uno summary_test&amp;gt; summariesSummarizer         : org.apache.accumulo.core.client.summary.summarizers.VisibilitySummarizer vis {}File Statistics    : [total:1, missing:0, extra:0, large:0]Summary Statistics :   c:                                                           = 4   c:PI                                                         = 1   c:PI&amp;amp;GEO                                                     = 2   c:PI&amp;amp;TIME                                                    = 3   emitted                                                      = 10   seen                                                         = 10   tooLong                                                      = 0   tooMany                                                      = 0VisibilitySummarizer has an option maxCounters that determines the max numberof column visibilities it will track.  Below this option is set and compactionis forced to regenerate summary data.  The new summary data only has threevisibilities and now the tooMany statistic is 4.  This is the number ofvisibilities that were not counted.root@uno summary_test&amp;gt; config -t summary_test -s table.summarizer.vis.opt.maxCounters=3root@uno summary_test&amp;gt; compact -w2017-02-24 19:54:46,267 [shell.Shell] INFO : Compacting table ...2017-02-24 19:54:47,127 [shell.Shell] INFO : Compaction of table summary_test completed for given rangeroot@uno summary_test&amp;gt; summaries   Summarizer         : org.apache.accumulo.core.client.summary.summarizers.VisibilitySummarizer vis {maxCounters=3}   File Statistics    : [total:1, missing:0, extra:0, large:0]   Summary Statistics :      c:PI                                                         = 1      c:PI&amp;amp;GEO                                                     = 2      c:PI&amp;amp;TIME                                                    = 3      emitted                                                      = 10      seen                                                         = 10      tooLong                                                      = 0      tooMany                                                      = 4Another summarizer is configured below that tracks the number of deletes.  Also,a compaction strategy that uses this summary data is configured.  TheTooManyDeletesCompactionStrategy will force a compaction of the tablet whenthe ratio of deletes to non-deletes is over 25%.  This threshold isconfigurable.  Below a delete is added and it’s reflected in the statistics.  Inthis case there is 1 delete and 10 non-deletes, not enough to force acompaction of the tablet.root@uno summary_test&amp;gt; config -t summary_test -s table.summarizer.del=org.apache.accumulo.core.client.summary.summarizers.DeletesSummarizerroot@uno summary_test&amp;gt; compact -w2017-02-24 19:54:47,282 [shell.Shell] INFO : Compacting table ...2017-02-24 19:54:49,236 [shell.Shell] INFO : Compaction of table summary_test completed for given rangeroot@uno summary_test&amp;gt; config -t summary_test -s table.compaction.major.ratio=10root@uno summary_test&amp;gt; config -t summary_test -s table.majc.compaction.strategy=org.apache.accumulo.tserver.compaction.strategies.TooManyDeletesCompactionStrategyroot@uno summary_test&amp;gt; deletemany -r d5d18dd -c date -f[DELETED] d5d18dd date:birth [PI&amp;amp;TIME]root@uno summary_test&amp;gt; flush -w2017-02-24 19:54:49,686 [shell.Shell] INFO : Flush of table summary_test completed.root@uno summary_test&amp;gt; summaries Summarizer         : org.apache.accumulo.core.client.summary.summarizers.VisibilitySummarizer vis {maxCounters=3} File Statistics    : [total:2, missing:0, extra:0, large:0] Summary Statistics :    c:PI                                                         = 1    c:PI&amp;amp;GEO                                                     = 2    c:PI&amp;amp;TIME                                                    = 4    emitted                                                      = 11    seen                                                         = 11    tooLong                                                      = 0    tooMany                                                      = 4 Summarizer         : org.apache.accumulo.core.client.summary.summarizers.DeletesSummarizer del {} File Statistics    : [total:2, missing:0, extra:0, large:0] Summary Statistics :    deletes                                                      = 1    total                                                        = 11Some more deletes are added and the table is flushed below.  This results in 4deletes and 10 non-deletes, which triggers a full compaction.  A fullcompaction of all files is the only time when delete markers are dropped.  Thecompaction ratio was set to 10 above to show that the number of files did nottrigger the compaction.   After the compaction there no deletes 6 non-deletes.root@uno summary_test&amp;gt; deletemany -r d5d18dd -f[DELETED] d5d18dd contact:address [PI&amp;amp;GEO][DELETED] d5d18dd name:first [][DELETED] d5d18dd name:last []root@uno summary_test&amp;gt; flush -w2017-02-24 19:54:52,800 [shell.Shell] INFO : Flush of table summary_test completed.root@uno summary_test&amp;gt; summaries Summarizer         : org.apache.accumulo.core.client.summary.summarizers.VisibilitySummarizer vis {maxCounters=3} File Statistics    : [total:1, missing:0, extra:0, large:0] Summary Statistics :    c:PI                                                         = 1    c:PI&amp;amp;GEO                                                     = 1    c:PI&amp;amp;TIME                                                    = 2    emitted                                                      = 6    seen                                                         = 6    tooLong                                                      = 0    tooMany                                                      = 2 Summarizer         : org.apache.accumulo.core.client.summary.summarizers.DeletesSummarizer del {} File Statistics    : [total:1, missing:0, extra:0, large:0] Summary Statistics :    deletes                                                      = 0    total                                                        = 6root@uno summary_test&amp;gt;",
      "url": " /docs/2.x/development/summaries",
      "categories": "development"
    },
  
    "docs-2-x-getting-started-clients": {
      "title": "Accumulo Clients",
      "content": "Creating Client CodeIf you are using Maven to create Accumulo client code, add the following dependency to your pom:&amp;lt;dependency&amp;gt;  &amp;lt;groupId&amp;gt;org.apache.accumulo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;accumulo-core&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;2.1.3&amp;lt;/version&amp;gt;&amp;lt;/dependency&amp;gt;When writing code that uses Accumulo, only use the Accumulo Public API.The accumulo-core artifact includes implementation code that falls outside thePublic API and should be avoided.Creating an Accumulo ClientBefore creating an Accumulo client, you will need the following information:  Accumulo instance name  Zookeeper connection string  Accumulo username &amp;amp; passwordThe AccumuloClient object is the main entry point for Accumulo clients. It can be created using oneof the following methods:  Using the accumulo-client.properties file (a template can be found in the conf/ directoryof the tarball distribution):     AccumuloClient client = Accumulo.newClient()                           .from(&quot;/path/to/accumulo-client.properties&quot;).build();        Using the builder methods of AccumuloClient:     AccumuloClient client = Accumulo.newClient()                           .to(&quot;myinstance&quot;, &quot;zookeeper1,zookeeper2&quot;)                           .as(&quot;myuser&quot;, &quot;mypassword&quot;).build();        Using a Java Properties object.     Properties props = new Properties() props.put(&quot;instance.name&quot;, &quot;myinstance&quot;) props.put(&quot;instance.zookeepers&quot;, &quot;zookeeper1,zookeeper2&quot;) props.put(&quot;auth.type&quot;, &quot;password&quot;) props.put(&quot;auth.principal&quot;, &quot;myuser&quot;) props.put(&quot;auth.token&quot;, &quot;mypassword&quot;) AccumuloClient client = Accumulo.newClient().from(props).build();      If an accumulo-client.properties file or a Java Properties object is used to create an AccumuloClient, the followingclient properties must be set:  instance.name - Name of Accumulo instance to connect to  instance.zookeepers - ZooKeeper connection information for this Accumulo instance  auth.type - Authentication method. Possible values are password, kerberos, or authentication token class (i.e PasswordToken, org.apache.accumulo.core.client.security.tokens.PasswordToken)  auth.principal - Accumulo principal/username  auth.token - Token associated with auth.type. See table for mapping below:            auth.type      expected auth.token      example auth.token                  password      Password string      mypassword              kerberos      Path to Kerberos keytab      /path/to/keytab              Authentication token class      Base64 encoded token      AAAAGh+LCAAAAAAAAAArTk0uSi0BAOXoolwGAAAA      If a token class is used for auth.type, you can create a Base64 encoded token using the accumulo create-token command.$ accumulo create-tokenUsername (aka principal): rootthe password for the principal: ******auth.type = org.apache.accumulo.core.client.security.tokens.PasswordTokenauth.principal = rootauth.token = AAAAGh+LCAAAAAAAAAArTk0uSi0BAOXoolwGAAAAAuthenticationWhen creating an AccumuloClient, the user must be authenticated using one of the followingimplementations of AuthenticationToken below:  PasswordToken is the must commonly used implementation.  CredentialProviderToken leverages the Hadoop CredentialProviders (new in Hadoop 2.6).For example, the CredentialProviderToken can be used in conjunction with a Java KeyStore toalleviate passwords stored in cleartext. When stored in HDFS, a single KeyStore can be used acrossan entire instance. Be aware that KeyStores stored on the local filesystem must be made availableto all nodes in the Accumulo cluster.      KerberosToken can be provided to use the authentication provided by Kerberos. Using Kerberosrequires external setup and additional configuration, but provides a single point of authenticationthrough HDFS, YARN and ZooKeeper and allowing for password-less authentication with Accumulo.     KerberosToken token = new KerberosToken(); AccumuloClient client = Accumulo.newClient().to(&quot;myinstance&quot;, &quot;zookeeper1,zookeper2&quot;)                           .as(token.getPrincipal(), token).build();      Writing DataWith a AccumuloClient created, it can be used to create objects (like the BatchWriter) forreading and writing from Accumulo:BatchWriter writer = client.createBatchWriter(&quot;table&quot;);Data is written to Accumulo by creating Mutation objects that represent all thechanges to the columns of a single row. The changes are made atomically in theTabletServer. Clients then add Mutations to a BatchWriter which submits them tothe appropriate TabletServers.The code below shows how a Mutation is created.Mutation mutation = new Mutation(&quot;row1&quot;);mutation.at().family(&quot;myColFam1&quot;).qualifier(&quot;myColQual1&quot;).visibility(&quot;public&quot;).put(&quot;myValue1&quot;);mutation.at().family(&quot;myColFam2&quot;).qualifier(&quot;myColQual2&quot;).visibility(&quot;public&quot;).put(&quot;myValue2&quot;);BatchWriterThe BatchWriter is highly optimized to send Mutations to multiple TabletServersand automatically batches Mutations destined for the same TabletServer toamortize network overhead. Care must be taken to avoid changing the contents ofany Object passed to the BatchWriter since it keeps objects in memory whilebatching.The code below shows how a Mutation is added to a BatchWriter:try (BatchWriter writer = client.createBatchWriter(&quot;mytable&quot;)) {  Mutation m = new Mutation(&quot;row1&quot;);  m.at().family(&quot;myfam&quot;).qualifier(&quot;myqual&quot;).visibility(&quot;public&quot;).put(&quot;myval&quot;);  writer.addMutation(m);}For more example code, see the batch writing and scanning example.ConditionalWriterThe ConditionalWriter enables efficient, atomic read-modify-write operations onrows.  The ConditionalWriter writes special Mutations which have a list of percolumn conditions that must all be met before the mutation is applied.  Theconditions are checked in the tablet server while a row lock isheld (Mutations written by the BatchWriter will not obtain a rowlock).  The conditions that can be checked for a column are equality andabsence.  For example a conditional mutation can require that column A isabsent inorder to be applied.  Iterators can be applied when checkingconditions.  Using iterators, many other operations besides equality andabsence can be checked.  For example, using an iterator that converts valuesless than 5 to 0 and everything else to 1, it’s possible to only apply amutation when a column is less than 5.In the case when a tablet server dies after a client sent a conditionalmutation, it’s not known if the mutation was applied or not.  When this happensthe ConditionalWriter reports a status of UNKNOWN for the ConditionalMutation.In many cases this situation can be dealt with by simply reading the row againand possibly sending another conditional mutation.  If this is not sufficient,then a higher level of abstraction can be built by storing transactionalinformation within a row.See the reservations example for example code that uses the ConditionalWriter.DurabilityBy default, Accumulo writes out any updates to the Write-Ahead Log (WAL). Every changegoes into a file in HDFS and is sync’d to disk for maximum durability. Inthe event of a failure, writes held in memory are replayed from the WAL. Likeall files in HDFS, this file is also replicated. Sending updates to thereplicas, and waiting for a permanent sync to disk can significantly slow down write speeds.Accumulo allows users to use less tolerant forms of durability when writing.These levels are:  none - no durability guarantees are made, the WAL is not used  log - the WAL is used, but not flushed; loss of the server probably means recent writes are lost  flush - updates are written to the WAL, and flushed out to replicas; loss of a single server is unlikely to result in data loss.  sync - updates are written to the WAL, and synced to disk on all replicas before the write is acknowledge. Data will not be lost even if the entire cluster suddenly loses power.Durability can be set in multiple ways:  The default durability of all tables can be set using table.durability.     root@uno&amp;gt; config -s table.durability=flush        The default durability of a table can be overriden by setting table.durability for that table.     root@uno&amp;gt; config -t mytable -s table.durability=sync        When creating an AccumuloClient, the default durability can be overridden using withBatchWriterConfig()or by setting batch.writer.durability in accumulo-client.properties.      When a BatchWriter or ConditionalWriter is created, the durability settings above will be overriddenby the BatchWriterConfig that is passed in.     BatchWriterConfig cfg = new BatchWriterConfig(); // We don&#39;t care about data loss with these writes: // This is DANGEROUS: cfg.setDurability(Durability.NONE); BatchWriter bw = client.createBatchWriter(table, cfg);      Reading DataAccumulo is optimized to quickly retrieve the value associated with a given key, andto efficiently return ranges of consecutive keys and their associated values.ScannerTo retrieve data, create a Scanner using AccumuloClient. A Scanner acts like an Iterator overkeys and values in the table.If a Scanner is created without Authorizations, it uses all Authorizations grantedto the user that created the AccumuloClient:Scanner s = client.createScanner(&quot;table&quot;);A scanner can also be created to only use a subset of a user’s Authorizations.Scanner s = client.createScanner(&quot;table&quot;, new Authorizations(&quot;public&quot;));Scanners can be configured to start and stop at particular keys, andto return a subset of the columns available.// return data with visibilities that match specified authsAuthorizations auths = new Authorizations(&quot;public&quot;);try (Scanner scan = client.createScanner(&quot;table&quot;, auths)) {  scan.setRange(new Range(&quot;harry&quot;,&quot;john&quot;));  scan.fetchColumnFamily(&quot;attributes&quot;);  for (Entry&amp;lt;Key,Value&amp;gt; entry : scan) {    Text row = entry.getKey().getRow();    Value value = entry.getValue();  }}Isolated ScannerAccumulo supports the ability to present an isolated view of rows whenscanning. There are three possible ways that a row could change in Accumulo :  a mutation applied to a table  iterators executed as part of a minor or major compaction  bulk import of new filesIsolation guarantees that either all or none of the changes made by theseoperations on a row are seen. Use the IsolatedScanner to obtain an isolatedview of an Accumulo table. When using the regular scanner it is possible to seea non isolated view of a row. For example if a mutation modifies threecolumns, it is possible that you will only see two of those modifications.With the isolated scanner either all three of the changes are seen or none.The IsolatedScanner buffers rows on the client side so a large row will notcrash a tablet server. By default, rows are buffered in memory, but the usercan easily supply their own buffer if they wish to buffer to disk when rows arelarge.See the isolation example for example code that uses the IsolatedScanner.BatchScannerFor some types of access, it is more efficient to retrieve several rangessimultaneously. This arises when accessing a set of rows that are not consecutivewhose IDs have been retrieved from a secondary index, for example.The BatchScanner is configured similarly to the Scanner; it can be configured toretrieve a subset of the columns available, but rather than passing a single Range,BatchScanners accept a set of Ranges. It is important to note that the keys returnedby a BatchScanner are not in sorted order since the keys streamed are from multipleTabletServers in parallel.ArrayList&amp;lt;Range&amp;gt; ranges = new ArrayList&amp;lt;Range&amp;gt;();// populate list of ranges ...try (BatchScanner bscan = client.createBatchScanner(&quot;table&quot;, auths, 10)) {  bscan.setRanges(ranges);  bscan.fetchColumnFamily(&quot;attributes&quot;);  for (Entry&amp;lt;Key,Value&amp;gt; entry : bscan) {    System.out.println(entry.getValue());  }}For more example code, see the batch writing and scanning example.At this time, there is no client side isolation support for the BatchScanner.You may consider using the WholeRowIterator with the BatchScanner to achieveisolation. The drawback of this approach is that entire rows are read intomemory on the server side. If a row is too big, it may crash a tablet server.Running Client CodeThere are multiple ways to run Java code that use Accumulo. Below is a listof the different ways to execute client code.  build and execute an uber jar  add accumulo classpath to your Java classpath  use the accumulo commandBuild and execute an uber jarIf you have included accumulo-core as dependency in your pom, you can build an uber jarusing the Maven assembly or shade plugin and use it to run Accumulo client code. When buildingan uber jar, you should set the versions of any Hadoop dependencies in your pom to match theversion running on your cluster.Add ‘accumulo classpath’ to your Java classpathTo run Accumulo client code using the java command, use the accumulo classpath commandto include all of Accumulo’s dependencies on your classpath:java -classpath /path/to/my.jar:/path/to/dep.jar:$(accumulo classpath) com.my.Main arg1 arg2Use the accumulo commandAnother option for running your code is to use the Accumulo script which can execute amain class (if it exists on its classpath):accumulo com.foo.Client arg1 arg2While the Accumulo script will add all of Accumulo’s dependencies to the classpath, youwill need to add any jars that your create or depend on beyond what Accumulo alreadydepends on. This can be accomplished by either adding the jars to the lib/ext directoryof your Accumulo installation or by adding jars to the CLASSPATH variable before callingthe accumulo command.export CLASSPATH=/path/to/my.jar:/path/to/dep.jar; accumulo com.foo.Client arg1 arg2Additional DocumentationThis page covers Accumulo client basics.  Below are links to additional documentation that may be useful when creating Accumulo clients:  Iterators - Server-side programming mechanism that can modify key/value pairs at various points in data management process  Proxy - Documentation for interacting with Accumulo using non-Java languages through a proxy server  MapReduce - Documentation for reading and writing to Accumulo using MapReduce.",
      "url": " /docs/2.x/getting-started/clients",
      "categories": "getting-started"
    },
  
    "docs-2-x-getting-started-design": {
      "title": "Design",
      "content": "BackgroundThe design of Apache Accumulo is inspired by Google’s BigTable paper.Data ModelAccumulo provides a richer data model than simple key-value stores, but is not afully relational database. Data is represented as key-value pairs, where the key andvalue are comprised of the following elements:All elements of the Key and the Value are represented as byte arrays except forTimestamp, which is a Long. Accumulo sorts keys by element and lexicographicallyin ascending order. Timestamps are sorted in descending order so that laterversions of the same Key appear first in a sequential scan. Tables consist of a set ofsorted key-value pairs.ArchitectureAccumulo is a distributed data storage and retrieval system and as such consists ofseveral architectural components, some of which run on many individual servers.Much of the work Accumulo does involves maintaining certain properties of thedata, such as organization, availability, and integrity, across many commodity-classmachines.ComponentsAn instance of Accumulo includes many TabletServers, one Garbage Collector process,one Manager server and many Clients.Tablet ServerThe TabletServer manages some subset of all the tablets (partitions of tables). This includes receiving writes from clients, persisting writes to awrite-ahead log, sorting new key-value pairs in memory, periodicallyflushing sorted key-value pairs to new files in HDFS, and respondingto reads from clients, forming a sorted merge view of all keys andvalues from all the files it has created and the sorted in-memorystore.TabletServers also perform recovery of a tabletthat was previously on a server that failed, reapplying any writesfound in the write-ahead log to the tablet.Garbage CollectorAccumulo processes will share files stored in HDFS. Periodically, the GarbageCollector will identify files that are no longer needed by any process, anddelete them. Multiple garbage collectors can be run to provide hot-standby support.They will perform leader election among themselves to choose a single active instance.ManagerThe Accumulo Manager is responsible for detecting and responding to TabletServerfailure. It tries to balance the load across TabletServer by assigning tablets carefullyand instructing TabletServers to unload tablets when necessary. The Manager ensures alltablets are assigned to one TabletServer each, and handles table creation, alteration,and deletion requests from clients. The Manager also coordinates startup, gracefulshutdown and recovery of changes in write-ahead logs when Tablet servers fail.Multiple managers may be run. The managers will choose among themselves a single manager,and the others will become backups if the manager should fail.TracerThe Accumulo Tracer process supports the distributed timing API provided by Accumulo.One to many of these processes can be run on a cluster which will write the timinginformation to a given Accumulo table for future reference. See thetracing documentation for more information.MonitorThe Accumulo Monitor is a web application that provides a wealth of information aboutthe state of an instance. The Monitor shows graphs and tables which contain informationabout read/write rates, cache hit/miss rates, and Accumulo table information such as scanrate and active/queued compactions. Additionally, the Monitor should always be the firstpoint of entry when attempting to debug an Accumulo problem as it will show high-level problemsin addition to aggregated errors from all nodes in the cluster. See the Accumulo monitor documentationfor more information.Multiple Monitors can be run to provide hot-standby support in the face of failure. Due to theforwarding of logs from remote hosts to the Monitor, only one Monitor process should be activeat one time. Leader election will be performed internally to choose the active Monitor.Compactor (experimental)The Accumulo Compactor process is an optional application that can be used to run compactionsoutside of the TabletServer. One to many Compactors can be run on a cluster and each Compactorprocess performs one compaction at a time. The Compactor registers its existence in ZooKeeperand communicates with the Compaction Coordinator to retrieve its work and to register thecompletion status of the compaction. The Compactor process will continue to perform compactionsin situations where normal in-TabletServer compactions would fail, such as TabletServer restartand Tablet re-hosting.Compaction Coordinator (experimental)The Accumulo Compaction Coordinator is an optional application that is required to run compactionsoutside of the TabletServer. The Coordinator is responsible for communicating with theTabletServers, to identify what external compaction work needs to be done, and the Compactorsto assign work, get status updates, and cancel running external compactions.Multiple Coordinators may be run. The Coordinators will choose among themselves a single active Coordinator,and the others will become backups if the active Coordinator should fail.Scan Server (experimental)The Accumulo Scan Server is an optional application that can be used to run scans on a tablet’s dataoutside of the Tablet Server. Many Scan Servers can be run on a cluster and each Scan Server may runone or more scans concurrently (dependent on configuration). Scans running in a Scan Server do not haveto be concerned about tablet re-hosting or contention with ingest, compactions, and other tabletmaintenance activities. The trade-off when using Scan Server’s is that the tablet hosted withinthe ScanServer may not contain the exact same data as the corresponding tablet hosted by theTablet Server. The Scan Server does not have any of the Tablet data that may reside within thein-memory maps and the tablet may reference files that have been compacted as tablet metadata canbe cached within the Scan Server (See Scan Server configuration properties).ClientAccumulo has a client library that can be used to write applications that write and readdata to/from Accumulo. See the Accumulo clients documentation for more information.Data ManagementAccumulo stores data in tables, which are partitioned into tablets. Tablets arepartitioned on row boundaries so that all of the columns and values for a particularrow are found together within the same tablet. The Manager assigns Tablets to oneTabletServer at a time. This enables row-level transactions to take place withoutusing distributed locking or some other complicated synchronization mechanism. Asclients insert and query data, and as machines are added and removed from thecluster, the Manager migrates tablets to ensure they remain available and that theingest and query load is balanced across the cluster.Tablet ServerWhen a write arrives at a TabletServer it is written to a Write-Ahead Log andthen inserted into a sorted data structure in memory called a MemTable. When theMemTable reaches a certain size, the TabletServer writes out the sortedkey-value pairs to a file in HDFS called an RFile. This process iscalled a minor compaction. A new MemTable is then created and the fact of thecompaction is recorded in the Write-Ahead Log.When a request to read data arrives at a TabletServer, the TabletServer does abinary search across the MemTable as well as the index blocks associated with each RFileto find the relevant values. If clients are performing a scan, several key-value pairsare returned to the client in order from the MemTable and data blocks of RFiles by performinga sorted merge as they are read. If caching is enabled for the table, any index or datablock is stored in the block cache to speed up future scans.RFileRFile (short for Relative Key File) is a file that contains Accumulo’s sorted key-valuepairs. The file is written to HDFS by Tablet Servers during a minor compaction. RFiles areorganized using the Index Sequential Access Method (ISAM). RFiles consist of data (key/value) block,index blocks (which are used to find data block), and meta blocks (which containmetadata for bloom filters and summary statistics). Data in an RFile is separated bylocality group. The diagram below shows the logical view and HDFS file view of an RFile.CompactionsIn order to manage the number of files per tablet, periodically the TabletServerperforms Major Compactions of files within a tablet, in which some set of RFilesare combined into one file. The previous files will eventually be removed by theGarbage Collector. This also provides an opportunity to permanently removedeleted key-value pairs by omitting key-value pairs suppressed by a delete entrywhen the new file is created. See the compaction documentationfor more information.SplittingWhen a table is created it has one tablet. As the table grows its initialtablet eventually splits into two tablets. It’s likely that one of thesetablets will migrate to another tablet server. As the table continues to grow,its tablets will continue to split and be migrated. The decision toautomatically split a tablet is based on the size of a tablets files. Thesize threshold at which a tablet splits is configurable per table. In additionto automatic splitting, a user can manually add split points to a table tocreate new tablets. Manually splitting a new table can parallelize reads andwrites giving better initial performance without waiting for automaticsplitting.As data is deleted from a table, tablets may shrink. Over time this can leadto small or empty tablets. To deal with this, the merging of tabletswas introduced in Accumulo 1.4.Fault-ToleranceIf a TabletServer fails, the Manager detects it and automatically reassigns the tabletsassigned from the failed server to other servers. Any key-value pairs that were inmemory at the time the TabletServer fails are automatically reapplied from the Write-AheadLog(WAL) to prevent any loss of data.Tablet servers write their WALs directly to HDFS so the logs are available to all tabletservers for recovery. To make the recovery process efficient, the updates within a log aregrouped by tablet.  TabletServers can quickly apply the mutations from the sorted logsthat are destined for the tablets they have now been assigned.TabletServer failures are noted on the Manager’s monitor page, accessible viahttp://manager-address:9995/monitor.",
      "url": " /docs/2.x/getting-started/design",
      "categories": "getting-started"
    },
  
    "docs-2-x-getting-started-features": {
      "title": "Features",
      "content": "  Table Design and Configuration  Integrity/Availability  Performance  Testing  Client API  Plugins  General Administration  Internal Data Management  On-demand Data ManagementTable Design and ConfigurationIteratorsIterators are server-side programming mechanisms that encode functions such as filtering andaggregation within the data management steps (scopes where data is read from orwritten to disk) that happen in the tablet server.Security labelsAccumulo Keys can contain a security label(called a Column Visibility) that enables expressive cell-level access control.Authorizations are passed with each query to control what data is returned to the user.Column visibilities support boolean AND and OR combinations of arbitrary strings (suchas (A&amp;amp;B)|C) and authorizations are sets of strings (such as {C,D}).ConstraintsConstraints are configurableconditions where table writes are rejected. Constraints are written in Java and configurableon a per-table basis.ShardingThrough the use of specialized iterators, Accumulo can be a parallel shardeddocument store. For example, Wikipedia could be stored and searched fordocuments containing certain words.Large RowsWhen reading rows, there is no requirement that an entire row fits into memory.NamespacesTable namespaces (since 1.6.0) allow for logical grouping and configuration of Accumulotables. By default, tables are created in a default namespace which is the empty stringto preserve the feel for how tables operate in previous versions. One application oftable namespaces is placing the Accumulo root and metadata table in an “accumulo”namespace to denote that these tables are used internally by Accumulo.Volume supportWhile Accumulo typically runs on a single HDFS instance, it supports multi-volume installations(since 1.6.0) which allow it to run over multiple disjoint HDFS instances and scale beyond the limitsof a single namenode. When used in conjunction with HDFS federation, multiple namenodescan share a pool of datanodes.Integrity/AvailabilityManager fail overMultiple managers can be configured.  Zookeeper locks are used to determinewhich manager is active.  The remaining managers simply wait for the currentmanager to lose its lock.  Current manager state is held in the metadata tableand Zookeeper.Logical timeA mechanism to ensure that server set times never go backwards, even when timeacross the cluster is incorrect. This ensures that updates and deletes are notlost. If a tablet is served on machine with time a year in the future, then thetablet will continue to issue new timestamps a year in the future, even when itmoves to another server. In this case the timestamps preserve ordering, butlose their meaning. In addition to logical time, Accumulo has managerauthoritative time. The manager averages the time of all of the tablet serversand sends this back to the tablet servers. Tablet servers use this informationto adjust the timestamps they issue. So logical time ensures ordering isalways correct and manager authoritative time tries to ensure that timestampsare meaningful.Logical Time for bulk importLogical time as described above works with streaming (batch) ingest, where thetablet server assigns the timestamp.  Logical time is also important for bulkimported data, for which the client code may be choosing a timestamp.  Accumulouses specialized system iterators to lazily set times in a bulk importedfile.  This mechanism guarantees that times set by unsynchronized multi-nodeapplications (such as those running on MapReduce) will maintain some semblanceof causal ordering.  This mitigates the problem of the time being wrong on thesystem that created the file for bulk import. These times are not set when thefile is imported, but whenever it is read by scans or compactions. At import, atime is obtained and always used by the specialized system iterator to set thattime.FATEFATE (short for Fault Tolerant Executor) is a framework for executingoperations in a fault tolerant manner. Before FATE, if the manager process died in themiddle of creating a table it could leave the system in an inconsistent state.With this new framework, if the manager dies in the middle of create table itwill continue on restart. Also, the client requesting the create table operationwill never know anything happened. The framework serializes work in Zookeeperbefore attempting to do the work. Clients start a FATE transaction, seed itwith work, and then wait for it to finish. Most table operations are executedusing this framework. Persistent, per table, read-write locks are created inZookeeper to synchronize operations across process faults.Scalable managerStores its metadata in an Accumulo table and Zookeeper.IsolationScans will not see data inserted into a row after the scan of that row begins.PerformanceRelative encodingIf consecutive keys have identical portions (row, colf, colq, or colvis), thereis a flag to indicate that a portion is the same as that of the previous key.This is applied when keys are stored on disk and when transferred over thenetwork.  Starting with 1.5, prefix erasure is supported.  When it is costeffective, prefixes repeated in subsequent key fields are not repeated.Native In-Memory MapBy default, data written is stored outside of Java managed memory into a C++ STLmap of maps.  It maps rows to columns to values.  This hierarchical structureimproves performance of inserting a mutation with multiple column values in asingle row. A custom STL allocator is used to avoid the global malloc lock andmemory fragmentation.Scan pipelineA long running Accumulo scan will eventually cause multiple threads to start.One server thread to read data from disk, one server thread to serialize andsend data, and one client thread to deserialize and read data. When pipeliningkicks in, it substantially increases scan speed while maintaining key order. Itdoes not activate for short scans.CachingRecently scanned data is cached into memoryThere are separate caches for indexes and data.  Caching can be turned on and offfor individual tables.Multi-level RFile IndexRFiles store an index of the last key in each block. For large files, the indexcan become quite large. When the index is large, a lot of memory is consumed andfiles take a long time to open. To avoid this problem, RFiles have amulti-level index tree. Index blocks can point to other index blocks or datablocks. The entire index never has to be resident, even when the file iswritten. When an index block exceeds the configurable size threshold, it’swritten out between data blocks. The size of index blocks is configurable on aper-table basis.Binary search in RFile blocksRFile uses its index to locate a block of key values.  Once it reaches a block,it performs a linear scan to find a key of interest.  Accumulo will generateindexes of cached blocks in an adaptive manner.  Accumulo indexes the mostfrequently read blocks.  When a block is read a few times, a small indexis generated.  As a block is read more, larger indexes are generated, makingfuture seeks faster. This strategy allows Accumulo to dynamically respond toread patterns without precomputing block indexes when RFiles are written.TestingMini Accumulo ClusterMini Accumulo cluster is a set of utility code that makes it easy to spin upa local Accumulo instance running against the local filesystem.  Mini Accumulois slower than Mock Accumulo, but its behavior mirrors a real Accumuloinstance more closely.Accumulo Maven PluginUsing the Mini Accumulo Cluster in unit and integration tests is a great way fordevelopers to test their applications against Accumulo in an environment that ismuch closer to physical deployments than a Mock Accumulo environment.Accumulo 1.6.0 also introduced a maven-accumulo-plugin whichcan be used to start a Mini Accumulo Cluster instance as a part of the Mavenlifecycle that your application tests can use.Functional TestSmall, system-level tests of basic Accumulo features run in a test harness,external to the build and unit-tests.  These tests start a complete Accumuloinstance, and require Hadoop and Zookeeper to be running.  They attempt tosimulate the basic functions of Accumulo, as well as common failure conditions,such as lost disks, killed processes, and read-only file systems.Scale TestA test suite that verifies data is not lost at scale. This test runs manyingest clients that continually create linked lists containing 25 millionnodes. At some point the clients are stopped and a map reduce job is run toensure no linked list has a hole. A hole indicates data was lost by Accumulo.The Agitator can be run in conjunction with this test to randomly kill tabletservers. This test suite has uncovered many obscure data loss bugs.  This testalso helps find bugs that impact uptime and stability when run for days orweeks.Random Walk TestA test suite that looks for unexpected system states that may emerge inplausible real-world applications.  Application components are defined as testnodes (such as create table, insert data, scan data, delete table, etc.), andare programmed as Java classes that implement a specified interface.  The nodesare connected together in a graph specified in an XML document. Many processesindependently and concurrently execute a random walk of the test graphs. Someof the test graphs have a concept of correctness and can verify data over time.Other tests have no concept of data correctness and have the simple goal ofcrashing Accumulo. Many obscure bugs have been uncovered by this testingframework and subsequently corrected.Client APIBatch ScannerThe BatchScanner takes a list of Ranges, batches them to the appropriate tablet servers, andreturns data as it is received (i.e. not in sorted order).Batch WriterThe BatchWriter client buffers writes in memory before sending them in batches to theappropriate tablet servers.Bulk ImportInstead of writing individual mutations to Accumulo, entire files of sorted keyvalue pairs can be imported using BulkImport. These files are moved into the Accumulo directoryand referenced by Accumulo. This feature is useful for ingesting a large amountof data. This method of ingest usually offers higher throughput at the cost ofhigher latency for data availability for scans.  Usually, the data is sortedusing map reduce and then bulk imported. This method of ingest also allows forflexibility in resource allocation.  The nodes running map reduce to sort datacould be different from the Accumulo nodes.MapReduceAccumulo can be a source and/or sink for MapReduce jobs.ProxyAccumulo has a proxy which enables interactionto with Accumulo using other languages like Python, Ruby, C++, etc.Conditional MutationsConditional Mutations (since 1.6.0) allow users to perform efficient, atomicread-modify-write operations on rows. Conditions can be defined using equality checks of the valuesin a column or the absence of a column. For more information on using this feature, users can referencethe Javadoc for ConditionalMutation and ConditionalWriter.LexicodersLexicoders (since 1.6.0) help encode data (i.e numbers, dates)into Accumulo keys in a way that their natural sort order is preserved.PluginsThe Service Plugin Interface (SPI) was created to expose Accumulo system level information toplugins in a stable manner.BalancerUsers can provide a balancer plugin that decides how to distribute tabletsacross a table.  These plugins can be provided on a per-table basis.  This isuseful for ensuring a particular table’s tablets are placed optimally fortables with special query needs.  The default balancer randomly spreads eachtable’s tablets across the cluster.  It takes into account where a tablet waspreviously hosted to leverage locality.  When a tablet splits, the defaultbalancer moves one child to another tablet server.  The assumption here is thatsplitting tablets are being actively written to, so this keeps write load evenlyspread.CacheSee the page on CachingCompactionCompactions were reworked in 2.1 to allow plugin capabilities. See the documentation forcompactions.ScanScan Executors were added to the SPI in 2.0. See the Scan Executors page.Volume ChooserThe Volume Chooser has been around for some time but was refactored in 2.1 to be included in the SPI.See the javadoc for more information.Pluggable Block CachesAccumulo provides two BlockCacheManager implementations (LruBlockCacheManager andTinyLfuBlockCacheManager) that construct on-heap block caches. Users can providealternate BlockCacheManager implementations using the property tserver.cache.manager.class.General AdministrationMonitor pageThe Accumulo Monitor provides basic information about the system health andperformance.  It displays table sizes, ingest and query statistics, serverload, and last-update information.  It also allows the user to view recentdiagnostic logs and traces.TracingIt can be difficult to determine why some operations are taking longer thanexpected. For example, you may be looking up items with very low latency, butsometimes the lookups take much longer. Determining the cause of the delay isdifficult because the system is distributed, and the typical lookup is fast.Accumulo has been instrumented to record the time that various operations takewhen tracing is turned on. The fact that tracing is enabled follows all therequests made on behalf of the user throughout the distributed infrastructureof Accumulo, and across all threads of execution.Online reconfigurationSystem and per table configuration is stored in Zookeeper. Many, but not all,configuration changes take effect while Accumulo is running. Some do not takeeffect until server processes are restarted.Table renamingTables can be renamed easily because Accumulo uses internal table IDs andstores mappings between names and IDs in Zookeeper.Internal Data ManagementLocality groupsGroups columns within a single file. There is a default locality group so thatnot all columns need be specified. The locality groups can be restructuredwhile the table is online and the changes will take effect on the nextcompaction.  A tablet can have files with different locality groupconfigurations.  In this case, scans may be suboptimal, but correct untilcompactions rewrite all files.  After reconfiguring locality groups, a user canforce a table to compact in order to write all data into the new localitygroups.  Alternatively, the change could be allowed to happen over time aswrites to the table cause compactions to happen.Smart compaction algorithmIt is inefficient to merge small files with large files.  Accumulo merges filesonly if all files are larger than a configurable ratio (default is 3)multiplied by the largest file size.  If this cannot be done with all thefiles, the largest file is removed from consideration, and the remaining filesare considered for compaction.  This is done until there are no files to merge.EncryptionAccumulo can encrypt its data on disk anddata sent over the wire.On-demand Data ManagementCompactionsAbility to force tablets to compact to one file. Even tablets with one file arecompacted.  This is useful for improving query performance, permanentlyapplying iterators, or using a new locality group configuration.  One exampleof using iterators is applying a filtering iterator to remove data from atable. Additionally, users can initiate a compaction with iterators only applied tothat compaction event.Split pointsArbitrary split points can be added to an online table at any point in time.This is useful for increasing ingest performance on a new table. It can also beused to accommodate new data patterns in an existing table.Tablet MergingTablet merging is a new feature. Merging of tablets can be requested in theshell; Accumulo does not merge tablets automatically. If desired, the METADATAtablets can be merged.Table CloningAllows users to quickly create a new table that references an existing table’sdata and copies its configuration. A cloned table and its source table can bemutated independently. Testing was the motivating reason behind this newfeature. For example, to test a new filtering iterator, clone the table, add thefilter to the clone, and force a major compaction.Import/Export TableAn offline tables metadata and files can easily be copied to another cluster andimported.Compact RangeCompact each tablet that falls within a row range down to a single file.Delete RangeAdded an operation to efficiently delete a range of rows from a table. Tabletsthat fall completely within a range are simply dropped. Tablets overlapping thebeginning and end of the range are split, compacted, and then merged.",
      "url": " /docs/2.x/getting-started/features",
      "categories": "getting-started"
    },
  
    "docs-2-x-getting-started-glossary": {
      "title": "Glossary",
      "content": "  authorizations            a set of strings associated with a user or with a particular scan that willbe used to determine which key/value pairs are visible to the user.        cell            a set of key/value pairs whose keys differ only in timestamp.        column            the portion of the key that sorts after the row and is divided into family,qualifier, and visibility.        column family            the portion of the key that sorts second and controls locality groups, therow/column hybrid nature of accumulo.        column qualifier            the portion of the key that sorts third and provides additional keyuniqueness.        column visibility            the portion of the key that sorts fourth and controls user access toindividual key/value pairs. Visibilities are boolean AND (&amp;amp;) and OR (|)combinations of authorization strings with parentheses required to determineordering, e.g. (AB&amp;amp;C)|DEF.        iterator            a mechanism for modifying tablet-local portions of the key/value space.Iterators are used for standard administrative tasks as well as for customprocessing.        iterator priority            an iterator must be configured with a particular scope and priority. When atablet server enters that scope, it will instantiate iterators in priorityorder starting from the smallest priority and ending with the largest, andapply each to the data read before rewriting the data or sending the data tothe user.        iterator scopes            the possible scopes for iterators are where the tablet server is alreadyreading and/or writing data: minor compaction / flush time (mincscope), major compaction / file merging time (majc scope), and querytime (scan scope).        gc            process that identifies temporary files in HDFS that are no longer needed byany process, and deletes them.        key            the key into the distributed sorted map which is accumulo. The key issubdivided into row, column, and timestamp. The column is further divided intofamily, qualifier, and visibility.        locality group            a set of column families that will be grouped together on disk. With nolocality groups configured, data is stored on disk in row order. If eachcolumn family were configured to be its own locality group, the data for eachcolumn would be stored separately, in row order. Configuring sets of columnsinto locality groups is a compromise between the two approaches and willimprove performance when multiple columns are accessed in the same scan.        log-structured merge-tree            the sorting / flushing / merging scheme on which BigTable’s design is based.        logger            in 1.4 and older, process that accepts updates to tablet servers and writesthem to local on-disk storage for redundancy. in 1.5 the functionality wassubsumed by the tablet server and datanode with HDFS writes.        major compaction            merging multiple files into a single file. If all of a tablet’s files aremerged into a single file, it is called a full major compaction.        master            the old name for the manager process. This process was renamed as of the2.1.0 release of Accumulo.        manager            process that detects and responds to tablet failures, balances load acrosstablet servers by assigning and migrating tablets when required, coordinatestable operations, and handles tablet server logistics (startup, shutdown,recovery).        minor compaction            flushing data from memory to disk. Usually this creates a new file for atablet, but if the memory flushed is merge-sorted in with data from an existingfile (replacing that file), it is called a merging minor compaction.        monitor            process that displays status and usage information for all Accumulocomponents.        permissions            administrative abilities that must be given to a user such as creating tablesor users and changing permissions or configuration parameters.        row            the portion of the key that controls atomicity. Keys with the same row areguaranteed to remain on a single tablet hosted by a single tablet server,therefore multiple key/value pairs can be added to or removed from a row at thesame time. The row is used for the primary sorting of the key.        scan            reading a range of key/value pairs.        tablet            a contiguous key range; the unit of work for a tablet server.        tablet servers            a set of servers that hosts reads and writes for tablets. Each server hostsa distinct set of tablets at any given time, but the tablets may be hosted bydifferent servers over time.        timestamp            the portion of the key that controls versioning. Otherwise, identical keyswith differing timestamps are considered to be versions of a singlecell. Accumulo can be configured to keep the N newestversions of each cell. When a deletion entry is inserted, it deletesall earlier versions for its cell.        value            immutable bytes associated with a particular key.      ",
      "url": " /docs/2.x/getting-started/glossary",
      "categories": "getting-started"
    },
  
    "docs-2-x-getting-started-quickstart": {
      "title": "Setup",
      "content": "User Manual (2.x and 3.x)Starting with Accumulo 2.0, the user manual now lives on the website as a seriesof web pages. Previously, it was one large pdf document that was only generatedduring a release. The user manual can now be updated very quickly and indexedfor searching across many webpages.The manual can now be searched using the Search link at the top of thewebsite or navigated by clicking the links to the left. If you are new toAccumulo, follow the instructions below to get started. For detailedinstructions, see the in-depth installation guide.Master/Manager namingAs of release 2.1, all references to “master” have been changed to “manager.” If you are using/installinga release prior to 2.1, substitute “master” in place of “manager” for any property name, file name, orprocess name referenced in this documentation.Setup for testing or developmentIf you are setting up Accumulo for testing or development, consider usingthe following tools:  Uno sets up Accumulo on a single machine for development  Muchos sets up Accumulo on a cluster (optionally launched in Amazon EC2 andMicrosoft Azure VM)If you are setting up Accumulo for a production environment, follow theinstructions below.Setup for ProductionEither download or build a binary distribution of Accumulo from source codeand unpack as follows.tar xzf /path/to/accumulo-2.1.3-bin.tar.gzcd accumulo-2.1.3There are four scripts in the bin directory of the tarball distribution thatare used to manage Accumulo:  accumulo - Runs Accumulo command-line tools and starts Accumulo processes  accumulo-service - Runs individual Accumulo processes as backgroundservices  accumulo-cluster - Manages Accumulo cluster on a single node or severalnodes  accumulo-util - Accumulo utilities for building native libraries, runningjars, etc.These scripts will be used in the remaining instructions to configure and runAccumulo. For convenience, consider adding accumulo-2.1.3/bin/to your shell’s path.Configuring AccumuloAccumulo requires running Zookeeper and HDFS instances which should be setup before configuring Accumulo.Important note: If using Erasure Coding (EC), data loss will occur unlessit is configured properly for Accumulo. Please see the Erasure Coding guidefor more information.The primary configuration files for Accumulo are accumulo.properties,accumulo-env.sh, and accumulo-client.properties which are located in theconf/ directory.The accumulo.properties file configures Accumulo server processes (i.e. tabletserver, manager, monitor, etc). Follow these steps to set it up:      Run accumulo-util build-native to build native code. If this command fails,disable native maps by setting tserver.memory.maps.native.enabledto false.        Set instance.volumes to HDFS location where Accumulo will storedata. If your namenode is running at 192.168.1.9:8020, and you want to storedata in /accumulo in HDFS, then set instance.volumes tohdfs://192.168.1.9:8020/accumulo.        Set instance.zookeeper.host to the location of your Zookeepers        (Optional) Change instance.secret (which is used by Accumuloprocesses to communicate) from the default. This value should match on allservers.  The accumulo-env.sh file sets up environment variables needed by Accumulo:      Set HADOOP_HOME and ZOOKEEPER_HOME to the location of your Hadoop andZookeeper installations. Accumulo will use these locations to find Hadoop andZookeeper jars and add them to your CLASSPATH variable. If you are runninga vendor-specific release of Hadoop or Zookeeper, you may need to modify howthe CLASSPATH variable is built in accumulo-env.sh. If Accumulo hasproblems loading classes when you start it, run accumulo classpath to printAccumulo’s classpath.        Accumulo tablet servers are configured by default to use 1GB of memory (768MBis allocated to JVM and 256MB is allocated for native maps). Native maps areallocated memory equal to 33% of the tserver JVM heap. The table below can beused if you would like to change tserver memory usage in the JAVA_OPTSsection of accumulo-env.sh:                            Native?          512MB          1GB          2GB          3GB                                      Yes          -Xmx384m -Xms384m          -Xmx768m -Xms768m          -Xmx1536m -Xms1536m          -Xmx2g -Xms2g                          No          -Xmx512m -Xms512m          -Xmx1g -Xms1g          -Xmx2g -Xms2g          -Xmx3g -Xms3g                          (Optional) Review the memory settings for the Accumulo manager, garbage collector, and monitorin the JAVA_OPTS section of accumulo-env.sh.  The accumulo-client.properties file is used by the Accumulo shell and can bepassed to Accumulo clients to simplify connecting to Accumulo. Below are stepsto configure it.      Set instance.name and instance.zookeepers tothe Accumulo instance and zookeeper connection string of your instance.        Pick an authentication type and set auth.type accordingly. Themost common auth.type is password which requires auth.principalto be set and auth.token to be set the password ofauth.principal. For the Accumulo shell, auth.token can be commented outand the shell will prompt you for the password of auth.principal at login.  InitializationAccumulo needs to initialize the locations where it stores data in Zookeeper and HDFS.Note: Initialization only needs to be performed once for an instance - if you are performing anupgrade you should not run the initialization command a second time unless you really want a newinstance.The following command will perform the initialization.accumulo initThe initialization command will prompt for the following information.  Instance name : This is the name of the Accumulo instance and itsAccumulo clients need to know it in order to connect.  Root password : Initialization sets up an initial Accumulo root user andprompts for its password. This information will be needed to later connect toAccumulo.Run AccumuloThere are several methods for running Accumulo:      Run Accumulo processes using accumulo command which runs processes inforeground and will not redirect stderr/stdout. Useful for creating init.dscripts that run Accumulo.        Run individual Accumulo processes as services using accumulo-service whichuses accumulo command but backgrounds processes, redirects stderr/stdoutand manages pid files. This is useful if you are using a cluster managementtool (i.e. Ansible, Salt, etc).        Run an Accumulo cluster on one or more nodes using accumulo-cluster (whichuses accumulo-service to run services). Useful for local development andtesting or if you are not using your own cluster management tool inproduction.  Each method above has instructions below.Run individual Accumulo processesStart Accumulo processes (tserver, manager, monitor, etc) using the accumulocommand followed by the service name. For example, to start only the tserver,run:accumulo tserverThe process will run in the foreground. Use ctrl-c to quit.For a fully operational instance, each individual service will need to bestarted.Run individual Accumulo servicesStart individual Accumulo processes (tserver, master, monitor, etc.) as abackground service using the example accumulo-service script followed by theservice name. For example, to start only the tserver, run:accumulo-service tserver startFor a fully operational instance, each individual service will need to bestarted.Run an Accumulo clusterBefore using the accumulo-cluster script to start the cluster, additionalconfiguration files may need to be created. Use the command below to create themfrom provided templates:accumulo-cluster create-configThis creates a yaml configuration file in the conf/ directory namedcluster.yaml that contains the node names where Accumulo services arerun on your cluster. By default, all services are configured to localhost. If youare running a single-node Accumulo cluster, these files do not need to bechanged and the next section should be skipped. The external compaction servicesexist in the file but are commented out as they are optional.Multi-node configurationIf you are running an Accumulo cluster on multiple nodes, the conf/cluster.yamlfile contains sections that should be configured with a list of node names in yaml format:  manager : Accumulo primary coordinating process. Must specify one node. Canspecify a few for fault tolerance.  gc      : Accumulo garbage collector. Must specify one node. Can specify afew for fault tolerance.  monitor : Node where Accumulo monitoring web server is run.  tserver : Accumulo worker processes. List all of the nodes where tabletservers should run.  sserver : Optional. List of all nodes where scan servers should run.  compaction.coordinator : Optional. Must specify one node. Can specify a fewfor fault tolerance.  compaction.compactor : Optional. Accumulo external compactor processes. List ofall nodes where compactors should run.The Accumulo, Hadoop, and Zookeeper software should be present at the samelocation on every node. Also, the files in the conf directory must be copied toevery node. There are many ways to replicate the software and configuration, twopossible tools that can help replicate software and/or config are pdcp andprsync.The accumulo-cluster script uses ssh to start processes on remote nodes.Before attempting to start Accumulo, passwordless ssh must be setup onthe cluster.Start clusterAfter configuring and initializing Accumulo, use the following command to startthe cluster using the provided cluster management script:accumulo-cluster startFirst stepsOnce you have started Accumulo, use the following command to run the Accumuloshell:accumulo shell -u rootUse your web browser to connect the Accumulo monitor page on port 9995.http://&amp;lt;hostname in conf/monitor&amp;gt;:9995/Stopping AccumuloWhen finished, use the following commands to stop Accumulo:  Stop an individual Accumulo service: accumulo-service tserver stop  Stop Accumulo cluster: accumulo-cluster stop",
      "url": " /docs/2.x/getting-started/quickstart",
      "categories": "getting-started"
    },
  
    "docs-2-x-getting-started-shell": {
      "title": "Accumulo Shell",
      "content": "Accumulo provides a simple shell that can be used to examine the contents andconfiguration settings of tables, insert/update/delete values, and changeconfiguration settings.The shell can be started by the following command:accumulo shell -u [username]The shell will prompt for the corresponding password to the username specifiedand then display the following prompt:Shell - Apache Accumulo Interactive Shell-- version: 2.1.3- instance name: myinstance- instance id: 00000000-0000-0000-0000-000000000000-- type &#39;help&#39; for a list of available commands-root@myinstance&amp;gt;Basic AdministrationThe tables command will list all existing tables.root@myinstance&amp;gt; tablesaccumulo.metadataaccumulo.rootThe createtable command creates a new table.root@myinstance&amp;gt; createtable mytableroot@myinstance mytable&amp;gt; tablesaccumulo.metadataaccumulo.rootmytableThe deletetable command deletes a table.root@myinstance testtable&amp;gt; deletetable testtabledeletetable { testtable } (yes|no)? yesTable: [testtable] has been deleted.The shell can be used to insert updates and scan tables. This is useful for inspecting tables.root@myinstance mytable&amp;gt; scanroot@myinstance mytable&amp;gt; insert row1 colf colq value1insert successfulroot@myinstance mytable&amp;gt; scanrow1 colf:colq [] value1The value in brackets [] would be the visibility labels. Since none were used, this is empty for this row.You can use the -st option to scan to see the timestamp for the cell, too.Table MaintenanceThe compact command instructs Accumulo to schedule a compaction of the table during whichfiles are consolidated and deleted entries are removed.root@myinstance mytable&amp;gt; compact -t mytable07 16:13:53,201 [shell.Shell] INFO : Compaction of table mytable started for given rangeIf needed, the compaction can be canceled using compact --cancel -t mytable.The flush command instructs Accumulo to write all entries currently in memory for a given tableto disk.root@myinstance mytable&amp;gt; flush -t mytable07 16:14:19,351 [shell.Shell] INFO : Flush of table mytable initiated...User AdministrationThe Shell can be used to add, remove, and grant privileges to users.root@myinstance mytable&amp;gt; createuser bobEnter new password for &#39;bob&#39;: *********Please confirm new password for &#39;bob&#39;: *********root@myinstance mytable&amp;gt; authenticate bobEnter current password for &#39;bob&#39;: *********Validroot@myinstance mytable&amp;gt; grant System.CREATE_TABLE -s -u bobroot@myinstance mytable&amp;gt; user bobEnter current password for &#39;bob&#39;: *********bob@myinstance mytable&amp;gt; userpermissionsSystem permissions: System.CREATE_TABLETable permissions (accumulo.metadata): Table.READTable permissions (mytable): NONEbob@myinstance mytable&amp;gt; createtable bobstablebob@myinstance bobstable&amp;gt;bob@myinstance bobstable&amp;gt; user rootEnter current password for &#39;root&#39;: *********root@myinstance bobstable&amp;gt; revoke System.CREATE_TABLE -s -u bob",
      "url": " /docs/2.x/getting-started/shell",
      "categories": "getting-started"
    },
  
    "docs-2-x-getting-started-table-configuration": {
      "title": "Table Configuration",
      "content": "Accumulo tables have a few options that can be configured to alter the defaultbehavior of Accumulo as well as improve performance based on the data stored.These include locality groups, constraints, bloom filters, iterators, and blockcache.  See the server properties documentation for a complete list of availableconfiguration options.Locality GroupsAccumulo supports storing sets of column families separately on disk to allowclients to efficiently scan over columns that are frequently used together and to avoidscanning over column families that are not requested. After a locality group is set,Scanner and BatchScanner operations will automatically take advantage of themwhenever the fetchColumnFamilies() method is used.By default, tables place all column families into the same default locality group.Additional locality groups can be configured at any time via the shell orprogrammatically as follows:Managing Locality Groups via the Shellusage: setgroups &amp;lt;group&amp;gt;=&amp;lt;col fam&amp;gt;{,&amp;lt;col fam&amp;gt;}{ &amp;lt;group&amp;gt;=&amp;lt;col fam&amp;gt;{,&amp;lt;col fam&amp;gt;}}    [-?] -t &amp;lt;table&amp;gt;user@myinstance mytable&amp;gt; setgroups group_one=colf1,colf2 -t mytableuser@myinstance mytable&amp;gt; getgroups -t mytableManaging Locality Groups via the Client APIAccumuloClient client = Accumulo.newClient()                          .from(&quot;/path/to/accumulo-client.properties&quot;).build();HashMap&amp;lt;String,Set&amp;lt;Text&amp;gt;&amp;gt; localityGroups = new HashMap&amp;lt;String, Set&amp;lt;Text&amp;gt;&amp;gt;();HashSet&amp;lt;Text&amp;gt; metadataColumns = new HashSet&amp;lt;Text&amp;gt;();metadataColumns.add(new Text(&quot;domain&quot;));metadataColumns.add(new Text(&quot;link&quot;));HashSet&amp;lt;Text&amp;gt; contentColumns = new HashSet&amp;lt;Text&amp;gt;();contentColumns.add(new Text(&quot;body&quot;));contentColumns.add(new Text(&quot;images&quot;));localityGroups.put(&quot;metadata&quot;, metadataColumns);localityGroups.put(&quot;content&quot;, contentColumns);client.tableOperations().setLocalityGroups(&quot;mytable&quot;, localityGroups);// existing locality groups can be obtained as followsMap&amp;lt;String, Set&amp;lt;Text&amp;gt;&amp;gt; groups = client.tableOperations().getLocalityGroups(&quot;mytable&quot;);The assignment of Column Families to Locality Groups can be changed at any time. Thephysical movement of column families into their new locality groups takes place viathe periodic major compaction process that takes place continuously in thebackground or manually using the compact command in the shell.ConstraintsAccumulo supports constraints applied on mutations at insert time. This can beused to disallow certain inserts according to a user defined policy. Any mutationthat fails to meet the requirements of the constraint is rejected and sent back to theclient.Constraints can be enabled by setting a table property as follows:user@myinstance mytable&amp;gt; constraint -t mytable -a com.test.ExampleConstraint com.test.AnotherConstraintuser@myinstance mytable&amp;gt; constraint -lcom.test.ExampleConstraint=1com.test.AnotherConstraint=2Currently, there are no general-purpose constraints provided with the Accumulodistribution. New constraints can be created by writing a Java class that implementsthe Constraint interface.To deploy a new constraint, create a jar file containing a class implementing Constraintand place it in the lib/ directory of the Accumulo installation. Newconstraint jars can be added to Accumulo and enabled without restarting but anychange to an existing constraint class requires Accumulo to be restarted.See the constraints examples for example code.Bloom FiltersAs mutations are applied to an Accumulo table, several files are created per tablet. Ifbloom filters are enabled, Accumulo will create and load a small data structure intomemory to determine whether a file contains a given key before opening the file.This can speed up lookups considerably. Bloom filters can be enabled on a table bysetting the table.bloom.enabled property to true in the shell:user@myinstance&amp;gt; config -t mytable -s table.bloom.enabled=trueThe bloom filter examples contains an extensive example of using Bloom Filters.IteratorsIterators provide a modular mechanism for adding functionality to be executed byTabletServers when scanning or compacting data. This allows users to efficientlysummarize, filter, and aggregate data. In fact, the built-in features of cell-levelsecurity and column fetching are implemented using Iterators.Some useful Iterators are provided with Accumulo and can be found in theorg.apache.accumulo.core.iterators.user package.In each case, any custom Iterators must be included in Accumulo’s classpath,typically by including a jar in lib/ or lib/ext/, although the VFS classloaderallows for classpath manipulation using a variety of schemes including URLs and HDFS URIs.Setting Iterators via the ShellIterators can be configured on a table at scan, minor compaction and/or majorcompaction scopes. If the Iterator implements the OptionDescriber interface, thesetiter command can be used which will interactively prompt the user to providevalues for the given necessary options.usage: setiter [-?] -ageoff | -agg | -class &amp;lt;name&amp;gt; | -regex |    -reqvis | -vers   [-majc] [-minc] [-n &amp;lt;itername&amp;gt;] -p &amp;lt;pri&amp;gt;    [-scan] [-t &amp;lt;table&amp;gt;]user@myinstance mytable&amp;gt; setiter -t mytable -scan -p 15 -n myiter -class com.company.MyIteratorThe config command can always be used to manually configure iterators which is usefulin cases where the Iterator does not implement the OptionDescriber interface.config -t mytable -s table.iterator.scan.myiter=15,com.company.MyIteratorconfig -t mytable -s table.iterator.minc.myiter=15,com.company.MyIteratorconfig -t mytable -s table.iterator.majc.myiter=15,com.company.MyIteratorconfig -t mytable -s table.iterator.scan.myiter.opt.myoptionname=myoptionvalueconfig -t mytable -s table.iterator.minc.myiter.opt.myoptionname=myoptionvalueconfig -t mytable -s table.iterator.majc.myiter.opt.myoptionname=myoptionvalueTypically, a table will have multiple iterators. Accumulo configures a set ofsystem level iterators for each table. These iterators provide corefunctionality like visibility label filtering and may not be removed byusers. User level iterators are applied in the order of their priority.Priority is a user configured integer; iterators with lower numbers go first,passing the results of their iteration on to the other iterators up thestack.Setting Iterators Programmaticallyscanner.addIterator(new IteratorSetting(    15, // priority    &quot;myiter&quot;, // name this iterator    &quot;com.company.MyIterator&quot; // class name));Some iterators take additional parameters from client code, as in the followingexample:IteratorSetting iter = new IteratorSetting(...);iter.addOption(&quot;myoptionname&quot;, &quot;myoptionvalue&quot;);scanner.addIterator(iter)Tables support separate Iterator settings to be applied at scan time, upon minorcompaction and upon major compaction. For most uses, tables will have identicaliterator settings for all three to avoid inconsistent results.Versioning Iterators and TimestampsAccumulo provides the capability to manage versioned data through the use oftimestamps within the Key. If a timestamp is not specified in the key created by theclient then the system will set the timestamp to the current time. Two keys withidentical rowIDs and columns but different timestamps are considered two versionsof the same key. If two inserts are made into Accumulo with the same rowID,column, and timestamp, then the behavior is non-deterministic.Timestamps are sorted in descending order, so the most recent data comes first.Accumulo can be configured to return the top k versions, or versions later than agiven date. The default is to return the one most recent version.The version policy can be changed by changing the VersioningIterator options for atable as follows:user@myinstance mytable&amp;gt; config -t mytable -s table.iterator.scan.vers.opt.maxVersions=3user@myinstance mytable&amp;gt; config -t mytable -s table.iterator.minc.vers.opt.maxVersions=3user@myinstance mytable&amp;gt; config -t mytable -s table.iterator.majc.vers.opt.maxVersions=3When a table is created, by default it’s configured to use theVersioningIterator and keep one version. A table can be created without theVersioningIterator with the -ndi option in the shell. Also, the Java APIhas the following methodclient.tableOperations.create(String tableName, boolean limitVersion);Logical TimeAccumulo 1.2 introduces the concept of logical time. This ensures that timestampsset by Accumulo always move forward. This helps avoid problems caused byTabletServers that have different time settings. The per tablet counter gives uniqueone up time stamps on a per-mutation basis. When using time in milliseconds, iftwo things arrive within the same millisecond then both receive the sametimestamp. When using time in milliseconds, Accumulo set times will stillalways move forward and never backwards.A table can be configured to use logical timestamps at creation time as follows:user@myinstance&amp;gt; createtable -tl logicalDeletesDeletes are special keys in Accumulo that get sorted along will all the other data.When a delete key is inserted, Accumulo will not show anything that has atimestamp less than or equal to the delete key. During major compaction, any keysolder than a delete key are omitted from the new file created, and the omitted keysare removed from disk as part of the regular garbage collection process.FiltersWhen scanning over a set of key-value pairs it is possible to apply an arbitraryfiltering policy through the use of a Filter. Filters are types of iterators that returnonly key-value pairs that satisfy the filter logic. Accumulo has a few built-in filtersthat can be configured on any table: AgeOff, ColumnAgeOff, Timestamp, NoVis, and RegEx. More can be addedby writing a Java class that extends the Filter class.The AgeOff filter can be configured to remove data older than a certain date or a fixedamount of time from the present. The following example sets a table to deleteeverything inserted over 30 seconds ago:user@myinstance&amp;gt; createtable filtertestuser@myinstance filtertest&amp;gt; setiter -t filtertest -scan -minc -majc -p 10 -n myfilter -ageoffAgeOffFilter removes entries with timestamps more than &amp;lt;ttl&amp;gt; milliseconds old----------&amp;gt; set org.apache.accumulo.core.iterators.user.AgeOffFilter parameter negate, default false                keeps k/v that pass accept method, true rejects k/v that pass accept method:----------&amp;gt; set org.apache.accumulo.core.iterators.user.AgeOffFilter parameter ttl, time to                live (milliseconds): 30000----------&amp;gt; set org.apache.accumulo.core.iterators.user.AgeOffFilter parameter currentTime, if set,                use the given value as the absolute time in milliseconds as the current time of day:user@myinstance filtertest&amp;gt;user@myinstance filtertest&amp;gt; scanuser@myinstance filtertest&amp;gt; insert foo a b cuser@myinstance filtertest&amp;gt; scanfoo a:b [] cuser@myinstance filtertest&amp;gt; sleep 4user@myinstance filtertest&amp;gt; scanuser@myinstance filtertest&amp;gt;To see the iterator settings for a table, use:user@example filtertest&amp;gt; config -t filtertest -f iterator---------+---------------------------------------------+------------------SCOPE    | NAME                                        | VALUE---------+---------------------------------------------+------------------table    | table.iterator.majc.myfilter .............. | 10,org.apache.accumulo.core.iterators.user.AgeOffFiltertable    | table.iterator.majc.myfilter.opt.ttl ...... | 30000table    | table.iterator.majc.vers .................. | 20,org.apache.accumulo.core.iterators.VersioningIteratortable    | table.iterator.majc.vers.opt.maxVersions .. | 1table    | table.iterator.minc.myfilter .............. | 10,org.apache.accumulo.core.iterators.user.AgeOffFiltertable    | table.iterator.minc.myfilter.opt.ttl ...... | 30000table    | table.iterator.minc.vers .................. | 20,org.apache.accumulo.core.iterators.VersioningIteratortable    | table.iterator.minc.vers.opt.maxVersions .. | 1table    | table.iterator.scan.myfilter .............. | 10,org.apache.accumulo.core.iterators.user.AgeOffFiltertable    | table.iterator.scan.myfilter.opt.ttl ...... | 30000table    | table.iterator.scan.vers .................. | 20,org.apache.accumulo.core.iterators.VersioningIteratortable    | table.iterator.scan.vers.opt.maxVersions .. | 1---------+---------------------------------------------+------------------CombinersAccumulo supports on the fly lazy aggregation of data using Combiners. Aggregation isdone at compaction and scan time. No lookup is done at insert time, which` greatlyspeeds up ingest.Accumulo allows Combiners to be configured on tables and columnfamilies. When a Combiner is set it is applied across the valuesassociated with any keys that share rowID, column family, and column qualifier.This is similar to the reduce step in MapReduce, which applied some function to allthe values associated with a particular key.For example, if a summing combiner were configured on a table and the followingmutations were inserted:Row     Family Qualifier Timestamp  ValuerowID1  colfA  colqA     20100101   1rowID1  colfA  colqA     20100102   1The table would reflect only one aggregate value:rowID1  colfA  colqA     -          2Combiners can be enabled for a table using the setiter command in the shell. Below is an example.root@a14 perDayCounts&amp;gt; setiter -t perDayCounts -p 10 -scan -minc -majc -n daycount                       -class org.apache.accumulo.core.iterators.user.SummingCombinerTypedValueCombiner can interpret Values as a variety of number encodings  (VLong, Long, or String) before combining----------&amp;gt; set SummingCombiner parameter columns,            &amp;lt;col fam&amp;gt;[:&amp;lt;col qual&amp;gt;]{,&amp;lt;col fam&amp;gt;[:&amp;lt;col qual&amp;gt;]} : day----------&amp;gt; set SummingCombiner parameter type, &amp;lt;VARNUM|LONG|STRING&amp;gt;: STRINGroot@a14 perDayCounts&amp;gt; insert foo day 20080101 1root@a14 perDayCounts&amp;gt; insert foo day 20080101 1root@a14 perDayCounts&amp;gt; insert foo day 20080103 1root@a14 perDayCounts&amp;gt; insert bar day 20080101 1root@a14 perDayCounts&amp;gt; insert bar day 20080101 1root@a14 perDayCounts&amp;gt; scanbar day:20080101 []    2foo day:20080101 []    2foo day:20080103 []    1Accumulo includes some useful Combiners out of the box. To find these look inthe org.apache.accumulo.core.iterators.user package.Additional Combiners can be added by creating a Java class that extendsCombiner and adding a jar containing that class to Accumulo’s lib/ext directory.See the combiner example for example code.Block CacheA Block Cache can be enabled on tables to limit reads from disk which can resultin reduced read latency. Read the Caching documentation to learn more.CompactionSee compaction Pre-splitting tablesAccumulo will balance and distribute tables across servers. Before atable gets large, it will be maintained as a single tablet on a singleserver. This limits the speed at which data can be added or queriedto the speed of a single node. To improve performance when a tableis new, or small, you can add split points and generate new tablets.In the shell:root@myinstance&amp;gt; createtable newTableroot@myinstance&amp;gt; addsplits -t newTable g n tThis will create a new table with 4 tablets. The table will be spliton the letters g, n, and t which will work nicely if therow data start with lower-case alphabetic characters. If your rowdata includes binary information or numeric information, or if thedistribution of the row information is not flat, then you would pickdifferent split points. Now ingest and query can proceed on 4 nodeswhich can improve performance.Merging tabletsOver time, a table can get very large, so large that it has hundredsof thousands of split points. Once there are enough tablets to spreada table across the entire cluster, additional splits may not improveperformance, and may create unnecessary bookkeeping. The distributionof data may change over time. For example, if row data contains dateinformation, and data is continually added and removed to maintain awindow of current information, tablets for older rows may be empty.Accumulo supports tablet merging, which can be used to reducethe number of split points. The following command will merge all rowsfrom A to Z into a single tablet:root@myinstance&amp;gt; merge -t myTable -s A -e ZIf the result of a merge produces a tablet that is larger than theconfigured split size, the tablet may be split by the tablet server.Be sure to increase your tablet size prior to any merges if the goalis to have larger tablets:root@myinstance&amp;gt; config -t myTable -s table.split.threshold=2GIn order to merge small tablets, you can ask Accumulo to mergesections of a table smaller than a given size.root@myinstance&amp;gt; merge -t myTable -s 100MBy default, small tablets will not be merged into tablets that arealready larger than the given size. This can leave isolated smalltablets. To force small tablets to be merged into larger tablets usethe --force option:root@myinstance&amp;gt; merge -t myTable -s 100M --forceMerging away small tablets works on one section at a time. If yourtable contains many sections of small split points, or you areattempting to change the split size of the entire table, it will befaster to set the split point and merge the entire table:root@myinstance&amp;gt; config -t myTable -s table.split.threshold=256Mroot@myinstance&amp;gt; merge -t myTableDelete RangeConsider an indexing scheme that uses date information in each row.For example 20110823-15:20:25.013 might be a row that specifies adate and time. In some cases, we might like to delete rows based onthis date, say to remove all the data older than the current year.Accumulo supports a delete range operation which efficientlyremoves data between two rows. For example:root@myinstance&amp;gt; deleterange -t myTable -s 2010 -e 2011This will delete all rows starting with 2010 and it will stop atany row starting 2011. You can delete any data prior to 2011with:root@myinstance&amp;gt; deleterange -t myTable -e 2011 --forceThe shell will not allow you to delete an unbounded range (no start)unless you provide the --force option.Range deletion is implemented using splits at the given start/endpositions, and will affect the number of splits in the table.Cloning TablesA new table can be created that points to an existing table’s data. This is avery quick metadata operation, no data is actually copied. The cloned tableand the source table can change independently after the clone operation. Oneuse case for this feature is testing. For example to test a new filteringiterator, clone the table, add the filter to the clone, and force a majorcompaction. To perform a test on less data, clone a table and then use deleterange to efficiently remove a lot of data from the clone. Another use case isgenerating a snapshot to guard against human error. To create a snapshot,clone a table and then disable write permissions on the clone.The clone operation will point to the source table’s files. This is why theflush option is present and is enabled by default in the shell. If the flushoption is not enabled, then any data the source table currently has in memorywill not exist in the clone.A cloned table copies the configuration of the source table. However, thepermissions of the source table are not copied to the clone. After a clone iscreated, only the user that created the clone can read and write to it.In the following example we see that data inserted after the clone operation isnot visible in the clone.root@a14&amp;gt; createtable peopleroot@a14 people&amp;gt; insert 890435 name last Doeroot@a14 people&amp;gt; insert 890435 name first Johnroot@a14 people&amp;gt; clonetable people testroot@a14 people&amp;gt; insert 890436 name first Janeroot@a14 people&amp;gt; insert 890436 name last Doeroot@a14 people&amp;gt; scan890435 name:first []    John890435 name:last []    Doe890436 name:first []    Jane890436 name:last []    Doeroot@a14 people&amp;gt; table testroot@a14 test&amp;gt; scan890435 name:first []    John890435 name:last []    Doeroot@a14 test&amp;gt;The du command in the shell shows how much space a table is using in HDFS.This command can also show how much overlapping space two cloned tables have inHDFS. In the example below du shows table ci is using 428M. Then ci is clonedto cic and du shows that both tables share 428M. After three entries areinserted into cic and its flushed, du shows the two tables still share 428M butcic has 226 bytes to itself. Finally, table cic is compacted and then du showsthat each table uses 428M.root@a14&amp;gt; du ci             428,482,573 [ci]root@a14&amp;gt; clonetable ci cicroot@a14&amp;gt; du ci cic             428,482,573 [ci, cic]root@a14&amp;gt; table cicroot@a14 cic&amp;gt; insert r1 cf1 cq1 v1root@a14 cic&amp;gt; insert r1 cf1 cq2 v2root@a14 cic&amp;gt; insert r1 cf1 cq3 v3root@a14 cic&amp;gt; flush -t cic -w27 15:00:13,908 [shell.Shell] INFO : Flush of table cic completed.root@a14 cic&amp;gt; du ci cic             428,482,573 [ci, cic]                     226 [cic]root@a14 cic&amp;gt; compact -t cic -w27 15:00:35,871 [shell.Shell] INFO : Compacting table ...27 15:03:03,303 [shell.Shell] INFO : Compaction of table cic completed for given rangeroot@a14 cic&amp;gt; du ci cic             428,482,573 [ci]             428,482,612 [cic]root@a14 cic&amp;gt;Exporting TablesAccumulo supports exporting tables for the purpose of copying tables to anothercluster. Exporting and importing tables preserves the tables’ configuration,splits, and logical time. Tables are exported and then copied via the hadoopdistcp command. To export a table, it must be offline and stay offline whiledistcp runs. Staying offline prevents files from being deleted during the process.An easy way to take a table offline without interrupting access is to clone itand take the clone offline.Table Import/Export ExampleThe following example demonstrates Accumulo’s mechanism for exporting andimporting tables.The shell session below illustrates creating a table, inserting data, andexporting the table.root@test15&amp;gt; createtable table1root@test15 table1&amp;gt; insert a cf1 cq1 v1root@test15 table1&amp;gt; insert h cf1 cq1 v2root@test15 table1&amp;gt; insert z cf1 cq1 v3root@test15 table1&amp;gt; insert z cf1 cq2 v4root@test15 table1&amp;gt; addsplits -t table1 b rroot@test15 table1&amp;gt; scana cf1:cq1 []    v1h cf1:cq1 []    v2z cf1:cq1 []    v3z cf1:cq2 []    v4root@test15&amp;gt; config -t table1 -s table.split.threshold=100Mroot@test15 table1&amp;gt; clonetable table1 table1_exproot@test15 table1&amp;gt; offline table1_exproot@test15 table1&amp;gt; exporttable -t table1_exp /tmp/table1_exportroot@test15 table1&amp;gt; quitAfter executing the export command, a few files are created in the hdfs dir.One of the files is a list of files to distcp as shown below.$ hadoop fs -ls /tmp/table1_exportFound 2 items-rw-r--r--   3 user supergroup        162 2012-07-25 09:56 /tmp/table1_export/distcp.txt-rw-r--r--   3 user supergroup        821 2012-07-25 09:56 /tmp/table1_export/exportMetadata.zip$ hadoop fs -cat /tmp/table1_export/distcp.txthdfs://n1.example.com:6093/accumulo/tables/3/default_tablet/F0000000.rfhdfs://n1.example.com:6093/tmp/table1_export/exportMetadata.zipBefore the table can be imported, it must be copied using distcp. After thedistcp completes, the cloned table may be deleted.$ hadoop distcp -f /tmp/table1_export/distcp.txt /tmp/table1_export_destThe Accumulo shell session below shows importing the table and inspecting it.The data, splits, config, and logical time information for the table werepreserved.root@test15&amp;gt; importtable table1_copy /tmp/table1_export_destroot@test15&amp;gt; table table1_copyroot@test15 table1_copy&amp;gt; scana cf1:cq1 []    v1h cf1:cq1 []    v2z cf1:cq1 []    v3z cf1:cq2 []    v4root@test15 table1_copy&amp;gt; getsplits -t table1_copybrroot@test15&amp;gt; config -t table1_copy -f split---------+--------------------------+-------------------------------------------SCOPE    | NAME                     | VALUE---------+--------------------------+-------------------------------------------default  | table.split.threshold .. | 1Gtable    |    @override ........... | 100M---------+--------------------------+-------------------------------------------root@test15&amp;gt; tables -laccumulo.metadata    =&amp;gt;        !0accumulo.root        =&amp;gt;        +rtable1_copy          =&amp;gt;         5trace                =&amp;gt;         1root@test15 table1_copy&amp;gt; scan -t accumulo.metadata -b 5 -c srv:time5;b srv:time []    M13432245004675;r srv:time []    M13432245004675&amp;lt; srv:time []    M1343224500467",
      "url": " /docs/2.x/getting-started/table_configuration",
      "categories": "getting-started"
    },
  
    "docs-2-x-getting-started-table-design": {
      "title": "Table Design",
      "content": "Basic TableSince Accumulo tables are sorted by row ID, each table can be thought of as beingindexed by the row ID. Lookups performed by row ID can be executed quickly, by doinga binary search, first across the tablets, and then within a tablet. Clients shouldchoose a row ID carefully in order to support their desired application. A simple ruleis to select a unique identifier as the row ID for each entity to be stored and assignall the other attributes to be tracked to be columns under this row ID. For example,if we have the following data in a comma-separated file:userid,age,address,account-balanceWe might choose to store this data using the userid as the rowID, the columnname in the column family, and a blank column qualifier:Mutation m = new Mutation(userid);m.at().family(&quot;age&quot;).put(age);m.at().family(&quot;address&quot;).put(address);m.at().family(&quot;balance&quot;).put(account_balance);writer.add(m);We could then retrieve any of the columns for a specific userid by specifying theuserid as the range of a scanner and fetching specific columns:AccumuloClient client = Accumulo.newClient()                          .from(&quot;/path/to/accumulo-client.properties&quot;).build();Range r = new Range(userid, userid); // single rowScanner s = client.createScanner(&quot;userdata&quot;, auths);s.setRange(r);s.fetchColumnFamily(&quot;age&quot;);for (Entry&amp;lt;Key,Value&amp;gt; entry : s) {  System.out.println(entry.getValue().toString());}RowID DesignOften it is necessary to transform the rowID in order to have rows ordered in a waythat is optimal for anticipated access patterns. A good example of this is reversingthe order of components of internet domain names in order to group rows of thesame parent domain together:com.google.codecom.google.labscom.google.mailcom.yahoo.mailcom.yahoo.researchSome data may result in the creation of very large rows - rows with many columns.In this case the table designer may wish to split up these rows for better loadbalancing while keeping them sorted together for scanning purposes. This can bedone by appending a random substring at the end of the row:com.google.code_00com.google.code_01com.google.code_02com.google.labs_00com.google.mail_00com.google.mail_01It could also be done by adding a string representation of some period of time such as date to the weekor month:com.google.code_201003com.google.code_201004com.google.code_201005com.google.labs_201003com.google.mail_201003com.google.mail_201004Appending dates provides the additional capability of restricting a scan to a givendate range.LexicodersSince Keys in Accumulo are sorted lexicographically by default, it’s often useful to encodecommon data types into a byte format in which their sort order corresponds to the sort orderin their native form. An example of this is encoding dates and numerical data so that they canperform better during a seek or range search.The lexicoders are a standard and extensible way of encoding Java types. Here’s an exampleof a lexicoder that encodes a java Date object so that it sorts lexicographically:// create new date lexicoderDateLexicoder dateEncoder = new DateLexicoder();// truncate time to hourslong epoch = System.currentTimeMillis();Date hour = new Date(epoch - (epoch % 3600000));// encode the rowId so that it is sorted lexicographicallyMutation mutation = new Mutation(dateEncoder.encode(hour));mutation.at().family(&quot;colf&quot;).qualifier(&quot;colq&quot;).put(new byte[]{});If we want to return the most recent date first, we can reverse the sort orderwith the reverse lexicoder:// create new date lexicoder and reverse lexicoderDateLexicoder dateEncoder = new DateLexicoder();ReverseLexicoder reverseEncoder = new ReverseLexicoder(dateEncoder);// truncate date to hourslong epoch = System.currentTimeMillis();Date hour = new Date(epoch - (epoch % 3600000));// encode the rowId so that it sorts in reverse lexicographic orderMutation mutation = new Mutation(reverseEncoder.encode(hour));mutation.at().family(&quot;colf&quot;).qualifier(&quot;colq&quot;).put(new byte[]{});IndexingIn order to support lookups via more than one attribute of an entity, additionalindexes can be built. However, because Accumulo tables can support any number ofcolumns without specifying them beforehand, a single additional index will oftensuffice for supporting lookups of records in the main table. Here, the index has, asthe rowID, the Value or Term from the main table, the column families are the same,and the column qualifier of the index table contains the rowID from the main table.            RowID      Column Family      Column Qualifier      Value                  Term      Field Name      MainRowID             Note: We store rowIDs in the column qualifier rather than the Value so that we canhave more than one rowID associated with a particular term within the index. If westored this in the Value we would only see one of the rows in which the valueappears since Accumulo is configured by default to return the one most recentvalue associated with a key.Lookups can then be done by scanning the Index Table first for occurrences of thedesired values in the columns specified, which returns a list of row ID from the maintable. These can then be used to retrieve each matching record, in their entirety, or asubset of their columns, from the Main Table.To support efficient lookups of multiple rowIDs from the same table, the Accumuloclient library provides a BatchScanner. Users specify a set of Ranges to theBatchScanner, which performs the lookups in multiple threads to multiple serversand returns an Iterator over all the rows retrieved. The rows returned are NOT insorted order, as is the case with the basic Scanner interface.HashSet&amp;lt;Range&amp;gt; matchingRows = new HashSet&amp;lt;Range&amp;gt;();// first we scan the index for IDs of rows matching our querytry (Scanner indexScanner = client.createScanner(&quot;index&quot;, auths)) {  indexScanner.setRange(Range.exact(&quot;mySearchTerm&quot;);  // we retrieve the matching rowIDs and create a set of ranges  for (Entry&amp;lt;Key,Value&amp;gt; entry : indexScanner) {    matchingRows.add(new Range(entry.getKey().getColumnQualifier()));  }}// now we pass the set of rowIDs to the batch scanner to retrieve themtry (BatchScanner bscan = client.createBatchScanner(&quot;table&quot;, auths, 10)) {  bscan.setRanges(matchingRows);  bscan.fetchColumnFamily(&quot;attributes&quot;);  for (Entry&amp;lt;Key,Value&amp;gt; entry : bscan) {    System.out.println(entry.getValue());  }}One advantage of the dynamic schema capabilities of Accumulo is that differentfields may be indexed into the same physical table. However, it may be necessary tocreate different index tables if the terms must be formatted differently in order tomaintain proper sort order. For example, real numbers must be formatteddifferently than their usual notation in order to be sorted correctly. In these cases,usually one index per unique data type will suffice.Entity-Attribute and Graph TablesAccumulo is ideal for storing entities and their attributes, especially of theattributes are sparse. It is often useful to join several datasets together on commonentities within the same table. This can allow for the representation of graphs,including nodes, their attributes, and connections to other nodes.Rather than storing individual events, Entity-Attribute or Graph tables storeaggregate information about the entities involved in the events and therelationships between entities. This is often preferable when single events aren’tvery useful and when a continuously updated summarization is desired.The physical schema for an entity-attribute or graph table is as follows:            RowID      Column Family      Column Qualifier      Value                  EntityID      Attribute Name      Attribute Value      Weight              EntityID      Edge Type      Related EntityID      Weight      For example, to keep track of employees, managers and products the followingentity-attribute table could be used. Note that the weights are not always necessaryand are set to 0 when not used.            RowID      Column Family      Column Qualifier      Value                  E001      name      bob      0              E001      department      sales      0              E001      hire_date      20030102      0              E001      units_sold      P001      780              E002      name      george      0              E002      department      sales      0              E002      manager_of      E001      0              E002      manager_of      E003      0              E003      name      harry      0              E003      department      accounts_recv      0              E003      hire_date      20000405      0              E003      units_sold      P002      566              E003      units_sold      P001      232              P001      product_name      nike_airs      0              P001      product_type      shoe      0              P001      in_stock      germany      900              P001      in_stock      brazil      200              P002      product_name      basic_jacket      0              P002      product_type      clothing      0              P002      in_stock      usa      3454              P002      in_stock      germany      700      To allow efficient updating of edge weights, an aggregating iterator can beconfigured to add the value of all mutations applied with the same key. These typesof tables can easily be created from raw events by simply extracting the entities,attributes, and relationships from individual events and inserting the keys intoAccumulo each with a count of 1. The aggregating iterator will take care ofmaintaining the edge weights.Document-Partitioned IndexingUsing a simple index as described above works well when looking for records thatmatch one of a set of given criteria. When looking for records that match more thanone criterion simultaneously, such as when looking for documents that contain all ofthe words ‘the’ and ‘white’ and ‘house’, there are several issues.First is that the set of all records matching any one of the search terms must be sentto the client, which incurs a lot of network traffic. The second problem is that theclient is responsible for performing set intersection on the sets of records returnedto eliminate all but the records matching all search terms. The memory of the clientmay easily be overwhelmed during this operation.For these reasons Accumulo includes support for a scheme known as shardedindexing, in which these set operations can be performed at the TabletServers anddecisions about which records to include in the result set can be made withoutincurring network traffic.This is accomplished via partitioning records into bins that each reside on at mostone TabletServer, and then creating an index of terms per record within each bin asfollows:            RowID      Column Family      Column Qualifier      Value                  BinID      Term      DocID      Weight      Documents or records are mapped into bins by a user-defined ingest application. Bystoring the BinID as the RowID we ensure that all the information for a particularbin is contained in a single tablet and hosted on a single TabletServer sinceAccumulo never splits rows across tablets. Storing the Terms as column familiesserves to enable fast lookups of all the documents within this bin that contain thegiven term.Finally, we perform set intersection operations on the TabletServer via a specialiterator called the Intersecting Iterator. Since documents are partitioned into manybins, a search of all documents must search every bin. We can use the BatchScannerto scan all bins in parallel. The Intersecting Iterator should be enabled on aBatchScanner within user query code as follows:Text[] terms = {new Text(&quot;the&quot;), new Text(&quot;white&quot;), new Text(&quot;house&quot;)};try (BatchScanner bscan = client.createBatchScanner(table, auths, 20)) {  IteratorSetting iter = new IteratorSetting(20, &quot;ii&quot;, IntersectingIterator.class);  IntersectingIterator.setColumnFamilies(iter, terms);  bscan.addScanIterator(iter);  bscan.setRanges(Collections.singleton(new Range()));  for (Entry&amp;lt;Key,Value&amp;gt; entry : bscan) {    System.out.println(&quot; &quot; + entry.getKey().getColumnQualifier());  }}This code effectively has the BatchScanner scan all tablets of a table, looking fordocuments that match all the given terms. Because all tablets are being scanned forevery query, each query is more expensive than other Accumulo scans, whichtypically involve a small number of TabletServers. This reduces the number ofconcurrent queries supported and is subject to what is known as the ‘straggler’problem in which every query runs as slow as the slowest server participating.Of course, fast servers will return their results to the client which can display themto the user immediately while they wait for the rest of the results to arrive. If theresults are unordered this is quite effective as the first results to arrive are as goodas any others to the user.",
      "url": " /docs/2.x/getting-started/table_design",
      "categories": "getting-started"
    },
  
    "docs-2-x-index": {
      "title": "Apache Accumulo Documentation",
      "content": "",
      "url": " /docs/2.x/index",
      "categories": ""
    },
  
    "docs-2-x-security-authentication": {
      "title": "Authentication",
      "content": "Accumulo has authentication to verify the identity of users.ConfigurationAccumulo can be configured to use different authentication methods:            Method      Setting for instance.security.authenticator                  Password (default)      org.apache.accumulo.server.security.handler.ZKAuthenticator              Kerberos      org.apache.accumulo.server.security.handler.KerberosAuthenticator      All authentication methods implement Authenticator. The default (password-based) implementation method is described in this document.Root userWhen Accumulo is initialized, a root user is created and givena password.  This root user is used to create other users.Creating usersUsers can be created in the shell:root@uno&amp;gt; createuser bobEnter new password for &#39;bob&#39;: ****Please confirm new password for &#39;bob&#39;: ****In the Java API using SecurityOperations:client.securityOperations().createLocalUser(&quot;bob&quot;, new PasswordToken(&quot;pass&quot;));Authenticating usersUsers are authenticated when they create an Accumulo clientor when they log in to the Accumulo shell.Authentication can also be tested in the shell:root@myinstance mytable&amp;gt; authenticate bobEnter current password for &#39;bob&#39;: ****ValidIn the Java API using SecurityOperations:boolean valid = client.securityOperations().authenticateUser(&quot;bob&quot;, new PasswordToken(&quot;pass&quot;));Changing user passwordsA user’s password can be changed in the shell:root@uno&amp;gt; passwd -u bobEnter current password for &#39;root&#39;: ******Enter new password for &#39;bob&#39;: ***In the Java API using SecurityOperations:client.securityOperations().changeLocalUserPassword(&quot;bob&quot;, new PasswordToken(&quot;pass&quot;));Removing usersUsers can be removed in the shell:root@uno&amp;gt; dropuser bobdropuser { bob } (yes|no)? yesIn the Java API using SecurityOperations:client.securityOperations().dropLocalUser(&quot;bob&quot;);",
      "url": " /docs/2.x/security/authentication",
      "categories": "security"
    },
  
    "docs-2-x-security-authorizations": {
      "title": "Authorizations",
      "content": "In Accumulo, data is written with security labels that limit access to only users with the properauthorizations.ConfigurationAccumulo’s Authorizor is configured by setting instance.security.authorizor. The defaultauthorizor is the ZKAuthorizor which is describedbelow.Security LabelsEvery Key-Value pair in Accumulo has its own security label, stored under the column visibilityelement of the key, which is used to determine whether a given user meets the securityrequirements to read the value. This enables data of various security levels to be storedwithin the same row, and users of varying degrees of access to query the same table, whilepreserving data confidentiality.Writing labeled dataWhen writing data to Accumulo, users canspecify a security label for each value by passing a ColumnVisibility to the Mutation.try (BatchWriter writer = client.createBatchWriter(&quot;employees&quot;)) {  Mutation mut = new Mutation(&quot;employee1&quot;);  mut.at().family(&quot;pay&quot;).qualifier(&quot;salary&quot;).visibility(&quot;payroll&quot;).value(&quot;50000&quot;);  mut.at().family(&quot;pay&quot;).qualifier(&quot;period&quot;).visibility(&quot;public&quot;).value(&quot;monthly&quot;);  writer.addMutation(mut)}Security Label Expression SyntaxSecurity labels consist of a set of user-defined tokens that are required to read thevalue the label is associated with. The set of tokens required can be specified usingsyntax that supports logical AND &amp;amp; and OR | combinations of terms, aswell as nesting groups () of terms together.Each term is comprised of one to many alphanumeric characters, hyphens, underscores orperiods. Optionally, each term may be wrapped in quotation markswhich removes the restriction on valid characters. In quoted terms, quotation marksand backslash characters can be used as characters in the term by escaping themwith a backslash.For example, suppose within our organization we want to label our data values withsecurity labels defined in terms of user roles. We might have tokens such as:adminauditsystemThese can be specified alone or combined using logical operators:// Users must have admin privilegesadmin// Users must have admin and audit privilegesadmin&amp;amp;audit// Users with either admin or audit privilegesadmin|audit// Users must have audit and one or both of admin or system(admin|system)&amp;amp;auditWhen both | and &amp;amp; operators are used, parentheses must be used to specifyprecedence of the operators.AuthorizationsWhen clients attempt to read data from Accumulo, any security labels present areexamined against an Authorizations object passed by the client code when theScanner or BatchScanner are created. If the Authorizations are determined to beinsufficient to satisfy the security label, the value is suppressed from the set ofresults sent back to the client.Authorizations are specified as a comma-separated list of tokens the user possesses:// user possesses both admin and system level accessAuthorizations auths = new Authorizations(&quot;admin&quot;,&quot;system&quot;);Scanner s = client.createScanner(&quot;table&quot;, auths);User AuthorizationsEach Accumulo user has a set of associated security labels. To manipulate these inthe Accumulo shell, use the setuaths and getauths commands. They can beretrieved and modified in Java using getUserAuthorizations and changeUserAuthorizationsmethods of SecurityOperations.When a user creates a Scanner or BatchScanner, a set of Authorizations is passed.If the Authorizations passed to the scanner are not a subset of the user’s Authorizations,then an exception will be thrown.To prevent users from writing data they can not read, add the visibilityconstraint to a table. Use the -evc option in the createtable shell command toenable this constraint. For existing tables, use the config command toenable the visibility constraint. Ensure the constraint number does notconflict with any existing constraints.config -t table -s table.constraint.1=org.apache.accumulo.core.security.VisibilityConstraintAny user with the alter table permission can add or remove this constraint.This constraint is not applied to bulk imported data, if this a concern thendisable the bulk import permission.Advanced Authorizations HandlingFor applications serving many users, it is not expected that an Accumulo userwill be created for each application user. In this case an Accumulo user withall authorizations needed by any of the application’s users must be created. Toservice queries, the application should create a scanner with the applicationuser’s authorizations. These authorizations could be obtained from a trusted 3rdparty.Often production systems will integrate with Public-Key Infrastructure (PKI) anddesignate client code within the query layer to negotiate with PKI servers in orderto authenticate users and retrieve their authorization tokens (credentials). Thisrequires users to specify only the information necessary to authenticate themselvesto the system. Once user identity is established, their credentials can be accessed bythe client code and passed to Accumulo outside of the reach of the user.",
      "url": " /docs/2.x/security/authorizations",
      "categories": "security"
    },
  
    "docs-2-x-security-kerberos": {
      "title": "Kerberos",
      "content": "OverviewKerberos is a network authentication protocol that provides a secure way forpeers to prove their identity over an unsecure network in a client-server model.A centralized key-distribution center (KDC) is the service that coordinatesauthentication between a client and a server. Clients and servers use “tickets”,obtained from the KDC via a password or a special file called a “keytab”, tocommunicate with the KDC and prove their identity. A KDC administrator mustcreate the principal (name for the client/server identity) and the passwordor keytab, securely passing the necessary information to the actual user/service.Properly securing the KDC and generated ticket material is central to the securitymodel and is mentioned only as a warning to administrators running their own KDC.To interact with Kerberos programmatically, GSSAPI and SASL are two standardswhich allow cross-language integration with Kerberos for authentication. GSSAPI,the generic security service application program interface, is a standard whichKerberos implements. In the Java programming language, the language itself also implementsGSSAPI which is leveraged by other applications, like Apache Hadoop and Apache Thrift.SASL, simple authentication and security layer, is a framework for authenticationand security over the network. SASL provides a number of mechanisms for authentication,one of which is GSSAPI. Thus, SASL provides the transport which authenticatesusing GSSAPI that Kerberos implements.Kerberos is a very complicated software application and is deserving of muchmore description than can be provided here. An explain like I’m 5blog post is very good at distilling the basics, while MIT Kerberos’s project pagecontains lots of documentation for users or administrators. Various Hadoop “vendors”also provide free documentation that includes step-by-step instructions forconfiguring Hadoop and ZooKeeper (which will be henceforth considered as prerequisites).Within HadoopOut of the box, HDFS and YARN have no ability to enforce that a user is whothey claim they are. Thus, any basic Hadoop installation should be treated asunsecure: any user with access to the cluster has the ability to access any data.Using Kerberos to provide authentication, users can be strongly identified, delegatingto Kerberos to determine who a user is and enforce that a user is who they claim to be.As such, Kerberos is widely used across the entire Hadoop ecosystem for strongauthentication. Since server processes accessing HDFS or YARN are requiredto use Kerberos to authenticate with HDFS, it makes sense that they also requireKerberos authentication from their clients, in addition to other features providedby SASL.A typical deployment involves the creation of Kerberos principals for all serverprocesses (Hadoop datanodes and namenode(s), ZooKeepers), the creation of a keytabfile for each principal and then proper configuration for the Hadoop site xml files.Users also need Kerberos principals created for them; however, a user typicallyuses a password to identify themselves instead of a keytab. Users can obtain aticket granting ticket (TGT) from the KDC using their password which allows themto authenticate for the lifetime of the TGT (typically one day by default) and alleviatesthe need for further password authentication.For client server applications, like web servers, a keytab can be created whichallow for fully-automated Kerberos identification removing the need to enter anypassword, at the cost of needing to protect the keytab file. These principalswill apply directly to authentication for clients accessing Accumulo and theAccumulo processes accessing HDFS.Delegation TokensMapReduce, a common way that clients interact with Accumulo, does not map well to theclient-server model that Kerberos was originally designed to support. Specifically, the parallelizationof tasks across many nodes introduces the problem of securely sharing the user credentials acrossthese tasks in as safe a manner as possible. To address this problem, Hadoop introduced the notionof a delegation token to be used in distributed execution settings.A delegation token is nothing more than a short-term, on-the-fly password generated after authenticating with the user’scredentials.  In Hadoop itself, the Namenode and ResourceManager, for HDFS and YARN respectively, act as the gateway fordelegation tokens requests. For example, before a YARN job is submitted, the implementation will request delegationtokens from the NameNode and ResourceManager so the YARN tasks can communicate with HDFS and YARN. In the same manner,support has been added in the Accumulo Manager to generate delegation tokens to enable interaction with Accumulo viaMapReduce when Kerberos authentication is enabled in a manner similar to HDFS and YARN.Generating an expiring password is, arguably, more secure than distributing the user’scredentials across the cluster as only access to HDFS, YARN and Accumulo would becompromised in the case of the token being compromised as opposed to the entireKerberos credential. Additional details for clients and servers will be coveredin subsequent sections.Configuring AccumuloTo configure Accumulo for use with Kerberos, both client-facing and server-facingchanges must be made for a functional system on secured Hadoop. As previously mentioned,numerous guidelines already exist on the subject of configuring Hadoop and ZooKeeper foruse with Kerberos and won’t be covered here. It is assumed that you have functionalHadoop and ZooKeeper already installed.Note that on an existing cluster the server side changes will require a full cluster shutdown and restart. You shouldwait to restart the TraceServers until after you’ve completed the rest of the cluster set up and provisioneda trace user with appropriate permissions.ServersThe first step is to obtain a Kerberos identity for the Accumulo server processes.When running Accumulo with Kerberos enabled, a valid Kerberos identity will be requiredto initiate any RPC between Accumulo processes (e.g. Manager and TabletServer) in additionto any HDFS action (e.g. client to HDFS or TabletServer to HDFS).Generate Principal and KeytabIn the kadmin.local shell or using the -q option on kadmin.local, create aprincipal for Accumulo for all hosts that are running Accumulo processes. A Kerberosprincipal is of the form “primary/instance@REALM”. “accumulo” is commonly the “primary”(although not required) and the “instance” is the fully-qualified domain name forthe host that will be running the Accumulo process – this is required.kadmin.local -q &quot;addprinc -randkey accumulo/host.domain.com&quot;Perform the above for each node running Accumulo processes in the instance, modifying“host.domain.com” for your network. The randkey option generates a random passwordbecause we will use a keytab for authentication, not a password, since the Accumuloserver processes don’t have an interactive console to enter a password into.kadmin.local -q &quot;xst -k accumulo.hostname.keytab accumulo/host.domain.com&quot;To simplify deployments, at the cost of security, all Accumulo principals couldbe globbed into a single keytabkadmin.local -q &quot;xst -k accumulo.service.keytab -glob accumulo*&quot;To ensure that the SASL handshake can occur from clients to servers and servers to servers,all Accumulo servers must share the same instance and realm principal components as the“client” needs to know these to set up the connection with the “server”.Server ConfigurationA number of properties need to be changed to account to properly configure serversin accumulo.properties.            Key      Suggested Value      Description                  general.kerberos.keytab      /etc/security/keytabs/accumulo.service.keytab      The path to the keytab for Accumulo on local filesystem. Change the value to the actual path on your system.              general.kerberos.principal      accumulo/_HOST@REALM      The Kerberos principal for Accumulo, needs to match the keytab. “_HOST” can be used instead of the actual hostname in the principal and will be automatically expanded to the current FQDN which reduces the configuration file burden.              instance.rpc.sasl.enabled      true      Enables SASL for the Thrift Servers (supports GSSAPI)              rpc.sasl.qop      auth      One of “auth”, “auth-int”, or “auth-conf”. These map to the SASL defined properties for quality of protection. “auth” is authentication only. “auth-int” is authentication and data integrity. “auth-conf” is authentication, data integrity and confidentiality.              instance.security.authenticator      org.apache.accumulo.server.security.handler.KerberosAuthenticator      Configures Accumulo to use the Kerberos principal as the Accumulo username/principal              instance.security.authorizor      org.apache.accumulo.server.security.handler.KerberosAuthorizor      Configures Accumulo to use the Kerberos principal for authorization purposes              instance.security.permissionHandler      org.apache.accumulo.server.security.handler.KerberosPermissionHandler      Configures Accumulo to use the Kerberos principal for permission purposes              trace.token.type      org.apache.accumulo.core.client.security.tokens.KerberosToken      Configures the Accumulo Tracer to use the KerberosToken for authentication when serializing traces to the trace table.              trace.user      accumulo/_HOST@REALM      The tracer process needs valid credentials to serialize traces to Accumulo. While the other server processes are creating a SystemToken from the provided keytab and principal, we can still use a normal KerberosToken and the same keytab/principal to serialize traces. Like non-Kerberized instances, the table must be created and permissions granted to the trace.user. The same _HOST replacement is performed on this value, substituted the FQDN for _HOST.              trace.token.property.keytab             You can optionally specify the path to a keytab file for the principal given in the trace.user property. If you don’t set this path, it will default to the value given in general.kerberos.principal.              general.delegation.token.lifetime      7d      The length of time that the server-side secret used to create delegation tokens is valid. After a server-side secret expires, a delegation token created with that secret is no longer valid.              general.delegation.token.update.interval      1d      The frequency in which new server-side secrets should be generated to create delegation tokens for clients. Generating new secrets reduces the likelihood of cryptographic attacks.      Although it should be a prerequisite, it is ever important that you have DNS properlyconfigured for your nodes and that Accumulo is configured to use the FQDN. Itis extremely important to use the FQDN in each of the “hosts” files for eachAccumulo process: managers, monitors, tservers, tracers, and gc.Normally, no changes are needed in accumulo-env.sh to enable Kerberos. Typically, the krb5.confis installed on the local machine in /etc/, and the Java library implementations will lookhere to find the necessary configuration to communicate with the KDC. Some installationsmay require a different krb5.conf to be used for Accumulo which can be accomplishedby adding the JVM system property -Djava.security.krb5.conf=/path/to/other/krb5.conf toJAVA_OPTS in accumulo-env.sh.KerberosAuthenticatorThe KerberosAuthenticator is an implementation of the pluggable security interfacesthat Accumulo provides. It builds on top of what the default ZooKeeper-based implementation,but removes the need to create user accounts with passwords in Accumulo for clients. Aslong as a client has a valid Kerberos identity, they can connect to and interact withAccumulo, but without any permissions (e.g. cannot create tables or write data). LeveragingZooKeeper removes the need to change the permission handler and authorizor, so other Accumulofunctions regarding permissions and cell-level authorizations do not change.It is extremely important to note that, while user operations like SecurityOperations.listLocalUsers(),SecurityOperations.dropLocalUser(), and SecurityOperations.createLocalUser() will not returnerrors, these methods are not equivalent to normal installations, as they will only operate onusers which have, at one point in time, authenticated with Accumulo using their Kerberos identity.The KDC is still the authoritative entity for user management. The previously mentioned methodsare provided as they simplify management of users within Accumulo, especially with respectto granting Authorizations and Permissions to new users.Administrative UserOut of the box (without Kerberos enabled), Accumulo has a single user with administrative permissions “root”.This user is used to “bootstrap” other users, creating less-privileged users for applications usingthe system. In Kerberos, to authenticate with the system, it’s required that the client presents Kerberoscredentials for the principal (user) the client is trying to authenticate as.Because of this, an administrative user named “root” would be useless in an instance using Kerberos,because it is very unlikely to have Kerberos credentials for a principal named root. When Kerberos isenabled, Accumulo will prompt for the name of a user to grant the same permissions as what the rootuser would normally have. The name of the Accumulo user to grant administrative permissions to canalso be given by the -u or --user options.If you are enabling Kerberos on an existing cluster, you will need to reinitialize the security system inorder to replace the existing “root” user with one that can be used with Kerberos. These steps should becompleted after you have done the previously described configuration changes and will require access toa complete accumulo.properties, including the instance secret. Note that this process will delete allexisting users in the system; you will need to reassign user permissions based on Kerberos principals.  Ensure Accumulo is not running.  Given the path to a accumulo.properties with the instance secret, run the security reset tool. If you areprompted for a password you can just hit return, since it won’t be used.  Start the Accumulo cluster$ accumulo-cluster stop...$ accumulo init --reset-securityRunning against secured HDFSPrincipal (user) to grant administrative privileges to : acculumo_admin@EXAMPLE.COMEnter initial password for accumulo_admin@EXAMPLE.COM (this may not be applicable for your security setup):Confirm initial password for accumulo_admin@EXAMPLE.COM:$ accumulo-cluster start...Verifying secure accessTo verify that servers have correctly started with Kerberos enabled, ensure that the processesare actually running (they should exit immediately if login fails) and verify that you seesomething similar to the following in the application log.2015-01-07 11:57:56,826 [security.SecurityUtil] INFO : Attempting to login with keytab as accumulo/hostname@EXAMPLE.COM2015-01-07 11:57:56,830 [security.UserGroupInformation] INFO : Login successful for user accumulo/hostname@EXAMPLE.COM using keytab file /etc/security/keytabs/accumulo.service.keytabImpersonationImpersonation is functionality which allows a certain user to act as another. One direct applicationof this concept within Accumulo is the Thrift proxy. The Thrift proxy is configured to acceptuser requests and pass them onto Accumulo, enabling client access to Accumulo via any thrift-compatiblelanguage. When the proxy is running with SASL transports, this enforces that clients present a validKerberos identity to make a connection. In this situation, the Thrift proxy server does not haveaccess to the secret key material in order to make a secure connection to Accumulo as the client,it can only connect to Accumulo as itself. Impersonation, in this context, refers to the abilityof the proxy to authenticate to Accumulo as itself, but act on behalf of an Accumulo user.Accumulo supports basic impersonation of end-users by a third party via static rules inaccumulo.properties. These two properties are semicolon separated properties which are alignedby index. This first element in the user impersonation property value matches the first elementin the host impersonation property value, etc.instance.rpc.sasl.allowed.user.impersonation=$PROXY_USER:*instance.rpc.sasl.allowed.host.impersonation=*Here, $PROXY_USER can impersonate any user from any host.The following is an example of specifying a subset of users $PROXY_USER can impersonate and alsolimiting the hosts from which $PROXY_USER can initiate requests from.instance.rpc.sasl.allowed.user.impersonation=$PROXY_USER:user1,user2;$PROXY_USER2:user2,user4instance.rpc.sasl.allowed.host.impersonation=host1.domain.com,host2.domain.com;*Here, $PROXY_USER can impersonate user1 and user2 only from host1.domain.com or host2.domain.com.$PROXY_USER2 can impersonate user2 and user4 from any host.In these examples, the value $PROXY_USER is the Kerberos principal of the server which is acting on behalf of a user.Impersonation is enforced by the Kerberos principal and the host from which the RPC originated (from the perspectiveof the Accumulo TabletServers/Managers). An asterisk (*) can be used to specify all users or all hosts (depending on the context).Delegation TokensWithin Accumulo services, the primary task to implement delegation tokens is the generation and distributionof a shared secret among all Accumulo tabletservers and the manager. The secret key allows for generationof delegation tokens for users and verification of delegation tokens presented by clients. If a serverprocess is unaware of the secret key used to create a delegation token, the client cannot be authenticated.As ZooKeeper distribution is an asynchronous operation (typically on the order of seconds), thevalue for general.delegation.token.update.interval should be on the order of hours to days to reduce thelikelihood of servers rejecting valid clients because the server did not yet see a new secret key.Supporting authentication with both Kerberos credentials and delegation tokens, the SASL thriftserver accepts connections with either GSSAPI and DIGEST-MD5 mechanisms set. The DIGEST-MD5 mechanismenables authentication as a normal username and password exchange which DelegationTokens leverages.Since delegation tokens are a weaker form of authentication than Kerberos credentials, user accessto obtain delegation tokens from Accumulo is protected with the DELEGATION_TOKEN system permission. Onlyusers with the system permission are allowed to obtain delegation tokens. It is also recommendedto configure confidentiality with SASL, using the rpc.sasl.qop=auth-conf configuration property, toensure that prying eyes cannot view the DelegationToken as it passes over the network.# Check a user&#39;s permissionsadmin@REALM@accumulo&amp;gt; userpermissions -u user@REALM# Grant the DELEGATION_TOKEN system permission to a useradmin@REALM@accumulo&amp;gt; grant System.DELEGATION_TOKEN -s -u user@REALMClientsCreate client principalLike the Accumulo servers, clients must also have a Kerberos principal created for them. Theprimary difference between a server principal is that principals for users are createdwith a password and also not qualified to a specific instance (host).kadmin.local -q &quot;addprinc $user&quot;The above will prompt for a password for that user which will be used to identify that $user.The user can verify that they can authenticate with the KDC using the command kinit $user.Upon entering the correct password, a local credentials cache will be made which can be usedto authenticate with Accumulo, access HDFS, etc.The user can verify the state of their local credentials cache by using the command klist.$ klistTicket cache: FILE:/tmp/krb5cc_123Default principal: user@EXAMPLE.COMValid starting       Expires              Service principal01/07/2015 11:56:35  01/08/2015 11:56:35  krbtgt/EXAMPLE.COM@EXAMPLE.COM    renew until 01/14/2015 11:56:35ConfigurationThe second thing clients need to do is to configure kerberos when an Accumulo client iscreated.  This can be done using client builder methods or by setting the propertiesbelow in accumulo-client.properties which can be provided to Accumulo utilities usingthe --config-file command line option.  sasl.enabled = true  sasl.qop = auth  sasl.kerberos.server.primary = accumuloEach of these properties must match the configuration of the accumulo servers; this isrequired to set up the SASL transport.Verifying Administrative AccessAt this point you should have enough configured on the server and client side to interact withthe system. You should verify that the administrative user you chose earlier can successfullyinteract with the system.While this example logs in via kinit with a password, any login method that caches Kerberos ticketsshould work.$ kinit accumulo_admin@EXAMPLE.COMPassword for accumulo_admin@EXAMPLE.COM: ******************************$ accumulo shellShell - Apache Accumulo Interactive Shell-- version: 1.7.2- instance name: MYACCUMULO- instance id: 483b9038-889f-4b2d-b72b-dfa2bb5dbd07-- type &#39;help&#39; for a list of available commands-accumulo_admin@EXAMPLE.COM@MYACCUMULO&amp;gt; userpermissionsSystem permissions: System.GRANT, System.CREATE_TABLE, System.DROP_TABLE, System.ALTER_TABLE, System.CREATE_USER, System.DROP_USER, System.ALTER_USER, System.SYSTEM, System.CREATE_NAMESPACE, System.DROP_NAMESPACE, System.ALTER_NAMESPACE, System.OBTAIN_DELEGATION_TOKENNamespace permissions (accumulo): Namespace.READ, Namespace.ALTER_TABLETable permissions (accumulo.metadata): Table.READ, Table.ALTER_TABLETable permissions (accumulo.replication): Table.READTable permissions (accumulo.root): Table.READ, Table.ALTER_TABLEaccumulo_admin@EXAMPLE.COM@MYACCUMULO&amp;gt; quit$ kdestroy$DelegationTokens with MapReduceTo use DelegationTokens in a custom MapReduce job, the user should create an AccumuloClientusing a KerberosToken and use it to call SecurityOperations.getDelegationToken. TheDelegationToken that is created can then be used to create a new client using thisdelegation token.  The ClientInfo object from this client can be passed into the MapReducejob. It is expected that the user launching the MapReduce job is already logged in via Kerberosvia a keytab or via a locally-cached Kerberos ticket-granting-ticket (TGT).KerberosToken kt = new KerberosToken();AccumuloClient client = Accumulo.newClient().to(&quot;myinstance&quot;, &quot;zoo1,zoo2&quot;)                          .as(principal, kt).build();DelegationToken dt = client.securityOperations().getDelegationToken();Properties props = Accumulo.newClientProperties().from(client.properties())                          .as(principal, dt).build();// Reading from AccumuloAccumuloInputFormat.configure().clientProperties(props).store(job);// Writing to AccumuloAccumuloOutputFormat.configure().clientProperties(props).store(job);Users must have the DELEGATION_TOKEN system permission to call the getDelegationTokenmethod. The obtained delegation token is only valid for the requesting user for a periodof time dependent on Accumulo’s configuration (general.delegation.token.lifetime).For the duration of validity of the DelegationToken, the user must take the necessary precautionsto protect the DelegationToken from prying eyes as it can be used by any user on any host to impersonatethe user who requested the DelegationToken. YARN ensures that passing the delegation token from the clientJVM to each YARN task is secure, even in multi-tenant instances.DebuggingQ: I have valid Kerberos credentials and a correct client configuration file, butI still get errors like:java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]A: When you have a valid client configuration and Kerberos TGT, it is possible that the searchpath for your local credentials cache is incorrect. Check the value of the KRB5CCNAME environmentvalue, and ensure it matches the value reported by klist.$ echo $KRB5CCNAME$ klistTicket cache: FILE:/tmp/krb5cc_123Default principal: user@EXAMPLE.COMValid starting       Expires              Service principal01/07/2015 11:56:35  01/08/2015 11:56:35  krbtgt/EXAMPLE.COM@EXAMPLE.COM    renew until 01/14/2015 11:56:35$ export KRB5CCNAME=/tmp/krb5cc_123$ echo $KRB5CCNAME/tmp/krb5cc_123Q: I thought I had everything configured correctly, but my client/server still fails to log in.I don’t know what is actually failing.A: Add the following system property to the JVM invocation:-Dsun.security.krb5.debug=trueThis will enable lots of extra debugging at the JVM level which is often sufficient todiagnose some high-level configuration problem. Client applications can add this system property byhand to the command line and Accumulo server processes or applications started using the accumuloscript by adding the property to JAVA_OPTS in accumulo-env.sh.Additionally, you can increase the log4j levels on org.apache.hadoop.security, which includes theHadoop UserGroupInformation class, which will include some high-level debug statements. Thiscan be controlled in your client application, or using log4j-service.propertiesQ: All of my Accumulo processes successfully start and log in with theirkeytab, but they are unable to communicate with each other, showing thefollowing errors:2015-01-12 14:47:27,055 [transport.TSaslTransport] ERROR: SASL negotiation failurejavax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)]        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)        at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:253)        at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)        at org.apache.accumulo.core.rpc.UGIAssumingTransport$1.run(UGIAssumingTransport.java:53)        at org.apache.accumulo.core.rpc.UGIAssumingTransport$1.run(UGIAssumingTransport.java:49)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:415)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)        at org.apache.accumulo.core.rpc.UGIAssumingTransport.open(UGIAssumingTransport.java:49)        at org.apache.accumulo.core.rpc.ThriftUtil.createClientTransport(ThriftUtil.java:357)        at org.apache.accumulo.core.rpc.ThriftUtil.createTransport(ThriftUtil.java:255)        at org.apache.accumulo.server.manager.LiveTServerSet$TServerConnection.getTableMap(LiveTServerSet.java:106)        at org.apache.accumulo.manager.Manager.gatherTableInformation(Manager.java:996)        at org.apache.accumulo.manager.Manager.access$600(Manager.java:160)        at org.apache.accumulo.manager.Manager$StatusThread.updateStatus(Manager.java:911)        at org.apache.accumulo.manager.Manager$StatusThread.run(Manager.java:901)Caused by: GSSException: No valid credentials provided (Mechanism level: Server not found in Kerberos database (7) - LOOKING_UP_SERVER)        at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:710)        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:248)        at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)        at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)        ... 16 moreCaused by: KrbException: Server not found in Kerberos database (7) - LOOKING_UP_SERVER        at sun.security.krb5.KrbTgsRep.&amp;lt;init&amp;gt;(KrbTgsRep.java:73)        at sun.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:192)        at sun.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:203)        at sun.security.krb5.internal.CredentialsUtil.serviceCreds(CredentialsUtil.java:309)        at sun.security.krb5.internal.CredentialsUtil.acquireServiceCreds(CredentialsUtil.java:115)        at sun.security.krb5.Credentials.acquireServiceCreds(Credentials.java:454)        at sun.security.jgss.krb5.Krb5Context.initSecContext(Krb5Context.java:641)        ... 19 moreCaused by: KrbException: Identifier doesn&#39;t match expected value (906)        at sun.security.krb5.internal.KDCRep.init(KDCRep.java:143)        at sun.security.krb5.internal.TGSRep.init(TGSRep.java:66)        at sun.security.krb5.internal.TGSRep.&amp;lt;init&amp;gt;(TGSRep.java:61)        at sun.security.krb5.KrbTgsRep.&amp;lt;init&amp;gt;(KrbTgsRep.java:55)        ... 25 moreor2015-01-12 14:47:29,440 [server.TThreadPoolServer] ERROR: Error occurred during processing of message.java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Peer indicated failure: GSS initiate failed        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:219)        at org.apache.accumulo.core.rpc.UGIAssumingTransportFactory$1.run(UGIAssumingTransportFactory.java:51)        at org.apache.accumulo.core.rpc.UGIAssumingTransportFactory$1.run(UGIAssumingTransportFactory.java:48)        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:356)        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1608)        at org.apache.accumulo.core.rpc.UGIAssumingTransportFactory.getTransport(UGIAssumingTransportFactory.java:48)        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:208)        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)        at java.lang.Thread.run(Thread.java:745)Caused by: org.apache.thrift.transport.TTransportException: Peer indicated failure: GSS initiate failed        at org.apache.thrift.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:190)        at org.apache.thrift.transport.TSaslServerTransport.handleSaslStartMessage(TSaslServerTransport.java:125)        at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:253)        at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)        at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)        ... 10 moreA: As previously mentioned, the hostname, and subsequently the address each Accumulo process is bound/listeningon, is extremely important when negotiating an SASL connection. This problem commonly arises when the Accumuloservers are not configured to listen on the address denoted by their FQDN.The values in the Accumulo “hosts” files (In accumulo/conf: managers, monitors, tservers, tracers,and gc) should match the instance component of the Kerberos server principal (e.g. host in accumulo/host@EXAMPLE.COM).Q: After configuring my system for Kerberos, server processes come up normally and I can interact with the system. However,when I attempt to use the “Recent Traces” page on the Monitor UI I get a stacktrace similar to:java.lang.AssertionError: AuthenticationToken should not be null    at org.apache.accumulo.monitor.servlets.trace.Basic.getScanner(Basic.java:139)    at org.apache.accumulo.monitor.servlets.trace.Summary.pageBody(Summary.java:164)    at org.apache.accumulo.monitor.servlets.BasicServlet.doGet(BasicServlet.java:63)    at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)    at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:738)    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:551)    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)    at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:568)    at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:221)    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1111)    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:478)    at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:183)    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1045)    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97)    at org.eclipse.jetty.server.Server.handle(Server.java:462)    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:279)    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:232)    at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:534)    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:607)    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:536)    at java.lang.Thread.run(Thread.java:745)A: This indicates that the Monitor has not been able to successfully log in a client-side user to read from the trace table. Accumulo allows the TraceServer to rely on the property general.kerberos.keytab as a fallback when logging in the trace user if the trace.token.property.keytab property isn’t defined. Some earlier versions of Accumulo did not do this same fallback for the Monitor’s use of the trace user. The end result is that if you configure general.kerberos.keytab and not trace.token.property.keytab you will end up with a system that properly logs trace information but can’t view it.Ensure you have set trace.token.property.keytab to point to a keytab for the principal defined in trace.user in the accumulo.properties file for the Monitor, since that should work in all versions of Accumulo.",
      "url": " /docs/2.x/security/kerberos",
      "categories": "security"
    },
  
    "docs-2-x-security-on-disk-encryption": {
      "title": "On Disk Encryption",
      "content": "For an additional layer of security, Accumulo can encrypt files stored on-disk.  On Disk encryption was reworkedfor 2.0, making it easier to configure and more secure.  Starting with 2.1, On Disk Encryption can now be configuredper table as well as for the entire instance (all tables). The files that can be encrypted include: RFiles and Write AheadLogs (WALs). NOTE: This feature is considered experimental and upgrading a previously encrypted instanceis not supported. For more information, see the notes below.ConfigurationTo encrypt tables on-disk, encryption must be enabled before an Accumulo instance is initialized. This isdone by configuring a crypto service factory. If on-disk encryption is enabled on an existing cluster, only filescreated after it is enabled will be encrypted and existing data won’t be encrypted until compaction.Encrypting All TablesTo encrypt all tables, the generic crypto service factory can be used, GenericCryptoServiceFactory. This factoryis useful for general purpose on-disk encryption with no table context.instance.crypto.opts.factory=org.apache.accumulo.core.spi.crypto.GenericCryptoServiceFactoryThe GenericCryptoServiceFactory requires configuring a crypto service to load and this can be done by setting thegeneral.custom.crypto.service property.  The value of this property is theclass name of the service which will perform crypto on RFiles and WALs.general.custom.crypto.service=org.apache.accumulo.core.spi.crypto.AESCryptoServicePer Table EncryptionTo encrypt per table, the per table crypto service factory can be used, PerTableCryptoServiceFactory. This factorywill load a crypto service configured by table.instance.crypto.opts.factory=org.apache.accumulo.core.spi.crypto.PerTableCryptoServiceFactoryThe PerTableCryptoServiceFactory requires configuring a crypto service to load for the table RFiles and this can be done by adding thetable.crypto.opts.service property to a table. Example in the accumulo shell:createtable table1 -prop table.crypto.opts.service=org.apache.accumulo.core.spi.crypto.AESCryptoServiceThe PerTableCryptoServiceFactory also requires configuring a recovery and WAL crypto service by adding the followingproperties to your accumulo.properties file.general.custom.crypto.recovery.service=org.apache.accumulo.core.spi.crypto.AESCryptoServicegeneral.custom.crypto.wal.service=org.apache.accumulo.core.spi.crypto.AESCryptoServiceOut of the box, Accumulo provides the AESCryptoService for basic encryption needs.  This class provides AES encryptionwith Galois/Counter Mode (GCM) for RFiles and Cipher Block Chaining (CBC) mode for WALs.  The additional propertybelow is required by this crypto service to be set using the general.custom.crypto.* prefix.general.custom.crypto.key.uri=file:///secure/path/to/crypto-key-fileThis property tells the crypto service where to find the file containing the key encryption key. The key file can be 16 or 32 bytes.For example, openssl can be used to create a random 32 byte key:openssl rand -out /path/to/keyfile 32Initializing Accumulo after these instance properties are set, will enable on-disk encryption across your entire cluster.Disabling CryptoWhen using the AESCryptoService, crypto can be disabled by setting the property general.custom.crypto.enabled to false.However, this will disable all crypto as there is currently no way to disable only for specific tables. When disabledexisting encrypted files can still be read and scanned as long as the Accumulo instance and any table specificproperties are still configured but new files will not be encrypted.general.custom.crypto.enabled=falseCustom CryptoThe new crypto interface for 2.0 allows for easier custom implementation of encryption and decryption. Yourclass only has to implement the CryptoService interface to work with Accumulo.The interface has 3 methods:  void init(Map&amp;lt;String,String&amp;gt; conf) throws CryptoException;  FileEncrypter getFileEncrypter(CryptoEnvironment environment);  FileDecrypter getFileDecrypter(CryptoEnvironment environment);The init method is where you will initialize any resources required for crypto and will get called once per Tablet Server.The getFileEncrypter method requires implementation of a FileEncrypterfor encryption and the getFileDecrypter method requires implementation of a FileDecrypterfor decryption. The CryptoEnvironment passed into these methods will provide the scope of the crypto.The FileEncrypter has two methods:  OutputStream encryptStream(OutputStream outputStream) throws CryptoService.CryptoException;  byte[] getDecryptionParameters();The encryptStream method performs the encryption on the provided OutputStream and returns an OutputStream, most likelywrapped in at least one other OutputStream.  The getDecryptionParameters returns a byte array of anything that will berequired to perform decryption. The FileDecrypter only has one method:  InputStream decryptStream(InputStream inputStream) throws CryptoService.CryptoException;For more help getting started see AESCryptoService.Things to keep in mindUtilities need access to encryption propertiesWhen utilities run that read encrypted files but do not connect to Zookeeper the utility needs to be providedthe encryption properties. For example, when using rfile-info to examinean encrypted rfile the accumulo.properties file can be copied, the necessary encryption parameters added,and then the properties file can be passed to the utility with the -p argument.Some data will be unencryptedThe on-disk encryption configured here is only for RFiles and Write Ahead Logs (WALs).  The majority of data in Accumulois written to disk with these files, but there are a few scenarios that can take place where data will be unencrypted,even with the crypto service enabled.Data in Memory &amp;amp; LogsFor queries, data is decrypted when read from RFiles and cached in memory.  This means that data is unencrypted in memorywhile Accumulo is running.  Depending on the situation, this also means that some data can be printed to logs. A stacktrace being loggedduring an exception is one example. Accumulo developers have made sure not to expose data protected by authorizations during logging, butit’s the additional data that gets encrypted on-disk that could be exposed in a log file.Bulk ImportThere are 2 ways to create RFiles for bulk ingest: with the RFile API and during Map Reduce using AccumuloFileOutputFormat.The RFile API allows passing in the configuration properties for encryption mentioned above.  The AccumuloFileOutputFormat doesnot allow for encryption of RFiles so any data bulk imported through this process will be unencrypted.ZookeeperAccumulo stores a lot of metadata about the cluster in Zookeeper.  Keep in mind that this metadata does not get encrypted with On Disk encryption enabled.GCM performanceThe AESCryptoService uses GCM mode for RFiles. Java 9 introduced GHASH hardware support used by GCM.A test was performed on a VM with 4 2.3GHz processors and 16GB of RAM. The test encrypted and decrypted arrays of size 131072 bytes 1000000 times. The results are as follows:Java 9 GCM times:    Time spent encrypting:        209.210s    Time spent decrypting:        276.800sJava 8 GCM times:    Time spent encrypting:        2,818.440s    Time spent decrypting:        2,883.960sAs you can see, there is a significant performance hit when running without the GHASH CPU instruction. It is advised Java 9 or later be used when enabling encryption.",
      "url": " /docs/2.x/security/on-disk-encryption",
      "categories": "security"
    },
  
    "docs-2-x-security-overview": {
      "title": "Security Overview",
      "content": "Accumulo has the following security features:  Only authenticated users can access Accumulo.          Kerberos can be enabled to replace Accumulo’s default, password-based authentication        Users can only perform actions if they are given permission.  Users can only view labeled data that they are authorized to see.  Data can be encrypted on disk and over-the-wireImplementationBelow is a description of how security is implemented in Accumulo.Once a user is authenticated by the Authenticator, the user has access to the other actions withinAccumulo. All actions in Accumulo are ACLed, and this ACL check is handled by the PermissionHandler.This is what manages all of the permissions, which are divided in system and per tablelevel. From there, if a user is doing an action which requires authorizations, the Authorizor isqueried to determine what authorizations the user has.This setup allows a variety of different mechanisms to be used for handling different aspects ofAccumulo’s security. A system like Kerberos can be used for authentication, then a system like LDAPcould be used to determine if a user has a specific permission, and then it may default back to thedefault Authorizor to determine what Authorizations a user is ultimately allowed to use.This is a pluggable system so custom components can be created depending on your need.",
      "url": " /docs/2.x/security/overview",
      "categories": "security"
    },
  
    "docs-2-x-security-permissions": {
      "title": "Permissions",
      "content": "Accumulo users can only perform actions if they are given permission.Accumulo has three types of permissions:  SystemPermission  NamespacePermission  TablePermissionThese permissions are managed by SecurityOperations in Java API or the Accumulo shell.ConfigurationAccumulo’s PermissionHandler is configured by setting instance.security.permissionHandler.The default permission handler is described below.Granting permissionUsers can be granted permissions in the shell:root@uno&amp;gt; grant System.CREATE_TABLE -s -u bobOr in the Java API using SecurityOperations:client.securityOperations().grantSystem(&quot;bob&quot;, SystemPermission.CREATE_TABLE);View permissionsPermissions can be listed for a user in the shell:root@uno&amp;gt; userpermissions -u bobSystem permissions: System.CREATE_TABLE, System.DROP_TABLENamespace permissions (accumulo): Namespace.READTable permissions (accumulo.metadata): Table.READTable permissions (accumulo.replication): Table.READTable permissions (accumulo.root): Table.READRevoking permissionsPermissions can be revoked for a user in the shellroot@uno&amp;gt; revoke System.CREATE_TABLE -s -u bobOr in the Java API using SecurityOperations:client.securityOperations().revokeSystemPermission(&quot;bob&quot;, SystemPermission.CREATE_TABLE);",
      "url": " /docs/2.x/security/permissions",
      "categories": "security"
    },
  
    "docs-2-x-security-wire-encryption": {
      "title": "Wire Encryption",
      "content": "Accumulo, through Thrift’s TSSLTransport, provides the ability to encryptwire communication between Accumulo servers and clients using securesockets layer (SSL). SSL certificates signed by the same certificate authoritycontrol the “circle of trust” in which a secure connection can be established.Typically, each host running Accumulo processes would be given a certificatewhich identifies itself.Clients can optionally also be given a certificate, when client-auth is enabled,which prevents unwanted clients from accessing the system. The SSL integrationpresently provides no authentication support within Accumulo (an Accumulo usernameand password are still required) and is only used to establish a means forsecure communication.Server configurationAs previously mentioned, the circle of trust is established by the certificateauthority which created the certificates in use. Because of the tight couplingof certificate generation with an organization’s policies, Accumulo does notprovide a method in which to automatically create the necessary SSL components.Administrators without existing infrastructure built on SSL are encourage touse OpenSSL and the keytool command. An example of these commands areincluded in a section below. Accumulo servers require a certificate and keystore,in the form of Java KeyStores, to enable SSL. The following configuration assumesthese files already exist.In accumulo.properties, the following properties are required:  rpc.javax.net.ssl.keyStore  = The path on the local filesystem to the keystore containing the server’s certificate  rpc.javax.net.ssl.keyStorePassword = The password for the keystore containing the server’s certificate  rpc.javax.net.ssl.trustStore = The path on the local filesystem to the keystore containing the certificate authority’s public key  rpc.javax.net.ssl.trustStorePassword = The password for the keystore containing the certificate authority’s public key  instance.rpc.ssl.enabled = trueOptionally, SSL client-authentication (two-way SSL) can also be enabled by settinginstance.rpc.ssl.clientAuth true in accumulo.properties.This requires that each client has access to a valid certificate to set up a secure connectionto the servers. By default, Accumulo uses one-way SSL which does not require clients to havetheir own certificate.Client configurationTo establish a connection to Accumulo servers, each client must also havespecial configuration. This is typically accomplished by creating an Accumuloclient using accumulo-client.properties and setting the followingthe properties to connect to an Accumulo instance using SSL:  ssl.enabled to true  ssl.truststore.path  ssl.truststore.passwordIf two-way SSL is enabled for the Accumulo instance (by setting instance.rpc.ssl.clientAuth to true in accumulo.properties),Accumulo clients must also define their own certificate by setting the following properties:  ssl.keystore.path  ssl.keystore.passwordGenerating SSL material using OpenSSLThe following is included as an example for generating your own SSL material (certificate authority and server/clientcertificates) using OpenSSL and Java’s KeyTool command.Generate a certificate authority# Create a private keyopenssl genrsa -des3 -out root.key 4096# Create a certificate request using the private keyopenssl req -x509 -new -key root.key -days 365 -out root.pem# Generate a Base64-encoded version of the PEM just createdopenssl x509 -outform der -in root.pem -out root.der# Import the key into a Java KeyStorekeytool -import -alias root-key -keystore truststore.jks -file root.der# Remove the DER formatted key file (as we don&#39;t need it anymore)rm root.derThe truststore.jks file is the Java keystore which contains the certificate authority’s public key.Generate a certificate/keystore per hostIt’s common that each host in the instance is issued its own certificate (notably to ensure that revocation procedurescan be easily followed). The following steps can be taken for each host.# Create the private key for our serveropenssl genrsa -out server.key 4096# Generate a certificate signing request (CSR) with our private keyopenssl req -new -key server.key -out server.csr# Use the CSR and the CA to create a certificate for the server (a reply to the CSR)openssl x509 -req -in server.csr -CA root.pem -CAkey root.key -CAcreateserial     -out server.crt -days 365# Use the certificate and the private key for our server to create PKCS12 fileopenssl pkcs12 -export -in server.crt -inkey server.key -certfile server.crt     -name &#39;server-key&#39; -out server.p12# Create a Java KeyStore for the server using the PKCS12 file (private key)keytool -importkeystore -srckeystore server.p12 -srcstoretype pkcs12 -destkeystore     server.jks -deststoretype JKS# Remove the PKCS12 file as we don&#39;t need itrm server.p12# Import the CA-signed certificate to the keystorekeytool -import -trustcacerts -alias server-crt -file server.crt -keystore server.jksThe server.jks file is the Java keystore containing the certificate for a given host. The abovemethods are equivalent whether the certificate is generated for an Accumulo server or a client.",
      "url": " /docs/2.x/security/wire-encryption",
      "categories": "security"
    },
  
    "docs-2-x-troubleshooting-advanced": {
      "title": "Advanced Troubleshooting",
      "content": "Tablet server locksMy tablet server lost its lock.  Why?The primary reason a tablet server loses its lock is that it has been pushed into swap.A large java program (like the tablet server) may have a large portionof its memory image unused.  The operating system will favor pushingthis allocated, but unused memory into swap so that the memory can bere-used as a disk buffer.  When the java virtual machine decides toaccess this memory, the OS will begin flushing disk buffers to return thatmemory to the VM.  This can cause the entire process to block longenough for the zookeeper lock to be lost.Configure your system to reduce the kernel parameter swappiness from the default (60) to zero.My tablet server lost its lock, and I have already set swappiness to zero.  Why?Be careful not to over-subscribe memory.  This can be easy to do ifyour accumulo processes run on the same nodes as hadoop’s map-reduceframework.  Remember to add up:  size of the JVM for the tablet server  size of the in-memory map, if using the native map implementation  size of the JVM for the data node  size of the JVM for the task tracker  size of the JVM times the maximum number of mappers and reducers  size of the kernel and any support processesIf a 16G node can run 2 mappers and 2 reducers, and each can be 2G,then there is only 8G for the data node, tserver, task tracker and OS.Reduce the memory footprint of each component until it fits comfortably.My tablet server lost its lock, swappiness is zero, and my node has lots of unused memory!The JVM memory garbage collector may fall behind and cause a“stop-the-world” garbage collection. On a large memory virtualmachine, this collection can take a long time.  This happens morefrequently when the JVM is getting low on free memory.  Check the logsof the tablet server.  You will see lines like this:2013-06-20 13:43:20,607 [tabletserver.TabletServer] DEBUG: gc ParNew=0.00(+0.00) secs    ConcurrentMarkSweep=0.00(+0.00) secs freemem=1,868,325,952(+1,868,325,952) totalmem=2,040,135,680When freemem becomes small relative to the amount of memoryneeded, the JVM will spend more time finding free memory thanperforming work.  This can cause long delays in sending keep-alivemessages to zookeeper.Ensure the tablet server JVM is not running low on memory.I’m seeing errors in tablet server logs that include the words “MutationsRejectedException” and “# constraint violations: 1”. Moments after that the server died.The error you are seeing is part of a failing tablet server scenario.This is a bit complicated, so name two of your tablet servers A and B.Tablet server A is hosting a tablet, let’s call it a-tablet.Tablet server B is hosting a metadata tablet, let’s call it m-tablet.m-tablet records the information about a-tablet, for example, the names of the files it is using to store data.When A ingests some data, it eventually flushes the updates from memory to a file.Tablet server A then writes this new information to m-tablet, on Tablet server B.Here’s a likely failure scenario:Tablet server A does not have enough memory for all the processes running on it.The operating system sees a large chunk of the tablet server being unused, and swaps it out to disk to make room for other processes.Tablet server A does a java memory garbage collection, which causes it to start using all the memory allocated to it.As the server starts pulling data from swap, it runs very slowly.It fails to send the keep-alive messages to zookeeper in a timely fashion, and it looses its zookeeper session.But, it’s running so slowly, that it takes a moment to realize it should no longer be hosting tablets.The thread that is flushing a-tablet memory attempts to update m-tablet with the new file information.Fortunately there’s a constraint on m-tablet.Mutations to the metadata table must contain a valid zookeeper session.This prevents tablet server A from making updates to m-tablet when it no long has the right to host the tablet.The “MutationsRejectedException” error is from tablet server A making an update to tablet server B’s m-tablet.It’s getting a constraint violation: tablet server A has lost its zookeeper session, and will fail momentarily.Ensure that memory is not over-allocated.  Monitor swap usage, or turn swap off.My accumulo client is getting a MutationsRejectedException. The monitor is displaying “No Such SessionID” errors.When your client starts sending mutations to accumulo, it creates a session. Once the session is created,mutations are streamed to accumulo, without acknowledgement, against this session.  Once the client is done,it will close the session, and get an acknowledgement.If the client fails to communicate with accumulo, it will release the session, assuming that the client has died.If the client then attempts to send more mutations against the session, you will see “No Such SessionID” errors onthe server, and MutationRejectedExceptions in the client.The client library should be either actively using the connection to the tablet servers,or closing the connection and sessions. If the session times out, something is causing your clientto pause.The most frequent source of these pauses are java garbage collection pausesdue to the JVM running out of memory, or being swapped out to disk.Ensure your client has adequate memory and is not being swapped out to disk.HDFS FailuresI had disastrous HDFS failure.  After bringing everything back up, several tablets refuse to go online.Data written to tablets is written into memory before being written into indexed files.  In case the serveris lost before the data is saved into an indexed file, all data stored in memory is first written into awrite-ahead log (WAL).  When a tablet is re-assigned to a new tablet server, the write-ahead logs are read torecover any mutations that were in memory when the tablet was last hosted.If a write-ahead log cannot be read, then the tablet is not re-assigned.  All it takes is for one ofthe blocks in the write-ahead log to be missing.  This is unlikely unless multiple data nodes in HDFS have beenlost.Get the WAL files online and healthy.  Restore any data nodes that may be down.How do find out which tablets are offline?Use accumulo admin checkTablets$ accumulo admin checkTabletsI lost three data nodes, and I’m missing blocks in a WAL.  I don’t care about data loss, howcan I get those tablets online?See the system metadata table page which shows a typical metadata table listing.The entries with a column family of log are references to the WAL for that tablet.If you know what WAL is bad, you can find all the references with a grep in the shell:shell&amp;gt; grep 0cb7ce52-ac46-4bf7-ae1d-acdcfaa979953&amp;lt; log:127.0.0.1+9997/0cb7ce52-ac46-4bf7-ae1d-acdcfaa97995 []    127.0.0.1+9997/0cb7ce52-ac46-4bf7-ae1d-acdcfaa97995|6You can remove the WAL references in the metadata table.shell&amp;gt; grant -u root Table.WRITE -t accumulo.metadatashell&amp;gt; delete 3&amp;lt; log 127.0.0.1+9997/0cb7ce52-ac46-4bf7-ae1d-acdcfaa97995Note: the colon (:) is omitted when specifying the row cf cq for the delete command.The manager will automatically discover the tablet no longer has a bad WAL reference and willassign the tablet.  You will need to remove the reference from all the tablets to get themonline.The metadata (or root) table has references to a corrupt WAL.This is a much more serious state, since losing updates to the metadata table will resultin references to old files which may not exist, or lost references to new files, resultingin tablets that cannot be read, or large amounts of data loss.The best hope is to restore the WAL by fixing HDFS data nodes and bringing the data back online.If this is not possible, the best approach is to re-create the instance and bulk import all files fromthe old instance into a new tables.A complete set of instructions for doing this is outside the scope of this guide,but the basic approach is:  Use tables -l in the shell to discover the table name to table id mapping  Stop all accumulo processes on all nodes  Move the accumulo directory in HDFS out of the way:     $ hadoop fs -mv /accumulo /corrupt  Re-initialize accumulo  Recreate tables, users and permissions  Import the directories under /corrupt/tables/&amp;lt;id&amp;gt; into the new instanceOne or more HDFS Files under /accumulo/tables are corruptAccumulo maintains multiple references into the tablet files in the metadatatables and within the tablet server hosting the file, this makes it difficult toreliably just remove those references.The directory structure in HDFS for tables will follow the general structure:/accumulo/accumulo/tables//accumulo/tables/!0/accumulo/tables/!0/default_tablet/A000001.rf/accumulo/tables/!0/t-00001/A000002.rf/accumulo/tables/1/accumulo/tables/1/default_tablet/A000003.rf/accumulo/tables/1/t-00001/A000004.rf/accumulo/tables/1/t-00001/A000005.rf/accumulo/tables/2/default_tablet/A000006.rf/accumulo/tables/2/t-00001/A000007.rfIf files under /accumulo/tables are corrupt, the best course of action is torecover those files in hdfs see the section on HDFS. Once these recovery effortshave been exhausted, the next step depends on where the missing file(s) arelocated. Different actions are required when the bad files are in Accumulo datatable files or if they are metadata table files.Data File CorruptionWhen an Accumulo data file is corrupt, the most reliable way to restore Accumulooperations is to replace the missing file with an ``empty’’ file so thatreferences to the file in the METADATA table and within the tablet serverhosting the file can be resolved by Accumulo. An empty file can be created usingthe CreateEmpty utility:$ accumulo org.apache.accumulo.core.file.rfile.CreateEmpty /path/to/empty/file/empty.rfThe process is to delete the corrupt file and then move the empty file into itsplace (The generated empty file can be copied and used multiple times if necessary and does not needto be regenerated each time)$ hadoop fs –rm /accumulo/tables/corrupt/file/thename.rf; hadoop fs -mv /path/to/empty/file/empty.rf /accumulo/tables/corrupt/file/thename.rfMetadata File CorruptionIf the corrupt files are metadata files, read the system metadata tables(under the path /accumulo/tables/!0). Then, you will need to rebuildthe metadata table by initializing a new instance of Accumulo and then importingall of the existing data into the new instance.  This is the same procedure asrecovering from a zookeeper failure (see next section), except thatyou will have the benefit of having the existing user and table authorizationsthat are maintained in zookeeper.You can use the DumpZookeeper utility to save this information for referencebefore creating the new instance.  You will not be able to use RestoreZookeeperbecause the table names and references are likely to be different between theoriginal and the new instances, but it can serve as a reference.If the files cannot be recovered, replace corrupt data files with an emptyrfiles to allow references in the metadata table and in the tablet servers to beresolved. Rebuild the metadata table if the corrupt files are metadata files.Write-Ahead Log(WAL) File CorruptionIn certain versions of Accumulo, a corrupt WAL file (caused by HDFS corruptionor a bug in Accumulo that created the file) can block the successful recoveryof one to many Tablets. Accumulo can be stuck in a loop trying to recover theWAL file, never being able to succeed.In the cases where the WAL file’s original contents are unrecoverable or some degreeof data loss is acceptable (beware if the WAL file contains updates to the Accumulometadata table!), the following process can be followed to create a valid, emptyWAL file. Run the following commands as the Accumulo unix user (to ensure thatthe proper file permissions in HDFS)$ echo -n -e &#39;--- Log File Header (v2) ---x00x00x00x00&#39; &amp;gt; empty.walThe above creates a file with the text “— Log File Header (v2) —” and thenfour bytes. You should verify the contents of the file with a hexdump tool.Then, place this empty WAL in HDFS and then replace the corrupt WAL file in HDFSwith the empty WAL.$ hdfs dfs -moveFromLocal empty.wal /user/accumulo/empty.wal$ hdfs dfs -mv /user/accumulo/empty.wal /accumulo/wal/tserver-4.example.com+10011/26abec5b-63e7-40dd-9fa1-b8ad2436606eAfter the corrupt WAL file has been replaced, the system should automatically recover.It may be necessary to restart the Accumulo Manager process as an exponentialbackup policy is used which could lead to a long wait before Accumulo willtry to re-load the WAL file.Zookeeper FailuresI lost my ZooKeeper quorum (hardware failure), but HDFS is still intact. How can I recover my Accumulo instance?ZooKeeper, in addition to its lock-service capabilities, also serves to bootstrap an Accumuloinstance from some location in HDFS. It contains the pointers to the root tablet in HDFS whichis then used to load the Accumulo metadata tablets, which then loads all user tables. ZooKeeperalso stores all namespace and table configuration, the user database, the mapping of table IDs totable names, and more across Accumulo restarts.Presently, the only way to recover such an instance is to initialize a new instance and import allof the old data into the new instance. The easiest way to tackle this problem is to first recreatethe mapping of table ID to table name and then recreate each of those tables in the new instance.Set any necessary configuration on the new tables and add some split points to the tables to closethe gap between how many splits the old table had and no splits.The directory structure in HDFS for tables will follow the general structure:/accumulo/accumulo/tables//accumulo/tables/1/accumulo/tables/1/default_tablet/A000001.rf/accumulo/tables/1/t-00001/A000002.rf/accumulo/tables/1/t-00001/A000003.rf/accumulo/tables/2/default_tablet/A000004.rf/accumulo/tables/2/t-00001/A000005.rfFor each table, make a new directory that you can move (or copy if you have the HDFS space to do so)all of the rfiles for a given table into. For example, to process the table with an ID of 1, make a new directory,say /new-table-1 and then copy all files from /accumulo/tables/1/*/*.rf into that directory. Additionally,make a directory, /new-table-1-failures, for any failures during the import process. Then, issue the importcommand using the Accumulo shell into the new table, telling Accumulo to not re-set the timestamp:user@instance new_table&amp;gt; importdirectory /new-table-1 /new-table-1-failures falseAny RFiles which were failed to be loaded will be placed in /new-table-1-failures. Rfiles that were successfullyimported will no longer exist in /new-table-1. For failures, move them back to the import directory and retrythe importdirectory command.It is extremely important to note that this approach may introduce stale data back intothe tables. For a few reasons, RFiles may exist in the table directory which are candidates for deletion but havenot yet been deleted. Additionally, deleted data which was not compacted away, but still exists in write-ahead logs ifthe original instance was somehow recoverable, will be re-introduced in the new instance. Table splits and merges(which also include the deleteRows API call on TableOperations, are also vulnerable to this problem. This process shouldnot be used if these are unacceptable risks. It is possible to try to re-create a view of the accumulo.metadatatable to prune out files that are candidates for deletion, but this is a difficult task that also may not be entirely accurate.Likewise, it is also possible that data loss may occur from write-ahead log (WAL) files which existed on the old table butwere not minor-compacted into an RFile. Again, it may be possible to reconstruct the state of these WAL files toreplay data not yet in an RFile; however, this is a difficult task and is not implemented in any automated fashion.The importdirectory shell command can be used to import RFiles from the old instance into a newly created instance,but extreme care should go into the decision to do this as it may result in reintroduction of stale data or theomission of new data.Upgrade IssuesI upgraded from 1.4 to 1.5 to 1.6 but still have some WAL files on local disk. Do I have any way to recover them?Yes, you can recover them by running the LocalWALRecovery utility (not available in 1.8 and later) on each node that needs recovery performed. The utilitywill default to using the directory specified by logger.dir.walog in your configuration, or can beoverridden by using the --local-wal-directories option on the tool. It can be invoked as follows:accumulo org.apache.accumulo.tserver.log.LocalWALRecoveryI am trying to start the manager after upgrading but the upgrade is aborting with the following message:  org.apache.accumulo.core.client.AccumuloException: Aborting upgrade because there are outstanding FATE transactions from a previous Accumulo version.For versions 2.1 and later, you can use the admin fate command to delete completed FATE transactions using the following:  Start tservers  Start shell  Run accumulo admin fate --print to list all transactions  If the transactions have completed, just delete with them with accumulo admin fate --delete TXID [TXID...]  Start managers once there are no more fate operationsIf any of the operations are not complete, you should rollback the upgrade and troubleshoot completing them with your prior version.Prior to 2.1, the same steps apply, but the fate command is accessed with the Accumulo shell instead of the admin command.  The shell commands are as follows:use fate print to list transactionsuse fate delete to delete completed transactionsFile Naming ConventionsWhy are files named like they are? Why do some start with C and others with F?The file names give you a basic idea for the source of the file.The base of the filename is a base-36 unique number. All filenames in accumulo are coordinatedwith a counter in zookeeper, so they are always unique, which is useful for debugging.The leading letter gives you an idea of how the file was created:  F - Flush: entries in memory were written to a file (Minor Compaction)  M - Merging compaction: entries in memory were combined with the smallest file to create one new file  C - Several files, but not all files, were combined to produce this file (Major Compaction)  A - All files were compacted, delete entries were dropped  I - Bulk import, complete, sorted index files. Always in a directory starting with b-This simple file naming convention allows you to see the basic structure of the files from justtheir filenames, and reason about what should be happening to them next, justby scanning their entries in the metadata tables.For example, if you see multiple files with M prefixes, the tablet is, or was, up against itsmaximum file limit, so it began merging memory updates with files to keep the file count reasonable.  Thisslows down ingest performance, so knowing there are many files like this tells you that the systemis struggling to keep up with ingest vs the compaction strategy which reduces the number of files.HDFS Decommissioning IssuesMy Hadoop DataNode is hung for hours trying to decommission.Write Ahead Logs stay open until they hit the size threshold, which could be many hours or days in some cases. These open files will prevent a DN from finishing its decommissioning process (HDFS-3599) in some versions of Hadoop 2. If you stop the DN, then the WALog file will not be closed and you could lose data. To work around this issue, we now close WALogs on a time period specified by the property tserver.walog.max.age with a default period of 24 hours.",
      "url": " /docs/2.x/troubleshooting/advanced",
      "categories": "troubleshooting"
    },
  
    "docs-2-x-troubleshooting-basic": {
      "title": "Basic Troubleshooting",
      "content": "GeneralThe tablet server does not seem to be running!? What happened?Accumulo is a distributed system.  It is supposed to run on remoteequipment, across hundreds of computers.  Each program that runs onthese remote computers writes down events as they occur, into a localfile. By default, this is defined in conf/accumulo-env.sh as ACCUMULO_LOG_DIR.Look in the $ACCUMULO_LOG_DIR/tserver*.log file.  Specifically, check the end of the file.The tablet server did not start and the debug log does not exist!  What happened?When the individual programs are started, the stdout and stderr outputof these programs are stored in .out and .err files in$ACCUMULO_LOG_DIR.  Often, when there are missing configurationoptions, files or permissions, messages will be left in these files.Probably a start-up problem.  Look in $ACCUMULO_LOG_DIR/tserver*.errAccumulo is not working, what’s wrong?There’s a small web server that collects information about all thecomponents that make up a running Accumulo instance. It will highlightunusual or unexpected conditions.Point your browser to the monitor (typically the manager host, on port 9995).  Is anything red or yellow?My browser is reporting connection refused, and I cannot get to the monitorThe monitor program’s output is also written to .err and .out files inthe $ACCUMULO_LOG_DIR. Look for problems in this file if the$ACCUMULO_LOG_DIR/monitor*.log file does not exist.The monitor program is probably not running.  Check the log files for errors.My browser hangs trying to talk to the monitor.Your browser needs to be able to reach the monitor program.  Oftenlarge clusters are firewalled, or use a VPN for internalcommunications. You can use SSH to proxy your browser to the cluster,or consult with your system administrator to gain access to the serverfrom your browser.It is sometimes helpful to use a text-only browser to sanity-check themonitor while on the machine running the monitor:$ links http://localhost:9995Verify that you are not firewalled from the monitor if it is running on a remote host.The monitor responds, but there are no numbers for tservers and tables.  The summary page says the manager is down.The monitor program gathers all the details about the manager and thetablet servers through the manager. It will be mostly blank if themanager is down. Check for a running manager.The ZooKeeper information is not available on the Overview page.The monitor uses the ZooKeeper stat four-letter-word command to retrieve information.The ZooKeeper configuration may require explicitly listing the stat command in the four-letter-word whitelist.I’ve lost the Accumulo root password, now what?Running accumulo init --reset-security will prompt you for a new root password. CAUTION: this command will delete all existing users. You will need to re-create all other users and set permissions accordingly. Running the accumulo admin dumpConfig command will output current configuration, including current users, which may aid in this process.Accumulo ProcessesMy tablet server crashed!  The logs say that it lost its zookeeper lock.Tablet servers reserve a lock in zookeeper to maintain their ownershipover the tablets that have been assigned to them.  Part of theirresponsibility for keeping the lock is to send zookeeper a keep-alivemessage periodically.  If the tablet server fails to send a message ina timely fashion, zookeeper will remove the lock and notify the tabletserver.  If the tablet server does not receive a message fromzookeeper, it will assume its lock has been lost, too.  If a tabletserver loses its lock, it kills itself: everything assumes it is deadalready.Investigate why the tablet server did not send a timely message tozookeeper.I need to decommission a node.  How do I stop the tablet server on it?Use the admin command:$ accumulo admin stop hostname:99972013-07-16 13:15:38,403 [util.Admin] INFO : Stopping server 12.34.56.78:9997I cannot login to a tablet server host, and the tablet server will not shut down.  How can I kill the server?Sometimes you can kill a “stuck” tablet server by deleting its lock in zookeeper:$ accumulo org.apache.accumulo.server.util.TabletServerLocks --list                  127.0.0.1:9997 TSERV_CLIENT=127.0.0.1:9997$ accumulo org.apache.accumulo.server.util.TabletServerLocks -delete 127.0.0.1:9997$ accumulo org.apache.accumulo.server.util.TabletServerLocks -list                  127.0.0.1:9997             nullYou can find the manager and instance id for any accumulo instances using the same zookeeper instance:$ accumulo org.apache.accumulo.server.util.ListInstancesINFO : Using ZooKeepers localhost:2181 Instance Name       | Instance ID                          | Manager---------------------+--------------------------------------+-------------------------------              &quot;test&quot; | 6140b72e-edd8-4126-b2f5-e74a8bbe323b |                127.0.0.1:9999One of my Accumulo processes died. How do I bring it back?The easiest way to bring all services online for an Accumulo instance is to run the accumulo-cluster script.$ accumulo-cluster startThis process will check the process listing, using jps on each host before attempting to restart a service on the given host.Typically, this check is sufficient except in the face of a hung/zombie process. For large clusters, it may beundesirable to ssh to every node in the cluster to ensure that all hosts are running the appropriate processes and accumulo-service may be of use.$ ssh host_with_dead_process$ accumulo-service tserver startMy process died again. Should I restart it via cron or tools like supervisord?A repeatedly dying Accumulo process is a sign of a larger problem. Typically, these problems are due to amisconfiguration of Accumulo or over-saturation of resources. Blind automation of any service restart inside of Accumulois generally an undesirable situation as it is indicative of a problem that is being masked and ignored. Accumuloprocesses should be stable on the order of months and not require frequent restart.Accumulo ClientsAccumulo is not showing me any data!Is your client configured with authorizations that match your visibilities?  See theAuthorizations documentation for help.What are my visibilities?Use the rfile-info tool on a representative file to get some ideaof the visibilities in the underlying data.Note that the use of rfile-info is an administrative tool and can onlybe used by someone who can access the underlying Accumulo data. Itdoes not provide the normal access controls in Accumulo.IngestWhy does my ingest rate periodically go down during heavy ingest?Periods of zero or low ingest rates can be caused by Java garbage collection pauses in tablet servers. This problemcan be mitigated by enabling native maps in tablet servers.HDFSAccumulo reads and writes to the Hadoop Distributed File System.Accumulo needs this file system available at all times for normal operations.Accumulo is having problems “getting a block blk_1234567890123”. How do I fix it?This troubleshooting guide does not cover HDFS, but in general, youwant to make sure that all the datanodes are running and an fsck checkfinds the file system clean:$ hadoop fsck /accumuloYou can use:$ hadoop fsck /accumulo/path/to/corrupt/file -locations -blocks -filesto locate the block references of individual corrupt files and use thosereferences to search the name node and individual data node logs to determine whichservers those blocks have been assigned and then try to fix any underlying filesystem issues on those nodes.On a larger cluster, you may need to increase the number of Xcievers for HDFS DataNodes:&amp;lt;property&amp;gt;    &amp;lt;name&amp;gt;dfs.datanode.max.xcievers&amp;lt;/name&amp;gt;    &amp;lt;value&amp;gt;4096&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;Verify HDFS is healthy, check the datanode logs.ZookeeperThe accumulo init command is hanging. It says something about talking to zookeeper.Zookeeper is also a distributed service.  You will need to ensure thatit is up.  You can run the zookeeper command line tool to connect toany one of the zookeeper servers:$ zkCli.sh -server zoohost...[zk: zoohost:2181(CONNECTED) 0]It is important to see the word CONNECTED!  If you only seeCONNECTING you will need to diagnose zookeeper errors.Check to make sure that zookeeper is up, and thataccumulo.properties has been pointed toyour zookeeper server(s).Zookeeper is running, but it does not say CONNECTEDZookeeper processes talk to each other to elect a leader.  All updatesgo through the leader and propagate to a majority of all the othernodes.  If a majority of the nodes cannot be reached, zookeeper willnot allow updates.  Zookeeper also limits the number connections to aserver from any other single host.  By default, this limit can be as small as 10and can be reached in some everything-on-one-machine test configurations.You can check the election status and connection status of clients byasking the zookeeper nodes for their status.  You connect to zookeeperand ask it with the four-letter stat command:$ nc zoohost 2181statZookeeper version: 3.4.5-1392090, built on 09/30/2012 17:52 GMTClients: /127.0.0.1:58289[0](queued=0,recved=1,sent=0) /127.0.0.1:60231[1](queued=0,recved=53910,sent=53915)Latency min/avg/max: 0/5/3008Received: 1561459Sent: 1561592Connections: 2Outstanding: 0Zxid: 0x621a3bMode: standaloneNode count: 22524Check zookeeper status, verify that it has a quorum, and has not exceeded maxClientCnxns.",
      "url": " /docs/2.x/troubleshooting/basic",
      "categories": "troubleshooting"
    },
  
    "docs-2-x-troubleshooting-performance": {
      "title": "Performance",
      "content": "Accumulo can be tuned to improve read and write performance.Read performance      Enable caching on tables to reduce reads to disk.        Enable bloom filters on tables to limit the number of disk lookups.        Decrease the major compaction ratio of a table to decrease the number offiles per tablet. Less files reduces the latency of reads.        Decrease the size of data blocks in RFiles by lowering table.file.compress.blocksize which can resultin better random seek performance. However, this can increase the size of indexes in the RFile. If the indexesare too large to fit in cache, this can hinder performance. Also, as the index size increases the depth of theindex tree in each file may increase. Increasing table.file.compress.blocksize.index can reduce the depth ofthe tree.  Write performance      Enable native maps on tablet servers to prevent Java garbage collection pauseswhich can slow ingest.        Pre-split new tables to distribute writes across multiple tablet servers.        Ingest data using multiple clients or bulk ingest to increase ingest throughput.        Increase the major compaction ratio of a table to limit the number of major compactionswhich improves ingest performance.        On large Accumulo clusters, use multiple HDFS volumes to increase write performance.        Change the compression format used by blocks in RFiles by setting table.file.compress.type tosnappy. This increases write speed at the expense of using more disk space.  ",
      "url": " /docs/2.x/troubleshooting/performance",
      "categories": "troubleshooting"
    },
  
    "docs-2-x-troubleshooting-system-metadata-tables": {
      "title": "System Metadata Tables",
      "content": "Accumulo tracks information about tables in metadata tables. The metadata formost tables is contained within the metadata table in the accumulo namespace,while metadata for that table is contained in the root table in the accumulonamespace. The root table is composed of a single tablet, which does notsplit, so it is also called the root tablet. Information about the roottable, such as its location and write-ahead logs, are stored in ZooKeeper.Let’s create a table and put some data into it:shell&amp;gt; createtable testshell&amp;gt; tables -laccumulo.metadata    =&amp;gt;        !0accumulo.root        =&amp;gt;        `rtest                 =&amp;gt;         2trace                =&amp;gt;         1shell&amp;gt; insert a b c dshell&amp;gt; flush -wNow let’s take a look at the metadata for this table:shell&amp;gt; table accumulo.metadatashell&amp;gt; scan -b 3; -e 3&amp;lt;3&amp;lt; file:/default_tablet/F000009y.rf []    186,13&amp;lt; last:13fe86cd27101e5 []    127.0.0.1:99973&amp;lt; loc:13fe86cd27101e5 []    127.0.0.1:99973&amp;lt; srv:dir []    /default_tablet3&amp;lt; srv:flush []    13&amp;lt; srv:lock []    tservers/127.0.0.1:9997/zlock-0000000001$13fe86cd27101e53&amp;lt; srv:time []    M13739983923233&amp;lt; ~tab:~pr []    x00Let’s decode this little session:      scan -b 3; -e 3&amp;lt; -   Every tablet gets its own row. Every row starts with the table id followed by  ; or &amp;lt;, and followed by the end row split point for that tablet.        file:/default_tablet/F000009y.rf [] 186,1 -  File entry for this tablet.  This tablet contains a single file reference. The  file is /accumulo/tables/3/default_tablet/F000009y.rf.  It contains 1  key/value pair, and is 186 bytes long.        last:13fe86cd27101e5 []    127.0.0.1:9997 -  The last location data was written locally (from a minor or major compaction). Data was last written on 127.0.0.1:9997, and the  unique tablet server lock data was 13fe86cd27101e5. The default balancer  will tend to assign tablets to the last location where its files have been written to  attempt to improve data locality.        loc:13fe86cd27101e5 []    127.0.0.1:9997 -  Last assigned location for this tablet (its current location, if it is still hosted). It was last assigned to 127.0.0.1:9997, and the  unique tablet server lock data was 13fe86cd27101e5.        srv:dir []    /default_tablet -  Files written for this tablet will be placed into  /accumulo/tables/3/default_tablet.        srv:flush []    1 -  Flush id.  This table has successfully completed the flush with the id of 1.        srv:lock []    tservers/127.0.0.1:9997/zlock-0000000001$13fe86cd27101e5 -  This is the lock information for the tablet holding the present lock.  This  information is checked against zookeeper whenever this is updated, which  prevents a metadata update from a tablet server that no longer holds its  lock.        srv:time []    M1373998392323 -  This indicates the time type (M for milliseconds or L for logical) and the timestamp of the most recently written key in this tablet.  It is used to ensure automatically assigned key timestamps are strictly increasing for the tablet, regardless of the tablet server’s system time.        ~tab:~pr []    x00 -  The end-row marker for the previous tablet (prev-row).  The first byte  indicates the presence of a prev-row.  This tablet has the range (-inf, `inf),  so it has no prev-row (or end row).  Besides these columns, you may see:      rowId future:zooKeeperID location -  Tablet has been assigned to a tablet, but not yet loaded.        ~del:filename -  When a tablet server is done use a file, it will create a delete marker in the appropriate metadata table, unassociated with any tablet.  The garbage collector will remove the marker, and the file, when no other reference to the file exists.        ~blip:txid -  Bulk-Load In Progress marker.        rowId loaded:filename -  A file has been bulk-loaded into this tablet, however the bulk load has not yet completed on other tablets, so this marker prevents the file from being loaded multiple times.        rowId !cloned -  A marker that indicates that this tablet has been successfully cloned.        rowId splitRatio:ratio -  A marker that indicates a split is in progress, and the files are being split at the given ratio.        rowId chopped -  A marker that indicates that the files in the tablet do not contain keys outside the range of the tablet.        rowId scan -  A marker that prevents a file from being removed while there are still active scans using it.  ",
      "url": " /docs/2.x/troubleshooting/system-metadata-tables",
      "categories": "troubleshooting"
    },
  
    "docs-2-x-troubleshooting-tools": {
      "title": "Troubleshooting Tools",
      "content": "The accumulo command can be used to run various tools and classes from the command line.RFileInfoThe rfile-info tool will examine an Accumulo storage file and print out basic metadata.$ accumulo rfile-info /accumulo/tables/1/default_tablet/A000000n.rf2013-07-16 08:17:14,778 [util.NativeCodeLoader] INFO : Loaded the native-hadoop libraryLocality group         : &amp;lt;DEFAULT&amp;gt;        Start block          : 0        Num   blocks         : 1        Index level 0        : 62 bytes  1 blocks        First key            : 288be9ab4052fe9e span:34078a86a723e5d3:3da450f02108ced5 [] 1373373521623 false        Last key             : start:13fc375709e id:615f5ee2dd822d7a [] 1373373821660 false        Num entries          : 466        Column families      : [waitForCommits, start, md major compactor 1, md major compactor 2, md major compactor 3,                                 bringOnline, prep, md major compactor 4, md major compactor 5, md root major compactor 3,                                 minorCompaction, wal, compactFiles, md root major compactor 4, md root major compactor 1,                                 md root major compactor 2, compact, id, client:update, span, update, commit, write,                                 majorCompaction]Meta block     : BCFile.index      Raw size             : 4 bytes      Compressed size      : 12 bytes      Compression type     : gzMeta block     : RFile.index      Raw size             : 780 bytes      Compressed size      : 344 bytes      Compression type     : gzWhen trying to diagnose problems related to key size, the rfile-info tool can provide a histogram of the individual key sizes:$ accumulo rfile-info --histogram /accumulo/tables/1/default_tablet/A000000n.rf...Up to size      count      %-age         10 :        222  28.23%        100 :        244  71.77%       1000 :          0   0.00%      10000 :          0   0.00%     100000 :          0   0.00%    1000000 :          0   0.00%   10000000 :          0   0.00%  100000000 :          0   0.00% 1000000000 :          0   0.00%10000000000 :          0   0.00%Likewise, rfile-info will dump the key-value pairs and show you the contents of the RFile:$ accumulo rfile-info --dump /accumulo/tables/1/default_tablet/A000000n.rfrow columnFamily:columnQualifier [visibility] timestamp deleteFlag -&amp;gt; Value...Encrypted FilesTo examine an encrypted rfile the necessary encryption properties must be provided to the utility. To do thisthe accumulo.properties file can be copied, the necessary encryption parameters added, and then the properties file canbe passed to the utility with the -p argument.For example, if using PerTableCryptoServiceFactory and the AESCryptoService, you would need the following properties inyour accumulo.properties file:general.custom.crypto.key.uri=&amp;lt;path-to-key&amp;gt;/data-encryption.keyinstance.crypto.opts.factory=org.apache.accumulo.core.spi.crypto.PerTableCryptoServiceFactorytable.crypto.opts.service=org.apache.accumulo.core.spi.crypto.AESCryptoServiceExample output:$ accumulo rfile-info hdfs://localhost:8020/accumulo/tables/1/default_tablet/F0000001.rf -p &amp;lt;path-to-properties&amp;gt;/accumulo.propertiesReading file: hdfs://localhost:8020/accumulo/tables/1/default_tablet/F0000001.rfEncrypted with Params: ......RFile Version            : 8Locality group           : &amp;lt;DEFAULT&amp;gt;      Num   blocks           : 1      Index level 0          : 37 bytes  1 blocks      ...Meta block     : BCFile.index      Raw size             : 4 bytes      ...Meta block     : RFile.index      Raw size             : 121 bytes      ......GetManagerStatsThe GetManagerStats tool can be used to retrieve Accumulo state and statistics:$ accumulo org.apache.accumulo.test.GetManagerStats | grep Load OS Load Average: 0.27FindOfflineTabletsIf the Accumulo monitor shows an offline tablet, use FindOfflineTablets to find out whichtablet it is.$ accumulo org.apache.accumulo.server.util.FindOfflineTablets2&amp;lt;&amp;lt;@(null,null,localhost:9997) is UNASSIGNED  #walogs:2Here’s what the output means:      2&amp;lt;&amp;lt; -  This is the tablet from (-inf, pass:[+]inf) for the  table with id 2.  The command tables -l in the shell will show table ids for  tables.        @(null, null, localhost:9997) -  Location information.  The  format is @(assigned, hosted, last).  In this case, the  tablet has not been assigned, is not hosted anywhere, and was once  hosted on localhost.        #walogs:2 -   The number of write-ahead logs that this tablet requires for recovery.  An unassigned tablet with write-ahead logs is probably waiting forlogs to be sorted for efficient recovery.CheckForMetadataProblemsThe CheckForMetadataProblems tool can be used to make sure metadatatables are up and consistent. It will verify the start/end ofevery tablet matches, and the start and stop for the table is empty:$ accumulo org.apache.accumulo.server.util.CheckForMetadataProblems -u root --passwordEnter the connection password:Checking tables whose metadata is found in: accumulo.root (+r)...All is well for table accumulo.metadata (!0)No problems found in accumulo.root (+r)Checking tables whose metadata is found in: accumulo.metadata (!0)...All is well for table accumulo.replication (+rep)...All is well for table trace (1)No problems found in accumulo.metadata (!0)RemoveEntriesForMissingFilesIf your Hadoop cluster has a lost a file due to a NameNode failure, you can removethe file reference using RemoveEntriesForMissingFiles. It will check every file referenceand ensure that the file exists in HDFS.  Optionally, it will remove the reference:$ accumulo org.apache.accumulo.server.util.RemoveEntriesForMissingFiles -u root --passwordEnter the connection password:2013-07-16 13:10:57,293 [util.RemoveEntriesForMissingFiles] INFO : File /accumulo/tables/2/default_tablet/F0000005.rf is missing2013-07-16 13:10:57,296 [util.RemoveEntriesForMissingFiles] INFO : 1 files of 3 missingChangeSecret (new in 2.1)Changes the unique secret given to the instance that all servers must know. The utility can be run using the accumulo admin command.Note that Accumulo must be shut down to run this utility.$ accumulo admin changeSecretOld secret:New secret:New instance id is 6e7f416b-c578-45df-8016-c9bc6b400e13Be sure to put your new secret in accumulo.propertiesDeleteZooInstance (new in 2.1)Deletes specific a specific instance name or id from zookeeper or cleans up all old instances. The utility can be run using the accumulo admin command.To delete a specific instance use -i or --instance flags.$ accumulo admin deleteZooInstance -i instance1Deleted instance: instance1If you try to delete the current instance a warning prompt will be displayed.$ accumulo admin deleteZooInstance -i unoWarning: This is the current instance, are you sure? (yes|no): noInstance deletion of &#39;uno&#39; cancelled.$ accumulo admin deleteZooInstance -i unoWarning: This is the current instance, are you sure? (yes|no): yesDeleted instance: instance1If you have entries in zookeeper for old instances that you no longer need, use the -c or --clean flags.This command will not delete the instance pointed to by the local accumulo.properties file.$ accumulo admin deleteZooInstance -cDeleted instance: instance1Deleted instance: instance2accumulo-util dump-zooTo view the contents of ZooKeeper, run the following command:$ accumulo-util dump-zooIt can also be run using the accumulo command.$ accumulo dump-zooIf you would like to backup ZooKeeper, run the following command to write its contents as XML to file.$ accumulo-util dump-zoo --xml --root /accumulo &amp;gt;dump.xmlRestoreZookeeperAn XML dump file can be later used to restore ZooKeeper. The utility can be run using the accumulo admin command.$ accumulo admin restoreZoo --overwrite &amp;lt; dump.xmlThis command overwrites ZooKeeper so take care when using it. This is also why it cannot be called using accumulo-util.TabletServerLocks (new in 2.1)List or delete Tablet Server locks. The utility can be run using the accumulo admin command.$ accumulo admin locks    localhost:9997 TSERV_CLIENT=localhost:9997$ accumulo admin locks -delete localhost:9997$ accumulo admin locks    localhost:9997             &amp;lt;none&amp;gt;VerifyTabletAssignments (new in 2.1)Verify all tablets are assigned to tablet servers. The utility can be run using the accumulo admin command.$ accumulo admin verifyTabletAssignsChecking table accumulo.metadataChecking table accumulo.replicationTablet +rep&amp;lt;&amp;lt; has no locationChecking table accumulo.rootChecking table t1Checking table t2Checking table t3$ accumulo admin verifyTabletAssigns -vChecking table accumulo.metadataTablet !0;~&amp;lt; is located at localhost:9997Tablet !0&amp;lt;;~ is located at localhost:9997Checking table accumulo.replicationTablet +rep&amp;lt;&amp;lt; has no locationChecking table accumulo.rootTablet +r&amp;lt;&amp;lt; is located at localhost:9997Checking table t1Tablet 1&amp;lt;&amp;lt; is located at localhost:9997Checking table t2Tablet 2&amp;lt;&amp;lt; is located at localhost:9997Checking table t3Tablet 3&amp;lt;&amp;lt; is located at localhost:9997zoo-info-viewer (new in 2.1)View Accumulo information stored in ZooKeeper in a human-readable format.  The utility can be run without an Accumuloinstance. If an instance id or name is not provided on the command line, the instance will be read fromHDFS, otherwise only a running ZooKeeper instance is required to run the command.To run the command:$ accumulo zoo-info-viewer [mode-options] [--outfile filename]mode options:--print-instances--print-id-map--print-props [--system] [-ns | --namespaces list] [-t | --tables list]--print-aclsmode: print instancesThe instance name(s) and instance id(s) are stored in ZooKeeper. To see the available name to id mapping run:$ accumulo zoo-info-viewer --print-instances-----------------------------------------------Report Time: 2022-05-31T21:07:19.673258Z-----------------------------------------------Instances (Instance Name, Instance ID)test_a=1111465d-b7bb-42c2-919b-111111111111test_b=2222465d-b7bb-42c2-919b-222222222222uno=9cc9465d-b7bb-42c2-919b-ddf74b610c82-----------------------------------------------mode: print id-mapIf a shell is not available or convenient, the zoo-info-viewer can provide the sameinformation as the namespaces -l and tables -l commands. Note, the zoo-info-viewer output issorted by the id.$ accumulo zoo-info-viewer --print-id-map-----------------------------------------------Report Time: 2022-05-25T19:33:42.079969Z-----------------------------------------------ID Mapping (id =&amp;gt; name) for instance: 8f006afd-8673-4a5a-b940-60405755197fNamespace ids:+accumulo =&amp;gt;                 accumulo+default  =&amp;gt;                       &quot;&quot;1         =&amp;gt;               ns_sample1Table ids:!0        =&amp;gt;        accumulo.metadata+r        =&amp;gt;            accumulo.root+rep      =&amp;gt;     accumulo.replication2         =&amp;gt;          ns_sample1.tbl13         =&amp;gt;                     tbl2-----------------------------------------------mode: print property mappingsWith Accumulo version 2.1, the storage of properties in ZooKeeper has changed and the properties are not directlyreadable with the ZooKeeper zkCli utility.  The properties can be listed in an Accumulo shell with the config command.However, if a shell is not available, this utility zoo-info-viewer can be used instead.The zoo-info-viewer option --print-props with no other options will print all the configuration propertiesfor system, namespaces and tables.  The print-props can be filtered the with additional options, --system will printthe system configuration, -ns or --namespaces expects a list of the namespace names,-t or --tables expects a list of table names included in the output.$ accumulo zoo-info-viewer  --print-props-----------------------------------------------Report Time: 2022-05-31T21:18:11.562867Z-----------------------------------------------ZooKeeper properties for instance ID: 9cc9465d-b7bb-42c2-919b-ddf74b610c82Name: System, Data Version:0, Data Timestamp: 2022-05-31T15:51:52.772265Z:-- none --Namespace:Name: , Data Version:0, Data Timestamp: 2022-05-31T15:51:53.015613Z:-- none --Name: accumulo, Data Version:0, Data Timestamp: 2022-05-31T15:51:53.034172Z:-- none --Name: ns1, Data Version:0, Data Timestamp: 2022-05-31T21:17:22.927165Z:-- none --Tables:Name: accumulo.metadata, Data Version:2, Data Timestamp: 2022-05-31T15:51:53.511811Z:table.cache.block.enable=truetable.cache.index.enable=true...Name: accumulo.replication, Data Version:1, Data Timestamp: 2022-05-31T15:51:53.516346Z:table.formatter=org.apache.accumulo.server.replication.StatusFormattertable.group.repl=repl...Name: accumulo.root, Data Version:2, Data Timestamp: 2022-05-31T15:51:53.501174Z:table.cache.block.enable=truetable.cache.index.enable=true...Name: ns1.tbl1, Data Version:1, Data Timestamp: 2022-05-31T21:17:41.111836Z:table.constraint.1=org.apache.accumulo.core.data.constraints.DefaultKeySizeConstrainttable.iterator.majc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator...Name: tbl3, Data Version:1, Data Timestamp: 2022-05-31T21:17:54.083044Z:table.constraint.1=org.apache.accumulo.core.data.constraints.DefaultKeySizeConstrainttable.iterator.majc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIterator...-----------------------------------------------mode: print ACLs (new in 2.1.1)With 2.1.1, the zoo-info-viewer option --print-acls will print the ZooKeeper ACLs for all nodes underthe /accumulo/INSTANCE_ID] path.See troubleshooting ZooKeeper for more information on the tool output and expected ACLs.$ accumulo zoo-info-viewer --print-acls-----------------------------------------------Report Time: 2023-01-27T23:00:26.079546Z-----------------------------------------------Output format:ACCUMULO_PERM:OTHER_PERM path user_acls...ZooKeeper acls for instance ID: f491223b-1413-494e-b75a-c2ca018db00fACCUMULO_OKAY:NOT_PRIVATE /accumulo/f491223b-1413-494e-b75a-c2ca018db00f cdrwa:accumulo, r:anyoneACCUMULO_OKAY:NOT_PRIVATE /accumulo/f491223b-1413-494e-b75a-c2ca018db00f/bulk_failed_copyq cdrwa:accumulo, r:anyoneACCUMULO_OKAY:NOT_PRIVATE /accumulo/f491223b-1413-494e-b75a-c2ca018db00f/bulk_failed_copyq/locks cdrwa:accumulo, r:anyoneACCUMULO_OKAY:NOT_PRIVATE /accumulo/f491223b-1413-494e-b75a-c2ca018db00f/compactors cdrwa:accumulo, r:anyoneACCUMULO_OKAY:PRIVATE /accumulo/f491223b-1413-494e-b75a-c2ca018db00f/config cdrwa:accumuloACCUMULO_OKAY:NOT_PRIVATE /accumulo/f491223b-1413-494e-b75a-c2ca018db00f/coordinators cdrwa:accumulo, r:anyone...ERROR_ACCUMULO_MISSING_SOME:NOT_PRIVATE /accumulo/f491223b-1413-494e-b75a-c2ca018db00f/users/root/Namespaces r:accumulo, r:anyone...ACCUMULO_OKAY:NOT_PRIVATE /accumulo/f491223b-1413-494e-b75a-c2ca018db00f/wals/localhost:9997[100003d35cc0004]/643b14db-b929-4570-b226-620bc5ac85ff cdrwa:accumulo, r:anyoneACCUMULO_OKAY:NOT_PRIVATE /accumulo/f491223b-1413-494e-b75a-c2ca018db00f/wals/localhost:9997[100003d35cc0004]/ad26be2a-dc52-4e0e-8e78-8fc8c3323d51 cdrwa:accumulo, r:anyoneACCUMULO_OKAY:NOT_PRIVATE /accumulo/instances cdrwa:anyoneACCUMULO_OKAY:NOT_PRIVATE /accumulo/instances/uno cdrwa:accumulo, r:anyonezoo-prop-editor (new in 2.1.1)The zoo-prop-editor tool provides an emergency capability to update properties stored in ZooKeeper without arunning Accumulo instance. Only ZooKeeper and Hadoop are required to be available to use the tool.  With Accumulo 2.1, properties are stored in single ZooKeeper config nodes for the system, each namespace andeach table. The properties are stored compressed and cannot be directly edited using the ZooKeeper command lines tools like zkCli.shThis tool is provided for a situation if invalid properties were set by the user that prevent the instance from running or if running the instance would lead to an unacceptable outcome. Users should prefer using the Accumulo shell to edit properties if at all possible. Alternatively, properties can be also be viewed using the zoo-info-viewer (it also does not need a running Accumulo instance).The zoo-prop-editor follows a similar command format of the shell config command. If a namespace or table is not specified, the tool assumes the system properties. If set or delete option is not provided, the tool prints the current properties.The tool displays only the properties stored in a single ZooKeeper config node. It does not provide the property hierarchy (default -&amp;gt; system -&amp;gt; namespace -&amp;gt; table) that is available with the shell config command.The output includes property metadata that is prefixed with : to support filtering with grep -v : to suppress those lines if desired when piping the output to follow on commands.For example, to view the current system config node properties (no properties are set in this example)$ accumulo zoo-prop-editor: Instance name: uno: Instance Id: e715caf8-f576-4b5d-871a-d47add90b7ba: Property scope: SYSTEM: ZooKeeper path: /accumulo/e715caf8-f576-4b5d-871a-d47add90b7ba/config: Name: system: Id: e715caf8-f576-4b5d-871a-d47add90b7ba: Data version: 0: Timestamp: 2023-06-12T21:52:15.727028ZFor example, to view the properties for table ns1.tbl1$ accumulo zoo-prop-editor -t ns1.tbl1: Instance name: uno: Instance Id: e715caf8-f576-4b5d-871a-d47add90b7ba: Property scope: TABLE: ZooKeeper path: /accumulo/e715caf8-f576-4b5d-871a-d47add90b7ba/tables/2/config: Name: ns1.tbl1: Id: 2: Data version: 1: Timestamp: 2023-06-12T21:54:31.817473Ztable.constraint.1=org.apache.accumulo.core.data.constraints.DefaultKeySizeConstrainttable.iterator.majc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIteratortable.iterator.majc.vers.opt.maxVersions=1table.iterator.minc.vers=20,org.apache.accumulo.core.iterators.user.VersioningIteratortable.iterator.minc.vers.opt.maxVersions=1table.iterator.scan.vers=20,org.apache.accumulo.core.iterators.user.VersioningIteratortable.iterator.scan.vers.opt.maxVersions=1To set a property, use the -s or --set option. For example:$ zoo-prop-editor -t ns1.tbl1 -s table.bloom.enabled=falseTo delete a property, use the -d or --delete option. For example:$ zoo-prop-editor -t ns1.tbl1 -d table.bloom.enabled",
      "url": " /docs/2.x/troubleshooting/tools",
      "categories": "troubleshooting"
    },
  
    "docs-2-x-troubleshooting-tracing": {
      "title": "Tracing",
      "content": "It can be difficult to determine why some operations are taking longerthan expected. For example, you may be looking up items with very lowlatency, but sometimes the lookups take much longer. Determining thecause of the delay is difficult because the system is distributed, andthe typical lookup is fast.Accumulo has been instrumented to record the time that variousoperations take when tracing is turned on. The fact that tracing isenabled follows all the requests made on behalf of the user throughoutthe distributed infrastructure of accumulo, and across all threads ofexecution.Starting with version 2.1.0, Accumulo uses OpenTelemetry to collectand transport trace information to a back-end server that can displaythe trace information. The Accumulo Trace server process and trace tableare no longer used. Also, the old trace configuration properties are nowdeprecated and not used.Configuring TracingTo collect traces, Accumulo needs two items:  general.opentelemetry.enabled must be set to true  The io.opentelemetry.api.GlobalOpenTelemetry.globalOpenTelemetry member variable must be set.One way to set the globalOpenTelemetry member variable is to use theOpenTelemetry Java Agent. Simply putting the agent jar on the classpathand configuring that agent.Hadoop is also working on using OpenTelemetry. This is being tracked athttps://issues.apache.org/jira/browse/HADOOP-15566.Instrumenting a ClientAccumulo client operations will be traced as part of a client applicationoperation if the client application is also instrumented using OpenTelemetryand invokes the Accumulo client operation in a Span. Client applicationdevelopers can use the OpenTelemetry documentation to instrument theapplication. To collect traces in the client, Accumulo needs theio.opentelemetry.api.GlobalOpenTelemetry.globalOpenTelemetry membervariable set to an OpenTelemetry instance.Tracing from the ShellYou can enable tracing for operations run from the shell by using thetrace on and trace off commands.",
      "url": " /docs/2.x/troubleshooting/tracing",
      "categories": "troubleshooting"
    },
  
    "docs-2-x-troubleshooting-zookeeper": {
      "title": "ZooKeeper",
      "content": "ZooKeeper ACLsAccumulo requires full access to nodes in ZooKeeper under the /accumulo path.  The ACLs can be examined using theZooKeeper cli getAcl and modified with setAcl commands.  With 2.1.1, the zoo-info-viewer utility has an optionthat will print all of the ACLs for the nodes under /accumulo/[INSTANCE_ID] (See [zoo-info-viewer]).To run the utility, only ZooKeeper needs to be running. If hdfs is running, the instance id can be read from hdfs, or it can be entered with the zoo-info-viewer command –instanceId option.  Accumulo management processes do not need to be running. This allows checking the ACLs before starting an upgrade.The utility also prints the same permissions and user strings as the ZooKeeper cli getAcl command, so you canfully evaluate the permissions in the context of your needs.Sample output (See the [zoo-info-viewer] tools documentation for a more complete sample):ACCUMULO_OKAY:NOT_PRIVATE /accumulo/f491223b-1413-494e-b75a-c2ca018db00f cdrwa:accumulo, r:anyoneACCUMULO_OKAY:PRIVATE /accumulo/f491223b-1413-494e-b75a-c2ca018db00f/config cdrwa:accumuloERROR_ACCUMULO_MISSING_SOME:NOT_PRIVATE /accumulo/f491223b-1413-494e-b75a-c2ca018db00f/users/root/Namespaces r:accumulo, r:anyoneThe utility prints out a line for each znode that contains two fields related to ZooKeeper ACL permissions:  [ACCUMULO_OKAY | ERROR_ACCUMULO_MISSING_SOME] - Are the permissions sufficient for Accumulo to operate  [PRIVATE | NOT_PRIVATE] - Can other users can read data from the ZooKeeper nodes.Nodes marked with ERROR_ACCUMULO_MISSING_SOME shows that Accumulo does not have cdrwa permissions.Without full permissions, the upgrade will fail checks. The node permissions need to be corrected with the ZooKeepersetAcl command.  If you do not have sufficient permissions to change the ACLs on a node, see the section below, ACL errors during upgrade.Most Accumulo nodes do not contain sensitive data. Allowing unauthenticated ZooKeeper client(s) to read values is not unusual in typical deployments. The exception to a permissive read policy are the nodes that store configuration and properties (generally, nodes named ../config). Because property values may be sensitive, access should berestricted to authenticated Accumulo clients.  The tool will mark those nodes as PRIVATE.Allowing users other than authenticated Accumulo clients to write or modify nodes is not recommended.The utility also prints the same permissions and user strings as the ZooKeeper cli getAcl command, so you can fully evaluate the permissions in the context of your needs.  See the [zoo-info-viewer] tools documentation for sample output.ACL errors during upgradeManual intervention is required in the event that an upgrade fails due to unexpected znode ACLs. To resolve this issue ZooKeeper will need to be restarted with an additional property to bypass existing ACLs so that the ACLs can be fixed. Specifically, the DigestAuthenticationProvider.superDigest ZooKeeper Authentication option needs to be set so that you can log into the ZooKeeper shell and fix the ACLs. The steps for this are:1. Stop ZooKeeper2. Run the following from ZOOKEEPER_HOME replacing `$secret` with some value:```export CLASSPATH=&quot;lib/*&quot;java org.apache.zookeeper.server.auth.DigestAuthenticationProvider accumulo:$secret```3. Add the following to zoo.cfg replacing `$digest` with the digest value returned from step 2:```DigestAuthenticationProvider.superDigest=accumulo:$digest```4. Restart ZooKeeper5. Log into ZooKeeper using the `zkCli` and run `addauth digest accumulo:$secret` using the secret from step 2.6. Then, correct the ACL on the znode using the command `setAcl -R &amp;lt;path&amp;gt; world:anyone:r,auth:accumulo:cdrwa`",
      "url": " /docs/2.x/troubleshooting/zookeeper",
      "categories": "troubleshooting"
    },
  
  
    "release-accumulo-2-1-3": {
      "title": "Apache Accumulo 2.1.3",
      "content": "AboutApache Accumulo 2.1.3 is a patch release of the 2.1 LTM line. It contains bugfixes and minor enhancements. This version supersedes 2.1.2. Users upgrading to2.1 should upgrade directly to this version instead of 2.1.2.Included here are some highlights of the most interesting bugs fixed andfeatures added in 2.1.3. For the full set of changes, please see the commithistory or issue tracker.Notable ImprovementsImprovements that affect performance:  #3722 Adds properties general.file.name.allocation.batch.size.min andgeneral.file.name.allocation.batch.size.max that allow the batch sizefor unique filename allocation in ZooKeeper to be configurable. In a system that requires large numbersof unique names, larger batch sizes can reduce ZooKeeper contention because more file names can bereserved with a single ZooKeeper call.  #3733 Avoid creating server side threads when failed writes are cancelled. In versions 2.1.2and earlier, the thrift close call creates a new thread to cancel the thrift session. With 2.1.3, anew thrift method is available to test if a session is reserved and deletes it if it is not reservedwithout creating an additional thread.  If the new method is not available it falls back to the previousclose method to preserve interoperability between 2.x versions.  #3738 Adds parameter gc.remove.in.use.candidates, that enables the Garbage Collectorto remove candidates that have active tablet file references. This is expected to increase the speed ofsubsequent GC Runs.  #3756 Added new RPC named cancelUpdate that reduces the amount of threads waiting to close failedbatch write sessions.  #4682 Changed the ScanServer file reference format in the metadata table to sort by UUID to increase performance.  #4536 Created alternate ScanServerSelector implementation that tries to use scan servers on the same host to leverageshared off-heap-cache usage.  #4544 Made scan servers refresh cached list of tablet files before expiration. Added new propertysserver.cache.metadata.refresh.percent to control when refresh happens.  #3813 Made compactors use cached address for compaction coordinator when getting next compaction job.This lowers load on zookeeper when running many compactors.  #3706 Avoid unnescessary copying of hadoop config that was causing Accumulo GC slowdown.  #4709 Modified Manager balancer code such that the root, metadata, and user tables will be balancedseparately, and in that order. For example, balancing for user tables will not occur while the metadata table is unbalanced.Notable Bug Fixes  #3721 Fixes issue with writes happening in a retry after batch writer was closed. Thisstrengthens metadata consistency.  #3749 Fixes issue where deleting a compaction pool with running compactions wouldleave the tserver in a bad state.  #3748 Fixes bug where wal could remained locked if an exception occurred.  #3747 Adds validation to prevent possible deadlock when acquiring wal locks.  #3737 Use custom Transport to set Transport message and frame size. This fixesa bug where Accumulo would not change the max message size allowed.  ##608, #3755 Add validation to GC that checks that the scanner used by GC to determinecandidates for deletion returned a complete row as a mitigation for ##608 wheregarbage collector removes file that are referenced and in-use.  #3744 Fixed bug regarding improperly created GCRun logger name.  #3737 Adds a custom Transport Factory to set transport message and frame size to avoid infinite loopsas described in #3731.  #3750 Fixed issue when deleting a compaction thread pool that would leave tablets in a bad state.  #4117 Fixed a bug in compaction properties where the replacement maxOpen property was beingignored in favor of the deprecated open.max property.  #4681 Stopped listing all compactors in each compactor to reduce load on Zookeeper.  #4309 Optimized logic for getting a random TabletServer connection which improved Shell startup times.  #3873 Increase Thrift max message size defaults which avoids errors with large key values.  Addedproperty rpc.message.size.max.  Removed experimental properties sserver.server.message.size.max,compactor.message.size.max,compaction.coordinator.message.size.max.  #3966 Changed the default value of the the property table.majc.compaction.strategyto empty string to fix a compatibility bug with old and new compaction plugins.  #4554  Fixed a race condition that could cause duplicate compactions to run. While harmless interms of data, the duplicate compactions could waste significant compute resources.  #4127 Updated new compaction plugins to honor table.file.max property. Implementeda much more efficient algorithm than old compaction plugins had for this property.  #4485 Interrupt compactions on tablet unload. This prevents long running compactions from blockingtablet migration.  #3512 Fixed issue with improperly cleaned up scans preventing metadata tablet unload.  #4456 Setting empty property value no longer deletes property  #4000 Fixed bug that could cause bulk import to lose files when errors happened in the tablet server.  #4462 Fixed bug that prevented listing Fate operations in some situations.  #4573 Modified CredentialProviderToken to no longer store password in serialized form.  #4684 Fixed an issue that was causing table creation to get progressively slower when creating a lot of tables.Improvements that help with administration:  #3697 Allow ACCUMULO_JAVA_PREFIX option in accumulo-env.sh so it can be passedas an array. This simplifies passing user options when starting Accumulo processes, for examplenumactl parameters.  #3751 Added property rpc.backlog to configure backlog size forThrift server sockets.  #3745 Adds prefix to gc deletion log messages. This makes it easier to isolate the deletionactions of the garbage collector for analysis.  #3724 Adds logging of transactions when metadata and in-memory differences are detected.  #3725 Changed the gc batch size from bytes to memory percentage value. Modified default value ofproperty gc.candidate.batch.size.  #3684 Consolidated y/n prompts in the shell. Users can now exit out of multi-table delete operationswithout accepting prompts for each one.  #3726 Adjusted reauthentication messages from the shell to assist with troubleshooting.  #4461, #4522, #4577 Added various metrics for scan servers to determine scan reservations and usage  #4492 Emit new metrics for the caches in tablet and scan servers.  #4459 Added the ability to specify multiple MeterRegistryFactorys to allow for various metric exportersto be used at the same time.  Updated documentation on the property general.micrometer.factory  #4622 Added compactor busy and server idle metrics #4740 to allow for proper scale-in operations.Added property general.metrics.process.idle to configure idle time for metrics.  #3927 Added a new JSON property type that validates the value is json. Updated the propertiesmonitor.resources.external and tserver.compaction.major.service.meta.planner.opts.executors to use this new type.  #4223 Added properties compactor.wait.time.job.min and compactor.wait.time.job.max tocontrol the min and max times compactors use when polling for work.  #3998 Added instance name tag to metrics. This is useful for the case when metrics from multiple Accumulo instancesare flowing into a single metrics system.  #4763 Improved the accumulo-cluster script and cluster.yaml file for the use case of starting and stopping specificgroups of compactors and scan servers.  #4487 Scan server properties can now be set in the shell.  #4768 Modified thread pool names so that the user will be able to easily find their location in the source codewhen looking at thread pool metrics. This change affects the thread names in the output of a jstack on the process.  #4470 Added new metrics to indicate how many migrations are remaining.  #4558 Added a log message in the Manager when it has been waiting over 15 minutes for a tablet to unload.  #4495 Added accumulo admin serviceStatus command to quickly get system process status from the command line.UpgradingView the Upgrading Accumulo documentation for guidance.Useful Links  All tickets related to this releaseThis release also contains bug fixes from 1.10.4, which was released after 2.1.2.",
      "url": " /release/accumulo-2.1.3/",
      "categories": "release"
    }
    ,
  
    "blog-2024-04-09-does-a-compactor-return-memory-to-os-html": {
      "title": "Does a compactor process return memory to the OS?",
      "content": "GoalThe goal of the project was to determine if, once an Accumulo process is finished using memory, the JVM would release this unused memory back to the operating system. This was specifically observed in a Compactor process during the tests, but the findings should apply to any Accumulo Server process. We looked at the memory usage of the compactor process specifically to help understand if oversubscribing compactors on a machine is a viable option.As background information, it’s important to note that modern JVMs are expected to release memory back to the operating system, rather than just growing from the initial heap size (-Xms) to the maximum heap size (-Xmx) and never releasing it. This behavior was introduced in Java 11 through the JEP 346: Promptly Return Unused Committed Memory from G1. This feature aims to improve the efficiency of memory usage by actively returning Java heap memory to the operating system when idle.Test ScenarioThere could be a scenario where the amount of memory on a machine limits the number of compactors that can be run. For example, on a machine with 32GB of memory, if each compactor process uses 6GB of memory, we can only “fit” 5 compactors on that machine (32/6=5.333). Since each compactor process only runs on a single core, we would only be utilizing 5 cores on that machine where we would like to be using as many as we can.If the compactor process does not return the memory to the OS, then we are stuck with only using the following number of compactor processes:(total memory)/(memory per compactor).If the compactor processes return the memory to the OS, i.e. does not stay at the maximum 6GB once they reach it, then we can oversubscribe the memory allowing us to run more compactor processes on that machine.It should be noted that there is an inherent risk when oversubscribing processes that the user must be willing to accept if they choose to do oversubscribe. In this case, there is the possibility that all compactors run at the same time which might use all the memory on the machine. This could cause one or more of the compactor processes to be killed by the OOM killer.Test SetupEnvironment PrerequisitesThe machines used for testing were running Pop!_OS 22.04 a debian-based OS. The following package installation and usage steps may vary if one were try to repeat these steps.Install gnuplotThis was used for plotting the memory usage of the compactor over time from the perspective of the OS  sudo apt install gnuplot  gnuplot was started with the command gnuplotInstall VisualVMThis was used for plotting the memory usage of the compactor over time from the perspective of the JVM  Downloaded the zip from visualvm.github.io  Extracted with unzip visualvm_218.zip  VisualVM was started with the command ./path/to/visualvm_218/bin/visualvmConfigure and start accumuloAccumulo 2.1 was used for experimentation. To stand up a single node instance, fluo-uno was used.Steps taken to configure accumulo to start compactors:  Uncommented lines in fluo-uno/install/accumulo-2.1.2/conf/cluster.yaml regarding the compaction coordinator and compactor q1. A single compactor process was used, q1. This allows the external compaction processes to start up.  Configured the java args for the compactor process in “accumulo-env.sh.” Line:compactor) JAVA_OPTS=(&#39;-Xmx256m&#39; &#39;-Xms256m&#39; &quot;${JAVA_OPTS[@]}&quot;) ;;  Started accumulo with uno start accumuloInstall java versions  Installed java versions 11, 17 and 21. For example, Java 17 was installed with:          sudo apt install openjdk-17-jdk      sudo update-alternatives --config java and select the intended version before starting the accumulo instance      Ensured JAVA_HOME was set to the intended version of java before each test run      Running the test  Started accumulo using fluo-uno (after changing the mentioned configuration)          uno start accumulo        Opened VisualVM and selected the running compactor q1 process taking note of the PID  Ran mem_usage_script.sh &amp;lt;compactor process PID&amp;gt;. This collected measurements of memory used by the compactor process over time from the perspective of the OS. We let this continue to run while the compaction script was running.  Configured the external compaction script as needed and executed:          uno jshell experiment.jsh        Memory usage was monitored from the perspective of the JVM (using VisualVM) and from the perspective of the OS (using our collection script).Navigated to the “Monitor” tab of the compactor in VisualVM to see the graph of memory usage from JVM perspective.Followed the info given in the OS Memory Data Collection Script section to plot the memory usage from OS perspective.Helpful resources:  External Compactions accumulo blog post  Z garbage collector heap size docs  Generational Garbage Collection docs  G1 garbage collector docs  Java 11 and memory release articleExternal compaction test scriptInitiates an external compaction of 700MB of data (20 files of size 35MB) on Compactor q1.referred to as experiment.jsh in the test setup sectionimport org.apache.accumulo.core.conf.Property; int dataSize = 35_000_000; byte[] data = new byte[dataSize]; Arrays.fill(data, (byte) 65); String tableName = &quot;testTable&quot;; void ingestAndCompact() throws Exception {   try {        client.tableOperations().delete(tableName);    } catch (TableNotFoundException e) {        // ignore    }       System.out.println(&quot;Creating table &quot; + tableName);    client.tableOperations().create(tableName);       // This is done to avoid system compactions, we want to initiate the compactions manually    client.tableOperations().setProperty(tableName, Property.TABLE_MAJC_RATIO.getKey(), &quot;1000&quot;);    // Configure for external compaction    client.instanceOperations().setProperty(&quot;tserver.compaction.major.service.cs1.planner&quot;,&quot;org.apache.accumulo.core.spi.compaction.DefaultCompactionPlanner&quot;);    client.instanceOperations().setProperty(&quot;tserver.compaction.major.service.cs1.planner.opts.executors&quot;,&quot;[{&quot;name&quot;:&quot;large&quot;,&quot;type&quot;:&quot;external&quot;,&quot;queue&quot;:&quot;q1&quot;}]&quot;);       client.tableOperations().setProperty(tableName, &quot;table.compaction.dispatcher&quot;, &quot;org.apache.accumulo.core.spi.compaction.SimpleCompactionDispatcher&quot;);    client.tableOperations().setProperty(tableName, &quot;table.compaction.dispatcher.opts.service&quot;, &quot;cs1&quot;);       int numFiles = 20;       try (var writer = client.createBatchWriter(tableName)) {        for (int i = 0; i &amp;lt; numFiles; i++) {            Mutation mut = new Mutation(&quot;r&quot; + i);            mut.at().family(&quot;cf&quot;).qualifier(&quot;cq&quot;).put(data);            writer.addMutation(mut);            writer.flush();                 System.out.println(&quot;Writing &quot; + dataSize + &quot; bytes to a single value&quot;);            client.tableOperations().flush(tableName, null, null, true);        }    }         System.out.println(&quot;Compacting table&quot;);    client.tableOperations().compact(tableName, new CompactionConfig().setWait(true));    System.out.println(&quot;Finished table compaction&quot;);} ingestAndCompact(); // Optionally sleep and ingestAndCompact() again, or just execute the script again.OS Memory Data Collection ScriptTracks the Resident Set Size (RSS) of the given PID over time, outputting the data to output_mem_usage.log.Data is taken every 5 seconds for an hour or until stopped.referred to as mem_usage_script.sh in the test setup section#!/bin/bash PID=$1 echo &quot;Tracking PID: $PID&quot; DURATION=3600 # for 1 hour INTERVAL=5    # every 5 seconds rm output_mem_usage.log while [ $DURATION -gt 0 ]; do     ps -o %mem,rss -p $PID | tail -n +2 &amp;gt;&amp;gt; output_mem_usage.log     sleep $INTERVAL     DURATION=$((DURATION - INTERVAL)) doneAfter compactions have completed plot the data using gnuplot:gnuplotset title &quot;Resident Set Size (RSS) Memory usage&quot; set xlabel &quot;Time&quot;set ylabel &quot;Mem usage in kilobytes&quot;plot &quot;output_mem_usage.log&quot; using ($0*5):2 with lines title &#39;Mem usage&#39;DataImportant Notes:  ZGC and G1PeriodicGCInterval are not available with Java 11, so couldn’t be tested for  ZGenerational for ZGC is only available in Java 21, so couldn’t be tested for in Java 17  G1 GC is the default GC in Java 11, 17, and 21 (doesn’t need to be specified in java args)All Experiments Performed:            Java Version      Manual Compaction      Xmx=1G      Xmx=2G      Xms=256m      XX:G1PeriodicGCInterval=60000      XX:-G1PeriodicGCInvokesConcurrent      XX:+UseShenandoahGC      XX:+UseZGC      XX:ZUncommitDelay=120      XX:+ZGenerational                  11      🗸      🗸             🗸                                                        11      🗸      🗸             🗸                                                        11                    🗸      🗸                                                        11                    🗸      🗸                    🗸                                   17             🗸             🗸      🗸                                                 17                    🗸      🗸      🗸                                                 17             🗸             🗸      🗸      🗸                                          17                    🗸      🗸                           🗸      🗸                     17             🗸             🗸                    🗸                                   17                    🗸      🗸                    🗸                                   21                    🗸      🗸      🗸                                                 21                    🗸      🗸                           🗸      🗸      🗸              21                    🗸      🗸                           🗸      🗸                     21             🗸             🗸                    🗸                                   21                    🗸      🗸                    🗸                           Java 11 G1 GC with manual GC (via VisualVM) every minute. Java args: -Xmx1G -Xms256m                        Java 11 G1 GC with manual GC (via VisualVM) after each compaction. Java args: -Xmx1G -Xms256m                        Java 11 G1 GC. Java args: -Xmx2G -Xms256                        Java 11 Shenandoah GC. Java args: -Xmx2G -Xms256 -XX:+UseShenandoahGC                        Java 17 G1 GC. Java args: -Xmx1G -Xms256m -XX:G1PeriodicGCInterval=60000                        Java 17 G1 GC. Java args: -Xmx2G -Xms256m -XX:G1PeriodicGCInterval=60000                        Java 17 G1 GC. Java args: -Xmx1G -Xms256m -XX:G1PeriodicGCInterval=60000 -XX:-G1PeriodicGCInvokesConcurrent                        Java 17 ZGC. Java args: -Xmx2G -Xms256m -XX:+UseZGC -XX:ZUncommitDelay=120                        Java 17 Shenandoah GC. Java args: -Xmx1G -Xms256m -XX:+UseShenandoahGC                        Java 17 Shenandoah GC. Java args: -Xmx2G -Xms256m -XX:+UseShenandoahGC                        Java 21 G1 GC. Java args: -Xmx2G -Xms256m -XX:G1PeriodicGCInterval=60000                        Java 21 ZGC. Java args: -Xmx2G -Xms256m -XX:+UseZGC -XX:+ZGenerational -XX:ZUncommitDelay=120                        Java 21 ZGC. Java args: -Xmx2G -Xms256m -XX:+UseZGC -XX:ZUncommitDelay=120                        Java 21 Shenandoah GC. Java args: -Xmx1G -Xms256m -XX:+UseShenandoahGC                        Java 21 Shenandoah GC. Java args: -Xmx2G -Xms256m -XX:+UseShenandoahGC                        ConclusionAll the garbage collectors tested (G1 GC, Shenandoah GC, and ZGC) and all the Java versions tested (11, 17, 21) will release memory that is no longer used by a compactor, back to the OS*. Regardless of which GC is used, after an external compaction is done, most (but usually not all) memory is eventually released back to the OS and all memory is released back to the JVM. Although a comparable amount of memory is returned to the OS in each case, the amount of time it takes for the memory to be returned and the amount of memory used during a compaction depends on which garbage collector is used and which parameters are set for the java process.The amount that is never released back to the OS appears to be minimal and may only be present with G1 GC and Shenandoah GC. In the following graph with Java 17 using G1 GC, we see that the baseline OS memory usage before any compactions are done is a bit less than 400MB. We see that after a compaction is done and the garbage collection runs, this baseline settles at about 500MB.   On the same test run, the JVM perspective (pictured in the graph below) shows that all memory is returned (memory usage drops back down to Xms=256m after garbage collection occurs).   The roughly 100MB of unreturned memory is also present with Shenandoah GC in Java 17 and Java 21 but does not appear to be present with Java 11. With ZGC, however, we see several runs where nearly all the memory used during a compaction is returned to the OS (the graph below was from a run using ZGC with Java 21). These findings regarding the unreturned memory may or may not be significant. They may also be the result of variance between runs. More testing would need to be done to confirm or deny these claims.   Another interesting finding was that the processes use more memory when more is allocated. These results were obtained from initiating a compaction of 700MB of data (see experiment.jsh script). For example, setting 2GB versus 1GB of max heap for the compactor process results in a higher peak memory usage. During a compaction, when only allocated 1GB of heap space, the max heap space is not completely utilized. When allocated 2GB, compactions exceed 1GB of heap space used. It appears that G1 GC and ZGC use the least amount of heap space during a compaction (maxing out around 1.5GB and when using ZGC with ZGeneration in Java 21, this maxes out around 1.7GB). Shenandoah GC appears to use the most heap space during a compaction with a max heap space around 1.9GB (for Java 11, 17, and 21). However, these differences might be due to differences between outside factors during runs and more testing may need to be done to confirm or deny these claims.Another difference found between the GCs tested was that Shenandoah GC sometimes required two garbage collections to occur after a compaction completed to clean up the memory. Based on our experiments, when a larger max heap size was allocated (2GB vs 1GB), the first garbage collection that occurred only cleaned up about half of the now unused memory, and another garbage collection had to occur for the rest to be cleaned up. This was not the case when 1GB of max heap space was allocated (almost all of the unused memory was cleaned up on the first garbage collection, with the rest being cleaned up on the next garbage collection). G1 GC and ZGC always cleaned up the majority of the memory on the first garbage collection.*Note: When using the default GC (G1 GC), garbage collection does not automatically occur unless further garbage collection settings are specified (e.g., G1PeriodicGCInterval)",
      "url": " /blog/2024/04/09/does-a-compactor-return-memory-to-OS.html",
      "categories": "blog"
    }
    ,
  
    "release-accumulo-1-10-4": {
      "title": "Apache Accumulo 1.10.4",
      "content": "AboutApache Accumulo 1.10.4 is the final bug fix release of the 1.10 LTM releaseline. As of this release, the 1.10 release line is now considered end-of-life.This means that any fixes that are applied because of a bug found in thisversion will not be applied and released as a new 1.10 patch version, butinstead will be applied and released to the currently active release lines, ifthey apply to those versions.These release notes are highlights of the changes since 1.10.3. The fulldetailed changes can be seen in the git history. If anything important ismissing from this list, please contact us to have it included.Users of any 1.10 version are encouraged to upgrade to the next LTM release,which is 2.1 at the time of this writing. This patch release is provided as afinal release with all the patches the developers have made to 1.10, foranybody who must remain using 1.10, and who want to upgrade from an earlier 1.xversion.Known IssuesApache Commons VFS was upgraded in #1295 for 1.10.0 and some users have reportedissues similar to VFS-683. Possible solutions are discussed in #2775.This issue is applicable to all 1.10 versions.Major Improvements  #3391 Drop support for MapFile file formats as an alternative toRFile; the use of MapFiles was already broken, and had been for a long time.So this change was done to cause an explicit and detectable failure, ratherthan allow a silent one to occur if a MapFile was attempted to be used.  #3703 Add verification checks to improve the reliability of theaccumulo-gc, in order to ensure that a full row for a tablet was seen when afile deletion candidate is checkedOther Improvements  #3300 Fix the documentation about iterator teardown in the user manual  #3343 Fix errors in the javadoc for RangeNote About JDK 15See the note in the 1.10.1 release notes about the use of JDK 15 or later, asthe information pertaining to the use of the CMS garbage collector remainsapplicable to all 1.10 releases.Useful Links  All Changes since 1.10.3  All tickets related to this release",
      "url": " /release/accumulo-1.10.4/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-3-0-0": {
      "title": "Apache Accumulo 3.0.0",
      "content": "AboutApache Accumulo 3.0.0 is a non-LTM major version release. While itprimarily contains the 2.1 codebase, including all patches through2.1.2, it has also removed a substantial number of deprecated featuresand code, in an attempt to clean up several years of accrued technicaldebt, and lower the maintenance burden to make way for futureimprovements. It also contains a few other minor improvements.Notable Removals  #1328 The FileSystem monitor has been removed and will nolonger watch for problems with local file systems and self-terminate.System administrators are encouraged to use whatever systems healthmonitoring is appropriate for their deployments, rather than depend onAccumulo to monitor these.  #2443 The MapReduce APIs embedded in the accumulo-core modulewere removed. The separate accumulo-hadoop-mapreduce jar is theirreplacement.  #3073 The legacy Connector and Instance client classes were removed.The AccumuloClient is their replacement.  #3080 The cross-data center replication feature was removed withoutreplacement due to lack of being maintained, having numerous outstandingunfixed issues with no volunteer to maintain it since it was deprecated, andsubstantial code complexity. The built-in replication table it used fortracking replication metadata will be removed on upgrade.  #3114, #3115, #3116, #3117 Removeddeprecated VolumeChooser, TabletBalancer, Constraint, and other APIs, infavor of their SPI replacements.  #3106 Remove deprecated configuration properties (see 2.1 propertydocumentation for which ones were deprecated)  #3112 Remove CompactionStrategy class in favor of CompactionSelectorand CompactionConfigurer.  #3160 Remove upgrade code for versions prior to 2.1 (minimum versionto upgrade from is now 2.1.  #3192 Remove arguments to server processes, such as (-a, -g,-q, etc.) were removed in favor of configuration properties that can bespecified in the Accumulo configuration files or supplied on a per-processbasis using the -o argument. The provided cluster management referencescripts were updated in #3197 to use the -o method.  #3136 Remove the built-in VFS classloader support. To use a customclassloader, users must now set the ContextClassLoaderFactory implementationin the properties. The default is now the URLContextClassLoaderFactory.  #3318 Remove the old bulk import implementation, replaced by the newbulk import API added in 2.0.0.  #3265 Remove scan interpreter and scan formatter from the shell  #3361 Remove all remaining references to the old “master” service(renamed to “manager”).  #3360 Remove checks and code related to the old password hashingmechanism in Accumulo. This will discontinue warnings about users passwordsthat are still out of date. Instead, those outdated passwords will simplybecome invalid. If the user authenticated to Accumulo at any time prior toupgrading, their password will have been converted. So this only affectsaccounts that were never used with 2.1 at all. As mitigation, such users willbe able to have their password reset by the root user. If the root user neverauthenticated (and neither had another admin user) while on 2.1 (very veryunlikely), an administrator can reset the entire user database through thenormal init step to reset security.  #3378 Remove broken support for old map files. (RFiles have been inuse for a long time, so this should not impact any users; if users had beentrying to use map files, they would have found that they were broken anyway)Notable Additions  #3088 New methods were added to compaction-related APIs to shareinformation about the current tablet being compacted to user code  #3107 Decompose internal thrift services by function to make RPCfunctionality more modular by server instances  #3189 Standardized server lock data structure in ZooKeeper  #3206 Internal caches now use Caffeine instead of Guava’s Cache  #3161, #3288 The internal service (renamed fromGarbageCollectionLogger to LowMemoryDetector) that was previously used onlyto report low memory in servers, was made configurable to allow pausingcertain operations like scanning, minor compactions, or major compactions,when memory is low. See the server properties for general.low.mem.*.UpgradingView the Upgrading Accumulo documentation for guidance.Useful Links  All tickets related to this releaseThis release also includes bug fixes from 2.1.2 and earlier.",
      "url": " /release/accumulo-3.0.0/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-2-1-2": {
      "title": "Apache Accumulo 2.1.2",
      "content": "AboutApache Accumulo 2.1.2 is a patch release of the 2.1 LTM line. It contains bugfixes and minor enhancements. This version supersedes 2.1.1. Users upgrading to2.1 should upgrade directly to this version instead of 2.1.1.Included here are some highlights of the most interesting bugs fixed andfeatures added in 2.1.2. For the full set of changes, please see the commithistory or issue tracker.Notable ImprovementsImprovements that affect performance:  #3499, #3543, #3549, #3500, #3509Made some optimizations around the processing of file references in theaccumulo-gc code, including optimizing a constructor in a class calledTabletFile used to track file references.  #3541, #3542 Added a new property,manager.tablet.watcher.interval, to make the time to wait betweenscanning the metadata table for outstanding tablet actions (such as assigningtablets, etc.) to be configurable.Improvements that help with administration:  #3678, #3683 Added extra validation of propertytable.class.loader.context at the time it is set, to preventinvalid contexts from being set on a table.  #3548, #3561 Added a banner to the manager page in theMonitor that displays the manager state and goal state when they are notnormal.  #3383, #3680 Prompt the user for confirmation when theyattempt to set a deprecated property in the Shell as a way to get them to usethe non-deprecated property.  #3233, #3562 Add option to --exclude-parent to allowcreating a table or namespace in the shell initialized with only theproperties set on another table or namespace, but not those the other tableor namespace were inheriting from their parent.  #3600 Normalized metric labels and structure.Notable Bug Fixes  #3488, #3612 Fixed sorting of some columns on the monitor  #3674, #3677, #3685 Prevent an invalid tablecontext and other errors from killing the minor compaction thread andpreventing a tablet from being closed and shutting down normally.  #3630, #3631 Fix a bug where BatchWriter latency andtimeout values were incorrectly converted to the wrong time unit..  #3617, #3622 Close LocalityGroupReader when IOException isthrown to release reference to a possibly corrupted stream in a cached blockfile.  #3570, #3571 Fixed the TabletGroupWatcher shutdown order.  #3569, #3579 #3644 Changes to ensure that scansessions are cleaned up.  #3553, #3555 A bug where a failed user compaction would notretry and would hang was fixed.Other Notable Changes  #3550 The contents of the contrib directory have been moved to moreappropriate locations for build-related resourcesUpgradingView the Upgrading Accumulo documentation for guidance.Useful Links  All tickets related to this release",
      "url": " /release/accumulo-2.1.2/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-2-1-1": {
      "title": "Apache Accumulo 2.1.1",
      "content": "AboutApache Accumulo 2.1.1 is a patch release of the 2.1 LTM line. It containsmany bug fixes and minor enhancements, including a critical fix. This versionsupersedes 2.1.0. Users upgrading to 2.1 should upgrade directly to thisversion instead of 2.1.0.Included here are some highlights of the most interesting bugs and featuresfixed in 2.1.1. Several trivial bugs were also fixed that related to thepresentation of information on the monitor, or to avoid spammy/excessivelogging, but are too numerous to list here. For the full set of bug fixes,please see the commit history or issue tracker.NOTE: This 2.1 release also includes any applicable bug fixes and improvementsthat occurred in 1.10.3 and earlier.Critical Fixes  CVE-2023-34340 Fixed a critical issue that improperly allowed a user undersome conditions to authenticate to Accumulo using an invalid password.Notable ImprovementsImprovements that add capabilities:  #3180 Enable users to provide per-volume Hadoop Filesystemconfiguration overrides via the Accumulo configuration. Hadoop Filesystemobjects are configured by the standard Hadoop mechanisms (defaultconfiguration, core-site.xml, hdfs-site.xml, etc.), but these configurationfiles don’t allow for the same property to be specified with different valuesfor different namespaces. This change allows users to specify differentproperty values for different Accumulo volumes, which will be applied to theHadoop Filesystem object created for each Accumulo volume  #1169, #3142 Add configuration option for users to selecthow the last location field is used, so users have better control overinitial assignments on restarts  #3400 Inject environment injected into ContextClassLoaderFactory SPIso implementations can read and make use of Accumulo’s own configurationImprovements that affect performance:  #3175 Reset number of locks in SynchronousLoadingBlockCache from2017 back to 5003, the value that it was in 1.10. #3226 Also,modified the lock to be fair, which allows the different scan threads in theserver to make progress in a more fair manner when they need to load a blockinto the cache  #3077, #3079, #3083, #3123 Avoid fillingOS page cache by calling setDropBehind on the FS data stream whenperforming likely one-time file accesses, as with WAL and compaction inputand output files. This should allow files that might benefit more fromcaching to stay in the cache longer. #3083 and #3123introduces new properties, table.compaction.major.output.drop.cache andtable.compaction.minor.output.drop.cache, for dropping pages from the OS pagecache for compaction output files. These changes will only have an impact onHDFS FileSystem implementations and operating systems that support theunderlying OS system call. See associated issue, HDFS-16864, that willimprove the underlying implementation when resolved.Improvements that help with administration:  #3445 Add emergency maintenance utility to edit properties inZooKeeper while the Accumulo cluster is shut down  #3118 Added option to the admin zoo-info-viewer command to dumpthe ACLs on ZooKeeper nodes. This information can be used to fix znodes withincorrect ACLs during the upgrade processOther notable changes:  #3126 Remove unintentionally bundled htrace4 from our packaging;users will need to provide that for themselves if they require it on theirclasspath  #3436 Deprecate gc.trash.ignore property. The trash can becustomized within Hadoop if one wishes to ignore it, or configure it to beignored for only specific files (and this has been tested with recentversions of Hadoop); In version 3.0, this property will be removed, and itwill no longer be possible to ignore the trash by changing this propertyNotable Bug Fixes  #3134 Fixed Thrift issues due to incorrect setting of maxMessageSize  #3144, #3150, #3164 Fixed bugs in ScanServer thatprevented a tablet from being scanned when some transient failures occurred  #3346, #3366 Fixed tablet metadata verification task so itdoesn’t unintentionally cause the server to halt  #3479 Fixed issue preventing servers from shutting down because theywere still receiving assignments  #3492 Fixed a bug where bulk imports could cause compactions to hangUpgradingView the Upgrading Accumulo documentation for guidance.Useful Links  All tickets related to this releaseThis release also includes bug fixes from 1.10.3, which was released after2.1.0, and the javadoc fix and MapFile removal changes (#3343 and#3391) from 1.10.4.",
      "url": " /release/accumulo-2.1.1/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-10-3": {
      "title": "Apache Accumulo 1.10.3",
      "content": "AboutApache Accumulo 1.10.3 is a bug fix release of the 1.10 LTM release line.These release notes are highlights of the changes since 1.10.2. The fulldetailed changes can be seen in the git history. If anything important ismissing from this list, please contact us to have it included.Users of 1.10.2 or earlier are encouraged to upgrade to 1.10.3, as this is acontinuation of the 1.10 LTM release line with bug fixes and improvements, andit supersedes any prior 1.x version. Users are also encouraged to considermigrating to a 2.x version when one that is suitable for their needs becomesavailable.Known IssuesApache Commons VFS was upgraded in #1295 for 1.10.0 and some users have reportedissues similar to VFS-683. Possible solutions are discussed in #2775.This issue is applicable to all 1.10 versions.Major ImprovementsNoneOther Improvements  #2708 Disabled merging minor-compactions by default  #3226 Change scan thread resource management to use a “fair”semaphore to avoid resource starvation in some situations  #3221, #3249, #3261 Improve some performance byimproving split point calculations  #3276 Improve performance by optimizing internal data structures infrequently used Authorizations objectOther Bug Fixes  #3069 Fix a minor bug with VFS on newer Java versions due toMIME-type changes  #3176 Fixed bug in client scanner code that was not using thecorrect timeout variable in some places  #3168 Fixed bug in TabletLocator that could cause the BatchScannerto return duplicate data  #3231, #3235 Fix wait timeout logic when waiting forminimum number of available tservers during startupNote About JDK 15See the note in the 1.10.1 release notes about the use of JDK 15 or later, asthe information pertaining to the use of the CMS garbage collector remainsapplicable to all 1.10 releases.Useful Links  Release VOTE email thread  All Changes since 1.10.2  All tickets related to this release",
      "url": " /release/accumulo-1.10.3/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-2-1-0": {
      "title": "Apache Accumulo 2.1.0",
      "content": "AboutApache Accumulo 2.1.0 brings many new features and updates since 1.10 and 2.0. The 2.1 releaseseries is an LTM series, and as such, is expected to receive stability-improving bugfixes, asneeded. This makes this series suitable for production environments where stability is preferableover new features that might appear in subsequent non-LTM releases.This release has received more than 1200 commits from over 50 contributors, including numerousbugfixes, updates, and features.Minimum RequirementsThis version of Accumulo requires at least Java 11 to run. Various Java 11 versions from differentdistributors were used throughout its testing and development, so we expect it to work with anystandard OpenJDK-based Java distribution.At least Hadoop 3 is required, though it is recommended to use a more recent version. Version 3.3was used extensively during testing, but we have no specific knowledge that an earlier version ofHadoop 3 will not work. Whichever major/minor version you use, it is recommended to use the latestbugfix/patch version available. By default, our POM depends on 3.3.4.During much of this release’s development, ZooKeeper 3.5 was used as a minimum. However, thatversion reach its end-of-life during development, and we do not recommend using end-of-life versionsof ZooKeeper. The latest bugfix version of 3.6, 3.7, or 3.8 should also work fine. By default, ourPOM depends on 3.8.0.Binary IncompatibilityThis release is known to be incompatible with prior versions of the client libraries. That is, the2.0.0 or 2.0.1 version of the client libraries will not be able to communicate with a 2.1.0 or laterinstallation of Accumulo, nor will the 2.1.0 or later version of the client libraries communicatewith a 2.0.1 or earlier installation.Major New FeaturesOverhaul of Table CompactionsSignificant changes were made to how Accumulo compacts files in this release. Seecompaction  for details, below are some highlights.  Multiple concurrent compactions per tablet on disjoint files is now supported.  Previously only asingle compaction could run on a tablet.  This allows tablets that are running long compactionson large files to concurrently compact new smaller files that arrive.  Multiple compaction thread pools per tablet server are now supported. Previously only a singlethread pool existed within a tablet server for compactions.  With a single thread pool, if allthreads are working on long compactions it can starve quick compactions.  Now compactions withlittle data can be processed by dedicated thread pools.  Accumulo’s default algorithm for selecting files to compact was modified to select the smallestset of files that meet the compaction ratio criteria instead of the largest set.  This changemakes tablets more aggressive about reducing their number files while still doing logarithmiccompaction work. This change also enables efficiently compacting new small files that arriveduring a long running compaction.  Having dedicated compaction threads pools for tables is now supported through configuration.  Thedefault configuration for Accumulo sets up dedicated thread pools for compacting the Accumulometadata table.  Merging minor compactions were dropped.  These were added to Accumulo to address the problem ofnew files arriving while a long running compaction was running.  Merging minor compactions couldcause O(N^2) compaction work. The new compaction changes in this release can satisfy this usecase while doing a logarithmic amount of work.CompactionStrategy was deprecated in favor of new public APIs. CompactionStrategy was never publicAPI as it used internal types and one of these types FileRef was removed in 2.1. Users who havewritten a CompactionStrategy can replace FileRef with its replacement internal typeStoredTabletFile but this is not recommended. Since it is very likely that CompactionStrategy willbe removed in a future release, any work put into rewriting a CompactionStrategy will be lost. It isrecommended that users implement CompactionSelector, CompactionConfigurer, and CompactionPlannerinstead.  The new compaction changes in 2.1 introduce new algorithms for optimally schedulingcompactions across multiple thread pools, configuring a deprecated compaction strategy may result ismissing out on the benefits of these new algorithms.See the javadoc for moreinformation.GitHub tickets related to these changes: #564 #1605 #1609 #1649External Compactions (experimental)This feature includes two new optional server components, CompactionCoordinator and Compactor, thatenables the user to run major compactions outside of the TabletServer. See design , compaction , and the External Compaction blogpost for more information. This work was completed over many tickets, see the GitHubproject for the related issues. #2096Scan Servers (experimental)This feature includes a new optional server component, Scan Server, that enables the user to runscans outside of the TabletServer. See design ,#2411, and #2665 for more information. Importantly, users can utilize thisfeature to avoid bogging down the TabletServer with long-running scans, slow iterators, etc.,provided they are willing to tolerate eventual consistency.New Per-Table On-Disk Encryption (experimental)On-disk encryption can now be configured on a per table basis as well as for the entire instance(all tables). See on-disk-encryption  for more information.New jshell entry pointCreated new “jshell” convenience entry point. Run bin/accumulo jshell to start up jshell,preloaded with Accumulo classes imported and with an instance of AccumuloClient already created foryou to connect to Accumulo (assuming you have a client properties file on the class path)  #1870 #1910Major ImprovementsFixed GC Metadata hotspotsPrior to this release, Accumulo stored GC file candidates in the metadata table using rows of theform ~del&amp;lt;URI&amp;gt;. This row schema lead to uneven load on the metadata table and metadata tabletsthat were eventually never used. In #1043 / #1344, the row format was changed to~del&amp;lt;hash(URI)&amp;gt;&amp;lt;URI&amp;gt; resulting in even load on the metadata table and even data spread in thetablets. After upgrading, there may still be splits in the metadata table using the old row format.These splits can be merged away as shown in the example below which starts off with splits generatedfrom the old and new row schema. The old splits with the prefix ~delhdfs are merged away.root@uno&amp;gt; getsplits -t accumulo.metadata2&amp;lt;~~del55~dela7~delhdfs://localhost:8020/accumulo/tables/2/default_tablet/F00000a0.rf~delhdfs://localhost:8020/accumulo/tables/2/default_tablet/F00000kb.rfroot@uno&amp;gt; merge -t accumulo.metadata -b ~delhdfs -e ~delhdfs~root@uno&amp;gt; getsplits -t accumulo.metadata2&amp;lt;~~del55~dela7Master Renamed to ManagerIn order to use more inclusive language in our code, the Accumulo team has renamed all references tothe word “master” to “manager” (with the exception of deprecated classes and packages retained forcompatibility). This change includes the master server process, configuration properties with masterin the name, utilities with master in the name, and packages/classes in the code base. Where thesechanges affect the public API, the deprecated “master” name will still be supported until Accumulo3.0.  Important  One particular change to be aware of is that certain state for the manager process is stored in  ZooKeeper, previously in under a directory named masters. This directory has been renamed to  managers, and the upgrade will happen automatically if you launch Accumulo using the provided  scripts. However, if you do not use the built in scripts (e.g., accumulo-cluster or  accumulo-service), then you will need to perform a one-time upgrade of the ZooKeeper state by  executing the RenameMasterDirInZK utility:    ${ACCUMULO_HOME}/bin/accumulo org.apache.accumulo.manager.upgrade.RenameMasterDirInZK  Some other specific examples of these changes include:  All configuration properties starting with master. have been renamed to start with manager.instead. The master.* property names in the site configuration file (or passed on thecommand-line) are converted internally to the new name, and a warning is printed. However, the oldname can still be used until at least the 3.0 release of Accumulo. Any master.* properties thathave been set in ZooKeeper will be automatically converted to the new manager.* name whenAccumulo is upgraded. The old property names can still be used by the config shell command orvia the methods accessible via AccumuloClient, but a warning will be generated when the oldnames are used. You are encouraged to update all references to master in your site configurationfiles to manager when installing Accumulo 2.1.  The tablet balancers in the org.apache.accumulo.server.master.balancer package have all beenrelocated to org.apache.accumulo.server.manager.balancer. DefaultLoadBalancer has been alsorenamed to SimpleLoadBalancer along with the move. The default balancer has been updated fromorg.apache.accumulo.server.master.balancer.TableLoadBalancer toorg.apache.accumulo.server.manager.balancer.TableLoadBalancer, and the default per-tablebalancer has been updated from org.apache.accumulo.server.master.balancer.DefaultLoadBalancer toorg.apache.accumulo.server.manager.balancer.SimpleLoadBalancer. If you have customized thetablet balancer configuration, you are strongly encouraged to update your configuration toreference the updated balancer names. If you have written a custom tablet balancer, it should beupdated to implement the new interfaceorg.apache.accumulo.server.manager.balancer.TabletBalancer rather than extending the deprecatedabstract org.apache.accumulo.server.master.balancer.TabletBalancer.  The configuration file masters for identifying the manager host(s) has been deprecated. If thisfile is found, a warning will be printed. The replacement file managers should be used (i.e.,rename your masters file to managers) instead.  The master argument to the accumulo-service script has been deprecated, and the replacementmanager argument should be used instead.  The -master argument to the org.apache.accumulo.server.util.ZooZap utility has been deprecatedand the replacement -manager argument should be used instead.  The GetMasterStats utility has been renamed to GetManagerStats.  org.apache.accumulo.master.state.SetGoalState is deprecated, and any custom scripts that invokethis utility should be updated to call org.apache.accumulo.manager.state.SetGoalState instead.  masterMemory in minicluster.properties has been deprecated and managerMemory should be usedinstead in any minicluster.properties files you have configured.  See also #1640 #1642 #1703 #1704 #1873 #1907New Tracing FacilityHTrace support was removed in this release and has been replaced with OpenTelemetry. Trace information will not be shown in the monitor. See comments in #2259 for an example of how to configure Accumulo to emit traces to supported OpenTelemetry sinks.#2257New Metrics ImplementationThe Hadoop Metrics2 framework is no longer being used to emit metrics from Accumulo. Accumulo is nowusing the Micrometer framework. Metric name and type changes have beendocumented in org.apache.accumulo.core.metrics.MetricsProducer, see the javadoc for more information. See comments in #2305 for an example of how to configure Accumulo to emit metrics to supported Micrometer sinks.#1134New SPI PackageA new Service Plugin Interface (SPI) package was created in the accumulo-core jar, atorg.apache.accumulo.core.spi, under which exists interfaces for the various pluggablecomponents. See #1900 #1905 #1880 #1891 #1426Minor ImprovementsNew listtablets Shell CommandA new command was created for debugging called listtablets, that shows detailed tablet informationon a single line. This command aggregates data about a tablet such as status, location, size, numberof entries and HDFS directory name. It even shows the start and end rows of tablets, displaying themin the same sorted order they are stored in the metadata. See example command output below. #1317 #1821root@uno&amp;gt; listtablets -t test_ingest -h2021-01-04T15:12:47,663 [Shell.audit] INFO : root@uno&amp;gt; listtablets -t test_ingest -hNUM  TABLET_DIR      FILES WALS  ENTRIES   SIZE      STATUS     LOCATION                       ID    START (Exclusive)    ENDTABLE: test_ingest1    t-0000007       1     0            60       552 HOSTED     CURRENT:ip-10-113-12-25:9997   2     -INF                 row_00000000052    t-0000006       1     0           500     2.71K HOSTED     CURRENT:ip-10-113-12-25:9997   2     row_0000000005       row_00000000553    t-0000008       1     0         5.00K    24.74K HOSTED     CURRENT:ip-10-113-12-25:9997   2     row_0000000055       row_00000005554    default_tablet  1     0         4.44K    22.01K HOSTED     CURRENT:ip-10-113-12-25:9997   2     row_0000000555       +INFroot@uno&amp;gt; listtablets -t accumulo.metadata2021-01-04T15:13:21,750 [Shell.audit] INFO : root@uno&amp;gt; listtablets -t accumulo.metadataNUM  TABLET_DIR      FILES WALS  ENTRIES   SIZE      STATUS     LOCATION                       ID    START (Exclusive)    ENDTABLE: accumulo.metadata1    table_info      2     0     7         524       HOSTED     CURRENT:ip-10-113-12-25:9997   !0    -INF                 ~2    default_tablet  0     0     0         0         HOSTED     CURRENT:ip-10-113-12-25:9997   !0    ~                    +INFNew Utility for Generating SplitsA new command line utility was created to generate split points from 1 or more rfiles. One or moreHDFS directories can be given as well. The utility will iterate over all the files provided anddetermine the proper split points based on either the size or number given. It uses ApacheDatasketches to get the split points from the data. #2361 #2368New Option for Cloning OfflineAdded option to leave cloned tables offline #1474 #1475New Max Tablets Option in Bulk ImportThe property table.bulk.max.tablets was created in new bulk import technique. This property actsas a cluster performance failsafe to prevent a single ingested file from being distributed acrosstoo much of a cluster. The value is enforced by the new bulk import technique and is the maximumnumber of tablets allowed for one bulk import file. When this property is set, an error will bethrown when the value is exceeded during a bulk import. #1614New Health Check Thread in TabletServerA new thread was added to the tablet server to periodically verify tablet metadata. #2320This thread also prints to the debug log how long it takes the tserver to scan the metadata table.The property tserver.health.check.interval was added to control the frequency at which this healthcheck takes place. #2583New ability for user to define context classloadersDeprecated the existing VFS ClassLoader for eventual removal and created a new mechanism for usersto load their own classloader implementations. The new VFS classloader and VFS context classloadersare in a new repo and can now be specified using Java’s own systemproperties. Alternatively, one can set their own classloader (they always could do this). #1747 #1715Please reference the Known Issues section of the 2.0.1 release notes for an issue affecting theVFSClassLoader.Change in uncaught Exception/Error handling in server-side threadsConsolidated and normalized thread pool and thread creation. All threads created through this codepath will have an UncaughtExceptionHandler attached to it that will log the fact that the Threadencountered an uncaught Exception and is now dead. When an Error is encountered in a server process,it will attempt to print a message to stderr then terminate the VM using Runtime.halt. On the clientside, the default UncaughtExceptionHandler will only log the Exception/Error in the client and doesnot terminate the VM. Additionally, the user has the ability to set their ownUncaughtExceptionHandler implementation on the client. #1808 #1818 #2554Updated hash algorithmWith the default password Authenticator, Accumulo used to store password hashes using SHA-256 andusing custom code to add a salt. In this release, we now use Apache commons-codec to store passwordhashes in the crypt(3) standard format. With this change, we’ve also defaulted to using thestronger SHA-512. Existing stored password hashes (if upgrading from an earlier version of Accumulo)will automatically be upgraded when users authenticate or change their passwords, and Accumulo willlog a warning if it detects any passwords have not been upgraded. #1787 #1788 #1798 #1810Various Performance improvements when deleting tables  Make delete table operations cancel user compactions #2030 #2169.  Prevent compactions from starting when delete table is called #2182 #2240.  Added check to not flush when table is being deleted #1887.  Added log message before waiting for deletes to finish #1881.  Added code to stop user flush if table is being deleted #1931New Monitor Pages, Improvements &amp;amp; Features  A page was added to the Monitor that lists the active compactions and the longest running activecompaction. As an optimization, this page will only fetch data if a user loads the page and willonly do so a maximum of once a minute. This optimization was also added for the Active Scans page,along with the addition of a “Fetched” column indicating when the data was retrieved.  A new feature was added to the TabletServer page to help users identify which tservers are inrecovery mode. When a tserver is recovering, its corresponding row in the TabletServer Statustable will be highlighted.  A new page was also created for External Compactions that allows users to see the progress ofcompactions and other details about ongoing compactions (see below).#2283 #2294 #2358 #2663New tserver scan timeout propertyThe new property tserver.scan.results.max.timeout was added to allow configuration of the timeout.A bug was discovered where tservers were running out of memory, partially due to this timeout beingso short. The default value is 1 second, but now it can be increased. It is the max time for thethrift client handler to wait for scan results before timing out. #2599 #2598Always choose volumes for new tablet filesIn #1389, we changed the behavior of the VolumeChooser. It now runs any time a new file iscreated. This means VolumeChooser decisions are no longer “sticky” for tablets. This allows tabletsto balance their files across multiple HDFS volumes, instead of the first selected. Now, only thedirectory name is “sticky” for a tablet, but the volume is not. So, new files will appear in adirectory named the same on different volumes that the VolumeChooser selects.Iterators package is now public API#1390 #1400 #1411 We declared that the core.iterators package is publicAPI, so it will now follow the semver rules for public API.Better accumulo-gc memory usage#1543 #1650 Switch from batching file candidates to delete based on the amount ofavailable memory, and instead use a fixed-size batching strategy. This allows the accumulo-gc to runconsistently using a batch size that is configurable by the user. The user is responsbile forensuring the process is given enough memory to accommodate the batch size they configure, but thismakes the process much more consistent and predictable.Log4j2#1528 #1514 #1515 #1516 While we still use slf4j, we haveupgraded the default logger binding to log4j2, which comes with a bunch of features, such as dynamicreconfiguration, colorized console logging, and more.Added forEach method to Scanner#1742 #1765 We added a forEach method to Scanner objects, so you can easilyiterate over the results using a lambda / BiConsumer that accepts a key-value pair.New public API to set multiple properties atomically#2692 We added a new public API added to support setting multiple properties at onceatomically using a read-modify-write pattern. This is available for table, namespace, and systemproperties, and is called modifyProperties(). This builds off a related change that allows us tomore efficiently store and properties in ZooKeeper, which also results in fewer ZooKeeper watches.Simplified cluster configuration#2138 #2903 Modified the accumulo-cluster script to read the server locations from a singlefile, cluster.yaml, in the conf directory instead of multiple files (tserver, manager, gc, etc.). Starting the new scan server and compactor server types is supported using this new file.  It also contains options for starting multiple Tablet and Scan Servers per host.Other notable changes  #1174 #816 Abstract metadata and change root metadata schema  #1309 Explicitly prevent cloning metadata table to prevent poor user experience  #1313 #936 Store Root Tablet list of files in Zookeeper  #1294 #1299 Add optional -t tablename to importdirectory shell command.  #1332 Disable FileSystemMonitor checks of /proc by default (to be removed in future)  #1345 #1352 Optionally disable gc-initiated compactions/flushes  #1397 #1461 Replace relative paths in the metadata tables on upgrade.  #1456 #1457 Prevent catastrophic tserver shutdown by rate limiting the shutdown  #1053 #1060 #1576 Support multiple volumes in import table  #1568 Support multiple tservers / node in accumulo-service  #1644 #1645 Fix issue with minor compaction not retrying  #1660 Dropped unused MemoryManager property  #1764 #1783 Parallelize listcompactions in shell  #1797 Add table option to shell delete command.  #2039 #2045 Add bulk import option to ignore empty dirs  #2117 #2236 Make sorted recovery write to RFiles. New tserver.wal.sort.file.property to configure  #2076 Sorted recovery files can now be encrypted  #2441 Upgraded to Junit 5  #2462 Added SUBMITTED FaTE status to differentiate between things submitted vs. running  #2467 Added fate shell command option to cancel FaTE operations that are NEW or SUBMITTED  #2807 Added several troubleshooting utilities to the accumulo admin command.  #2820 #2900 du command performance improved by using the metadata table forcomputation instead of HDFS  #2966 Upgrade Thrift to 0.17.0UpgradingView the Upgrading Accumulo documentation for guidance.Known IssuesAt the time of release, the following issues were known:  #3045 - External compactions may appear stuck until the coordinator is restarted  #3048 - The monitor may not show times in the correct format for the user’s locale  #3053 - ThreadPool creation is a bit spammy by default in the debug logs  #3057 - The monitor may have an annoying popup on the external compactions page if thecoordinator is offlineUseful Links  All tickets related to this releaseThis release also includes bug fixes from 1.10.2 and earlier, which werereleased after 2.0.0 and 2.0.1.",
      "url": " /release/accumulo-2.1.0/",
      "categories": "release"
    }
    ,
  
    "blog-2022-06-22-2-1-0-metrics-and-tracing-html": {
      "title": "2.1.0 Metrics and Tracing Changes",
      "content": "Metrics and Tracing changed in 2.1.0. This post explains the new implementations and provides examples on how to configure them.MetricsAccumulo was modified in version 1.7.0 (2015) to use the Hadoop Metrics2 framework for capturing and emitting internal Accumulo metrics. Micrometer, a newer metrics framework, supports sending metrics to many popular monitoring systems. In Accumulo 2.1.0 support for the Hadoop Metrics2 framework has been removed in favor of using Micrometer. Metrics are disabled by default.Micrometer has the concept of a MeterRegistry, which is used to create and emit metrics to the supported monitoring systems. Additionally, Micrometer supports sending metrics to multiple monitoring systems concurrently. Configuring Micrometer in Accumulo will require you to write a small peice of code to provide the MeterRegistry configuration. Specifically, you will need to create a class that implements MeterRegistryFactory. Your implementation will need to create and configure the appropriate MeterRegistry. Additionally, you will need to add the MeterRegistry jar file and the jar file containing your MeterRegistryFactory implementation to Accumulo’s classpath. The page for each monitoring system that Micrometer supports contains instructions on how to configure the registry and which jar file is required.Accumulo’s metrics integration test uses a TestStatsDRegistryFactory to create and configure a StatsD Meter Registry. The instructions below provide an example of how to use this class to emit Accumulo’s metrics to a Telegraf - InfluxDB - Grafana monitoring stack.Metrics ExampleThis example uses a Docker container that contains Telegraf-InfluxDB-Grafana system. We will configure Accumulo to send metrics to the Telegraf component running in the Docker image. Telegraf will persist the metrics in InfluxDB and then we will visualize the metrics using Grafana. This example assumes that you have installed Docker (or equivalent engine) and have an Accumulo database already installed and initialized. We will be installing some things, modifying the Accumulo configuration, and starting Accumulo.  Download the Telegraf-Influx-Grafana (TIG) Docker image    docker pull artlov/docker-telegraf-influxdb-grafana:latest        Create directories for the Docker container    mkdir -p /tmp/metrics/influxdbchmod 777 /tmp/metrics/influxdbmkdir /tmp/metrics/grafanamkdir /tmp/metrics/grafana-dashboardsmkdir -p /tmp/metrics/telegraf/conf        Download Telegraf configuration and Grafana dashboard    cd /tmp/metrics/telegraf/confwget https://raw.githubusercontent.com/apache/accumulo-testing/main/contrib/terraform-testing-infrastructure/modules/config-files/templates/telegraf.conf.tftplcat telegraf.conf.tftpl | sed &quot;s/${manager_ip}/localhost/&quot; &amp;gt; telegraf.confcd /tmp/metrics/grafana-dashboardswget https://raw.githubusercontent.com/apache/accumulo-testing/main/contrib/terraform-testing-infrastructure/modules/config-files/files/grafana_dashboards/accumulo-dashboard.jsonwget https://raw.githubusercontent.com/apache/accumulo-testing/main/contrib/terraform-testing-infrastructure/modules/config-files/files/grafana_dashboards/accumulo-dashboard.yaml        Start the TIG Docker container    docker run --ulimit nofile=66000:66000 -d --rm  --name tig-stack  -p 3003:3003  -p 3004:8888  -p 8086:8086  -p 22022:22  -p 8125:8125/udp  -v /tmp/metrics/influxdb:/var/lib/influxdb  -v /tmp/metrics/grafana:/var/lib/grafana  -v /tmp/metrics/telegraf/conf:/etc/telegraf  -v /tmp/metrics/grafana-dashboards:/etc/grafana/provisioning/dashboards  artlov/docker-telegraf-influxdb-grafana:latest        Download Micrometer StatsD Meter Registry jar    wget -O micrometer-registry-statsd-1.9.1.jar https://search.maven.org/remotecontent?filepath=io/micrometer/micrometer-registry-statsd/1.9.1/micrometer-registry-statsd-1.9.1.jar        At a mininum you need to enable the metrics using the property general.micrometer.enabled and supply the name of the MeterRegistryFactory class using the property general.micrometer.factory. To enable JVM metrics, use the property general.micrometer.jvm.metrics.enabled. Modify the accumulo.properties configuration file by adding the properties below.    # Micrometer settingsgeneral.micrometer.enabled=truegeneral.micrometer.jvm.metrics.enabled=truegeneral.micrometer.factory=org.apache.accumulo.test.metrics.TestStatsDRegistryFactory            Copy the micrometer-registry-statsd-1.9.1.jar and accumulo-test.jar into the Accumulo lib directory    The TestStatsDRegistryFactory uses system properties to determine the host and port of the StatsD server. In this example the Telegraf component started in step 4 above contains a StatsD server listening on localhost:8125. Configure the TestStatsDRegistryFactory by adding the following system properties to the JAVA_OPTS variable in accumulo-env.sh.    &quot;-Dtest.meter.registry.host=127.0.0.1&quot;&quot;-Dtest.meter.registry.port=8125&quot;        Start Accumulo.  You should see the following statement in the server log files    [metrics.MetricsUtil] INFO : initializing metrics, enabled:true, class:org.apache.accumulo.test.metrics.TestStatsDRegistryFactory        Log into Grafana (http://localhost:3003/) using the default credentials (root/root). Click the Home icon at the top, then click the Accumulo Micrometer Test Dashboard. If everything is working correctly, then you should see something like the image below.TracingWith the retirement of HTrace, Accumulo has selected to replace it’s tracing functionality with OpenTelemetry in version 2.1.0. Hadoop appears to be on the same path which, when finished, should provide better insight into Accumulo’s use of HDFS. OpenTelemetry supports exporting Trace information to several different systems, to include Jaeger, Zipkin, and others. The HTrace trace spans in the Accumulo source code have been updated to use OpenTelemetry trace spans. If tracing is enabled, then Accumulo will use the OpenTelemetry implementation registered with the GlobalOpenTelemetry object. Tracing is disabled by default and a no-op OpenTelemetry implementation is used.Tracing ExampleThis example uses the OpenTelemetry Java Agent jar file to configure and export trace information to Jaeger. The OpenTelemetry Java Agent jar file bundles together the supported Java exporters, provides a way to configure them, and registers them with the GlobalOpenTelemetry singleton that is used by Accumulo. An alternate method to supplying the OpenTelemetry dependencies, without using the Java Agent jar file, is to create a shaded jar with the OpenTelemetry autoconfigure module and it’s runtime dependencies and place the resulting shaded jar on the classpath. An example Maven pom.xml file to create the shaded jar is here. When using this alternate method you can skip step 2 and the uncommenting of the java agent in step 5 below.  Download Jaeger all-in-one Docker image      docker pull jaegertracing/all-in-one:1.35        Download OpenTelemetry Java Agent (https://github.com/open-telemetry/opentelemetry-java/tree/main/sdk-extensions/autoconfigure)      wget -O opentelemetry-javaagent-1.15.0.jar https://search.maven.org/remotecontent?filepath=io/opentelemetry/javaagent/opentelemetry-javaagent/1.15.0/opentelemetry-javaagent-1.15.0.jar        To enable tracing, you need to set the general.opentelemetry.enabled property. Modify the accumulo.properties configuration file and add the following property.    # OpenTelemetry settingsgeneral.opentelemetry.enabled=true        To enable tracing in the shell, set the general.opentelemetry.enabled property in the accumulo-client.properties configuration file.    # OpenTelemetry settingsgeneral.opentelemetry.enabled=true        Configure the OpenTelemetry JavaAgent in accumulo-env.sh by uncommenting the following and updating the path to the java agent jar:      ## Optionally setup OpenTelemetry SDK AutoConfigure  ## See https://github.com/open-telemetry/opentelemetry-java/tree/main/sdk-extensions/autoconfigure  #JAVA_OPTS=(&#39;-Dotel.traces.exporter=jaeger&#39; &#39;-Dotel.metrics.exporter=none&#39; &#39;-Dotel.logs.exporter=none&#39; &quot;${JAVA_OPTS[@]}&quot;)  ## Optionally setup OpenTelemetry Java Agent  ## See https://github.com/open-telemetry/opentelemetry-java-instrumentation for more options  #JAVA_OPTS=(&#39;-javaagent:path/to/opentelemetry-javaagent.jar&#39; &quot;${JAVA_OPTS[@]}&quot;)        Start Jaeger Docker container    docker run -d --rm --name jaeger  -e COLLECTOR_ZIPKIN_HOST_PORT=:9411  -p 5775:5775/udp  -p 6831:6831/udp  -p 6832:6832/udp  -p 5778:5778  -p 16686:16686  -p 14268:14268  -p 14250:14250  -p 9411:9411 jaegertracing/all-in-one:1.35        Start Accumulo.  You should see the following statement in the server log files    [trace.TraceUtil] INFO : Trace enabled in Accumulo: yes, OpenTelemetry instance: class io.opentelemetry.javaagent.instrumentation.opentelemetryapi.v1_10.ApplicationOpenTelemetry110, Tracer instance: class io.opentelemetry.javaagent.instrumentation.opentelemetryapi.trace.ApplicationTracer        View traces in Jaeger UI at http://localhost:16686. You can select the service name on the left panel and click Find Traces to view the trace information. If everything is working correctly, then you should see something like the image below.",
      "url": " /blog/2022/06/22/2.1.0-metrics-and-tracing.html",
      "categories": "blog"
    }
    ,
  
    "release-accumulo-1-10-2": {
      "title": "Apache Accumulo 1.10.2",
      "content": "AboutApache Accumulo 1.10.2 is a bug fix release of the 1.10 LTM release line.These release notes are highlights of the changes since 1.10.1. The fulldetailed changes can be seen in the git history. If anything important ismissing from this list, please contact us to have it included.Users of 1.10.1 or earlier are encouraged to upgrade to 1.10.2, as this is acontinuation of the 1.10 LTM release line with bug fixes and improvements, andit supersedes any prior 1.x version. Users are also encouraged to considermigrating to a 2.x version when one that is suitable for their needs becomesavailable.Known IssuesApache Commons VFS was upgraded in #1295 and some users have reportedissues similar to VFS-683. Possible solutions are discussed in #2775.Major ImprovementsThis release bundles reload4j (#2458) inthe convenience binary and uses that instead of log4j 1.2. This is to make iteasier for users to avoid the many CVEs that apply to log4j 1.2, which is nolonger being maintained. Accumulo 2.x versions will have already switched touse the latest log4j 2. However, doing so required making some breaking APIchanges and other substantial changes, so that can’t be done for Accumulo 1.10.Using reload4j instead, was deemed to be a viable interim solution untilAccumulo 2.x.Other Improvements  #1808 Re-throw exceptions in threads instead of merely logging them  #1863 Avoid unnecessory redundant log sorting  #1917 Ensure RFileWriterBuilder API validates filenames  #2006 Detect system config changes in HostRegexTableLoadBalancer without restarting master  #2464 Apply timeout to socket.connect()Other Bug Fixes  #1775 Ensure monitor reports a dead tserver when it is killed  #1858 Fix a bug in the monitor graphs due to use of int instead of long  #2370 Fix bug in getsplits command in the shellNote About JDK 15See the note in the 1.10.1 release notes about the use of JDK 15 or later, asthe information pertaining to the use of the CMS garbage collector remainsapplicable to this version.Useful Links  Release VOTE email thread  All Changes since 1.10.1  All tickets related to this release",
      "url": " /release/accumulo-1.10.2/",
      "categories": "release"
    }
    ,
  
    "blog-2021-07-08-external-compactions-html": {
      "title": "External Compactions",
      "content": "External compactions are a new feature in Accumulo 2.1.0 which allowscompaction work to run outside of Tablet Servers.OverviewThere are two types of compactions in Accumulo - Minor and Major. Minorcompactions flush recently written data from memory to a new file. Majorcompactions merge two or more Tablet files together into one new file. Startingin 2.1 Tablet Servers can run multiple major compactions for a Tabletconcurrently; there is no longer a single thread pool per Tablet Server thatruns compactions. Major compactions can be resource intensive and may run for along time depending on several factors, to include the number and size of theinput files, and the iterators configured to run during major compaction.Additionally, the Tablet Server does not currently have a mechanism in place tostop a major compaction that is taking too long or using too many resources.There is a mechanism to throttle the read and write speed of major compactionsas a way to reduce the resource contention on a Tablet Server where manyconcurrent compactions are running. However, throttling compactions on a busysystem will just lead to an increasing amount of queued compactions. Finally,major compaction work can be wasted in the event of an untimely death of theTablet Server or if a Tablet is migrated to another Tablet Server.An external compaction is a major compaction that occurs outside of a TabletServer. The external compaction feature is an extension of the major compactionservice in the Tablet Server and is configured as part of the systemscompaction service configuration. Thus, it is an optional feature. The goal ofthe external compaction feature is to overcome some of the drawbacks of theMajor compactions that happen inside the Tablet Server. Specifically, externalcompactions:  Allow major compactions to continue when the originating TabletServer dies  Allow major compactions to occur while a Tablet migrates to a new Tablet Server  Reduce the load on the TabletServer, giving it more cycles to insert mutations and respond to scans (assuming it’s running on different hosts).  MapReduce jobs and compactions can lower the effectiveness of processor and page caches for scans, so moving compactions off the host can be beneficial.  Allow major compactions to be scaled differently than the number of TabletServers, giving users more flexibility in allocating resources.  Even out hotspots where a few Tablet Servers have a lot of compaction work. External compactions allow this work to spread much wider than previously possible.The external compaction feature in Apache Accumulo version 2.1.0 adds two newsystem-level processes and new configuration properties. The new system-levelprocesses are the Compactor and the Compaction Coordinator.  The Compactor is a process that is responsible for executing a major compaction. There can be many Compactor’s running on a system. The Compactor communicates with the Compaction Coordinator to get information about the next major compaction it will run and to report the completion state.  The Compaction Coordinator is a single process like the Manager. It is responsible for communicating with the Tablet Servers to gather information about queued external compactions, to reserve a major compaction on the Compactor’s behalf, and to report the completion status of the reserved major compaction.  For external compactions that complete when the Tablet is offline, the Compaction Coordinator buffers this information and reports it later.DetailsBefore we explain the implementation for external compactions, it’s probablyuseful to explain the changes for major compactions that were made in the 2.1.0branch before external compactions were added. This is most apparent in thetserver.compaction.major.service and table.compaction.dispatcher configurationproperties. The simplest way to explain this is that you can now define aservice for executing compactions and then assign that service to a table(which implies you can have multiple services assigned to different tables).This gives the flexibility to prevent one table’s compactions from impactinganother table. Each service has named thread pools with size thresholds.ConfigurationThe configuration below defines a compaction service named cs1 usingthe DefaultCompactionPlanner that is configured to have three named threadpools (small, medium, and large). Each thread pool is configured with a numberof threads to run compactions and a size threshold. If the sum of the inputfile sizes is less than 16MB, then the major compaction will be assigned to thesmall pool, for example.tserver.compaction.major.service.cs1.planner=org.apache.accumulo.core.spi.compaction.DefaultCompactionPlannertserver.compaction.major.service.cs1.planner.opts.executors=[{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;16M&quot;,&quot;numThreads&quot;:8},{&quot;name&quot;:&quot;medium&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;128M&quot;,&quot;numThreads&quot;:4},{&quot;name&quot;:&quot;large&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;numThreads&quot;:2}]To assign compaction service cs1 to the table ci, you would use the following properties:config -t ci -s table.compaction.dispatcher=org.apache.accumulo.core.spi.compaction.SimpleCompactionDispatcherconfig -t ci -s table.compaction.dispatcher.opts.service=cs1A small modification to thetserver.compaction.major.service.cs1.planner.opts.executors property in theexample above would enable it to use external compactions. For example, let’ssay that we wanted all of the large compactions to be done externally, youwould use this configuration:tserver.compaction.major.service.cs1.planner.opts.executors=[{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;16M&quot;,&quot;numThreads&quot;:8},{&quot;name&quot;:&quot;medium&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;128M&quot;,&quot;numThreads&quot;:4},{&quot;name&quot;:&quot;large&quot;,&quot;type&quot;:&quot;external&quot;,&quot;queue&quot;:&quot;DCQ1&quot;}]&#39;In this example the queue DCQ1 can be any arbitrary name and allows you todefine multiple pools of Compactor’s.Behind these new configurations in 2.1 lies a new algorithm for choosing whichfiles to compact.  This algorithm attempts to find the smallest set of filesthat meets the compaction ratio criteria. Prior to 2.1, Accumulo looked for thelargest set of files that met the criteria.  Both algorithms do logarithmicamounts of work.  The new algorithm better utilizes multiple thread poolsavailable for running comactions of different sizes.CompactorA Compactor is started with the name of the queue for which it will completemajor compactions. You pass in the queue name when starting the Compactor, likeso:bin/accumulo compactor -q DCQ1Once started the Compactor tries to find the location of theCompaction Coordinator in ZooKeeper and connect to it. Then, it asks theCompaction Coordinator for the next compaction job for the queue. TheCompaction Coordinator will return to the Compactor the necessary information torun the major compaction, assuming there is work to be done. Note that theclass performing the major compaction in the Compactor is the same one used inthe Tablet Server, so we are just transferring all of the input parameters fromthe Tablet Server to the Compactor. The Compactor communicates information backto the Compaction Coordinator when the compaction has started, finished(successfully or not), and during the compaction (progress updates).Compaction CoordinatorThe Compaction Coordinator is a singleton process in the system like theManager. Also, like the Manager it supports standby Compaction Coordinator’susing locks in ZooKeeper. The Compaction Coordinator is started using thecommand:bin/accumulo compaction-coordinatorWhen running, the Compaction Coordinator polls the TabletServers for summaryinformation about their external compaction queues. It keeps track of the majorcompaction priorities for each Tablet Server and queue. When a Compactorrequests the next major compaction job the Compaction Coordinator finds theTablet Server with the highest priority major compaction for that queue andcommunicates with that Tablet Server to reserve an external compaction. Thepriority in this case is an integer value based on the number of input filesfor the compaction. For system compactions, the number is negative starting at-32768 and increasing to -1 and for user compactions it’s a non-negative numberstarting at 0 and limited to 32767. When the Tablet Server reserves theexternal compaction an entry is written into the metadata table row for theTablet with the address of the Compactor running the compaction and all of theconfiguration information passed back from the Tablet Server. Below is anexample of the ecomp metadata column:2;10ba2e8ba2e8ba5 ecomp:ECID:94db8374-8275-4f89-ba8b-4c6b3908bc50 []    {&quot;inputs&quot;:[&quot;hdfs://accucluster/accumulo/tables/2/t-00000ur/A00001y9.rf&quot;,&quot;hdfs://accucluster/accumulo/tables/2/t-00000ur/C00005lp.rf&quot;,&quot;hdfs://accucluster/accumulo/tables/2/t-00000ur/F0000dqm.rf&quot;,&quot;hdfs://accucluster/accumulo/tables/2/t-00000ur/F0000dq1.rf&quot;],&quot;nextFiles&quot;:[],&quot;tmp&quot;:&quot;hdfs://accucluster/accumulo/tables/2/t-00000ur/C0000dqs.rf_tmp&quot;,&quot;compactor&quot;:&quot;10.2.0.139:9133&quot;,&quot;kind&quot;:&quot;SYSTEM&quot;,&quot;executorId&quot;:&quot;DCQ1&quot;,&quot;priority&quot;:-32754,&quot;propDels&quot;:true,&quot;selectedAll&quot;:false}When the Compactor notifies the Compaction Coordinator that it has finished themajor compaction, the Compaction Coordinator attempts to notify the TabletServer and inserts an external compaction final state marker into the metadatatable. Below is an example of the final state marker:~ecompECID:de6afc1d-64ae-4abf-8bce-02ec0a79aa6c : []        {&quot;extent&quot;:{&quot;tableId&quot;:&quot;2&quot;},&quot;state&quot;:&quot;FINISHED&quot;,&quot;fileSize&quot;:12354,&quot;entries&quot;:100000}If the Compaction Coordinator is able to reach the Tablet Server and that TabletServer is still hosting the Tablet, then the compaction is committed and bothof the entries are removed from the metadata table. In the case that the Tabletis offline when the compaction attempts to commit, there is a thread in theCompaction Coordinator that looks for completed, but not yet committed, externalcompactions and periodically attempts to contact the Tablet Server hosting theTablet to commit the compaction. The Compaction Coordinator periodically removesthe final state markers related to Tablets that no longer exist. In the case ofan external compaction failure the Compaction Coordinator notifies the Tabletand the Tablet cleans up file reservations and removes the metadata entry.Edge CasesThere are several situations involving external compactions that we tested as part of this feature. These are:  Tablet migration  When a user initiated compaction is canceled  What a Table is taken offline  When a Tablet is split or merged  Coordinator restart  Tablet Server death  Table deletionCompactors periodically check if the compaction they are running is related toa deleted table, split/merged Tablet, or canceled user initiated compaction. Ifany of these cases happen the Compactor interrupts the compaction and notifiesthe Compaction Coordinator. An external compaction continues in the case ofTablet Server death, Tablet migration, Coordinator restart, and the Table beingtaken offline.Cluster TestThe following tests were run on a cluster to exercise this new feature.  Run continuous ingest for 24h with large compactions running externally in an autoscaled Kubernetes cluster.  After ingest completion, started a full table compaction with all compactions running externally.  Run continuous ingest verification process that looks for lost data.SetupFor these tests Accumulo, Zookeeper, and HDFS were run on a cluster in Azuresetup by Muchos and external compactions were run in a separate Kubernetescluster running in Azure.  The Accumulo cluster had the followingconfiguration.  Centos 7  Open JDK 11  Zookeeper 3.6.2  Hadoop 3.3.0  Accumulo 2.1.0-SNAPSHOT dad7e01  23 D16s_v4 VMs, each with 16x128G HDDs stripped using LVM. 22 were workers.The following diagram shows how the two clusters were setup.  The Muchos andKubernetes clusters were on the same private vnet, each with its own /16 subnetin the 10.x.x.x IP address space.  The Kubernetes cluster that ran externalcompactions was backed by at least 3 D8s_v4 VMs, with VMs autoscaling with thenumber of pods running.One problem we ran into was communication between Compactors running insideKubernetes with processes like the Compaction Coordinator and DataNodes runningoutside of Kubernetes in the Muchos cluster.  For some insights into how theseproblems were overcome, checkout the comments in the deploymentspec used.ConfigurationThe following Accumulo shell commands set up a new compaction service namedcs1.  This compaction service has an internal executor with 4 threads namedsmall for compactions less than 32M, an internal executor with 2 threads namedmedium for compactions less than 128M, and an external compaction queue namedDCQ1 for all other compactions.config -s &#39;tserver.compaction.major.service.cs1.planner.opts.executors=[{&quot;name&quot;:&quot;small&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;32M&quot;,&quot;numThreads&quot;:4},{&quot;name&quot;:&quot;medium&quot;,&quot;type&quot;:&quot;internal&quot;,&quot;maxSize&quot;:&quot;128M&quot;,&quot;numThreads&quot;:2},{&quot;name&quot;:&quot;large&quot;,&quot;type&quot;:&quot;external&quot;,&quot;queue&quot;:&quot;DCQ1&quot;}]&#39;config -s tserver.compaction.major.service.cs1.planner=org.apache.accumulo.core.spi.compaction.DefaultCompactionPlannerThe continuous ingest table was configured to use the above compaction service.The table’s compaction ratio was also lowered from the default of 3 to 2.  Alower compaction ratio results in less files per Tablet and more compactionwork.config -t ci -s table.compaction.dispatcher=org.apache.accumulo.core.spi.compaction.SimpleCompactionDispatcherconfig -t ci -s table.compaction.dispatcher.opts.service=cs1config -t ci -s table.compaction.major.ratio=2The Compaction Coordinator was manually started on the Muchos VM where theAccumulo Manager, Zookeeper server, and the Namenode were running. Thefollowing command was used to do this.nohup accumulo compaction-coordinator &amp;gt;/var/data/logs/accumulo/compaction-coordinator.out 2&amp;gt;/var/data/logs/accumulo/compaction-coordinator.err &amp;amp;To start Compactors, Accumulo’sdocker image wasbuilt from the next-release branch by checking out the Apache Accumulo gitrepo at commit dad7e01 and building the binary distribution using thecommand mvn clean package -DskipTests. The resulting tar file was copied tothe accumulo-docker base directory and the image was built using the command:docker build --build-arg ACCUMULO_VERSION=2.1.0-SNAPSHOT --build-arg ACCUMULO_FILE=accumulo-2.1.0-SNAPSHOT-bin.tar.gz              --build-arg HADOOP_FILE=hadoop-3.3.0.tar.gz              --build-arg ZOOKEEPER_VERSION=3.6.2  --build-arg ZOOKEEPER_FILE=apache-zookeeper-3.6.2-bin.tar.gz               -t accumulo .The Docker image was tagged and then pushed to a container registry accessible byKubernetes. Then the following commands were run to start the Compactors usingaccumulo-compactor-muchos.yaml.The yaml file contains comments explaining issues related to IP addresses and DNS names.kubectl apply -f accumulo-compactor-muchos.yamlkubectl autoscale deployment accumulo-compactor --cpu-percent=80 --min=10 --max=660The autoscale command causes Compactors to scale between 10and 660 pods based on CPU usage. When pods average CPU is above 80%, thenpods are added to meet the 80% goal. When it’s below 80%, podsare stopped to meet the 80% goal with 5 minutes between scale downevents. This can sometimes lead to running compactions beingstopped. During the test there were ~537 dead compactions that were probablycaused by this (there were 44K successful external compactions). The max of 660was chosen based on the number of datanodes in the Muchos cluster.  There were22 datanodes and 30x22=660, so this conceptually sets a limit of 30 externalcompactions per datanode.  This was well tolerated by the Muchos cluster.  Oneimportant lesson we learned is that external compactions can strain the HDFSDataNodes, so it’s important to consider how many concurrent externalcompactions will be running. The Muchos cluster had 22x16=352 cores on theworker VMs, so the max of 660 exceeds what the Muchos cluster could run itself.Ingesting dataAfter starting Compactors, 22 continuous ingest clients (fromaccumulo-testing) were started.  The following plot shows the number ofcompactions running in the three different compaction queuesconfigured.  The executor cs1_small is for compactions &amp;lt;= 32M and it stayedpretty busy as minor compactions constantly produce new small files.  In 2.1.0merging minor compactions were removed, so it’s important to ensure acompaction queue is properly configured for new small files. The executorcs1_medium was for compactions &amp;gt;32M and &amp;lt;=128M and it was not as busy, but didhave steady work.  The external compaction queue DCQ1 processed all compactionsover 128M and had some spikes of work.  These spikes are to be expected withcontinuous ingest as all Tablets are written to evenly and eventually all ofthe Tablets need to run large compactions around the same time.The following plot shows the number of pods running in Kubernetes.  AsCompactors used more and less CPU the number of pods automatically scaled upand down.The following plot shows the number of compactions queued.  When thecompactions queued for cs1_small spiked above 750, it was adjusted from 4threads per Tablet Server to 6 threads.  This configuration change was made whileeverything was running and the Tablet Servers saw it and reconfigured their threadpools on the fly.The metrics emitted by Accumulo for these plots had the following names.  TabletServer1.tserver.compactionExecutors.e_DCQ1_queued  TabletServer1.tserver.compactionExecutors.e_DCQ1_running  TabletServer1.tserver.compactionExecutors.i_cs1_medium_queued  TabletServer1.tserver.compactionExecutors.i_cs1_medium_running  TabletServer1.tserver.compactionExecutors.i_cs1_small_queued  TabletServer1.tserver.compactionExecutors.i_cs1_small_runningTablet servers emit metrics about queued and running compactions for everycompaction executor configured.  User can observe these metrics and tunethe configuration based on what they see, as was done in this test.The following plot shows the average files per Tablet during thetest. The numbers are what would be expected for a compaction ratio of 2 whenthe system is keeping up with compaction work. Also, animated GIFs were created toshow a few tablets files over time.The following is a plot of the number Tablets during the test.Eventually there were 11.28K Tablets around 512 Tablets per Tablet Server.  TheTablets were close to splitting again at the end of the test as each Tablet wasgetting close to 1G.The following plot shows ingest rate over time.  The rate goes down as thenumber of Tablets per Tablet Server goes up, this is expected.The following plot shows the number of key/values in Accumulo duringthe test.  When ingest was stopped, there were 266 billion key values in thecontinuous ingest table.Full table compactionAfter stopping ingest and letting things settle, a full table compaction waskicked off. Since all of these compactions would be over 128M, all of them werescheduled on the external queue DCQ1.  The two plots below show compactionsrunning and queued for the ~2 hours it took to do the compaction. When thecompaction was initiated there were 10 Compactors running in pods.  All 11KTablets were queued for compaction and because the pods were always runninghigh CPU Kubernetes kept adding pods until the max was reached resulting in 660Compactors running until all the work was done.VerificationAfter running everything mentioned above, the continuous ingest verificationmap reduce job was run.  This job looks for holes in the linked list producedby continuous ingest which indicate Accumulo lost data.  No holes were found.The counts below were emitted by the job.  If there were holes a non-zeroUNDEFINED count would be present.        org.apache.accumulo.testing.continuous.ContinuousVerify$Counts                REFERENCED=266225036149                UNREFERENCED=22010637HurdlesHow to Scale UpWe ran into several issues running the Compactors in Kubernetes. First, we knewthat we could use Kubernetes Horizontal Pod Autoscaler (HPA) to scale theCompactors up and down based on load. But the question remained how to do that.Probably the best metric to use for scaling the Compactors is the size of theexternal compaction queue. Another possible solution is to take the DataNodeutilization into account somehow. We found that in scaling up the Compactorsbased on their CPU usage we could overload DataNodes.  Once DataNodes wereoverwhelmed, Compactors CPU would drop and the number of pods would naturallyscale down.To use custom metrics you would need to get the metrics from Accumulo into ametrics store that has a metrics adapter. One possible solution, availablein Hadoop 3.3.0, is to use Prometheus, the Prometheus Adapter, and enablethe Hadoop PrometheusMetricsSink added inHADOOP-16398 to expose the custom queuesize metrics. This seemed like the right solution, but it also seemed like alot of work that was outside the scope of this blog post. Ultimately we decidedto take the simplest approach - use the native Kubernetes metrics-server andscale off CPU usage of the Compactors. As you can see in the “Compactions Queued”and “Compactions Running” graphs above from the full table compaction, it took about45 minutes for Kubernetes to scale up Compactors to the maximum configured (660). Compactorslikely would have been scaled up much faster if scaling was done off the queued compactionsinstead of CPU usage.Gracefully Scaling DownThe Kubernetes Pod termination process provides a mechanism for the user todefine a pre-stop hook that will be called before the Pod is terminated.Without this hook Kubernetes sends a SIGTERM to the Pod, followed by auser-defined grace period, then a SIGKILL. For the purposes of this test we didnot define a pre-stop hook or a grace period. It’s likely possible to handlethis situation more gracefully, but for this test our Compactors were killedand the compaction work lost when the HPA decided to scale down the Compactors.It was a good test of how we handled failed Compactors.  Investigation isneeded to determine if changes are needed in Accumulo to facilitate gracefulscale down.How to ConnectThe other major issue we ran into was connectivity between the Compactors andthe other server processes. The Compactor communicates with ZooKeeper and theCompaction Coordinator, both of which were running outside of Kubernetes.  Thereis no common DNS between the Muchos and Kubernetes cluster, but IPs werevisible to both. The Compactor connects to ZooKeeper to find the address of theCompaction Coordinator so that it can connect to it and look for work. Bydefault the Accumulo server processes use the hostname as their address whichwould not work as those names would not resolve inside the Kubernetes cluster.We had to start the Accumulo processes using the -a argument and set thehostname to the IP address. Solving connectivity issues between componentsrunning in Kubernetes and components external to Kubernetes depends on the capabilitiesavailable in the environment and the -a option may be part of the solution.ConclusionIn this blog post we introduced the concept and benefits of externalcompactions, the new server processes and how to configure the compactionservice. We deployed a 23-node Accumulo cluster using Muchos with a variablesized Kubernetes cluster that dynamically scaled Compactors on 3 to 100 computenodes from 10 to 660 instances. We ran continuous ingest on the Accumulocluster to create compactions that were run both internal and external to theTablet Server and demonstrated external compactions completing successfully andCompactors being killed.We discussed also running the following test, but did not have time.  Agitating the Compaction Coordinator, Tablet Servers and Compactors while ingest was running.  Comparing the impact on queries for internal vs external compactions.  Having multiple external compaction queues, each with its own set of autoscaled Compactor pods.  Forcing full table compactions while ingest was running.The test we ran shows that basic functionality works well, it would be nice tostress the feature in other ways though.",
      "url": " /blog/2021/07/08/external-compactions.html",
      "categories": "blog"
    }
    ,
  
    "blog-2021-04-21-jshell-accumulo-feature-html": {
      "title": "Jshell Accumulo Feature",
      "content": "OverviewFirst introduced in Java 9, JShell is an interactive Read-Evaluate-Print-Loop (REPL)Java tool that interprets user’s input and outputs the results. This tool provides a convenientway to test out and execute quick tasks with Accumulo in the terminal. This feature is a partof the upcoming Accumulo 2.1 release. If you’re a developer and want to get involved in testing,contact us or review our contributing guide.Major Features      Default JShell script provides initial imports for interacting with Accumulo’s API andprovided in Accumulo’s binary distribution tarball        On startup, JShell Accumulo  will automatically import the CLASSPATH, load in a configuredenvironment from user’s conf/accumulo-env.sh, and invoke conf/jshell-init.jshto allow rapid Accumulo task executions        JShell Accumulo can startup using default/custom JShell script and users can append any JShellcommand-line options to the startup command  Booting Up JShell Accumulo1) Open up a terminal and navigate to Accumulo’s installation directory2) To startup JShell with default script use this command:$ bin/accumulo jshell3) To startup JShell with custom script use this command:$ bin/accumulo jshell --startup file/path/to/custom_script.jshNote: One can execute the jshell command to startup JShell. However, doing so will requiremanually importing the CLASSPATH and the configured environment from conf/accumulo-env.shand manually specifying the startup file for conf/jshell-init.jsh before any Accumulo taskscan be performed. Using one of the startup commands above will automate that processfor convenience.JShell Accumulo Default ScriptThe auto-generated jshell-init.jsh is a customizable file located in Accumulo’s installationconf/ directory. Inside, jshell-init.jsh contains Accumulo Java APIsformatted as import statements and AccumuloClient build implementation. On startup,the script automatically loads in the APIs and attempts to construct a client. Should additionalAPIs and/or code implementations be needed, simply append them to jshell-init.jsh.Alternatively, you can create a separate JShell script and specify the custom script’s file pathon startup.To construct an AccumuloClient, the provided conf/jshell-init.jsh script findsand uses accumulo-client.properties in Accumulo’s class path, and assigns the resultto a variable called client.If accumulo-client.properties is found, a similar result will be produced below:Preparing JShell for Apache AccumuloBuilding Accumulo client using &#39;jar:file:/home/accumulo/lib/accumulo-client.jar!/accumulo-client.properties&#39;Use &#39;client&#39; to interact with Accumulo|  Welcome to JShell -- Version 11.0.10|  For an introduction type: /help introjshell&amp;gt;If accumulo-client.properties is not found, an AccumuloClient will notauto-generate and will produce the following result below:Preparing JShell for Apache Accumulo&#39;accumulo-client.properties&#39; was not found on the classpath|  Welcome to JShell -- Version 11.0.10|  For an introduction type: /help introjshell&amp;gt;JShell Accumulo Example1) Booting up JShell Accumulo using default scriptPreparing JShell for Apache AccumuloBuilding Accumulo client using &#39;file:/home/accumulo/conf/accumulo-client.properties&#39;Use &#39;client&#39; to interact with Accumulo|  Welcome to JShell -- Version 11.0.10|  For an introduction type: /help introjshell&amp;gt;2) Providing JShell with an Accumulo task  // Create a table called &quot;GothamPD&quot;.  client.tableOperations().create(&quot;GothamPD&quot;);  // Create a Mutation object to hold all changes to a row in a table.  // Each row has a unique row ID.  Mutation mutation = new Mutation(&quot;id0001&quot;);  // Create key/value pairs for Batman. Put them in the &quot;hero&quot; family.  mutation.put(&quot;hero&quot;, &quot;alias&quot;, &quot;Batman&quot;);  mutation.put(&quot;hero&quot;, &quot;name&quot;, &quot;Bruce Wayne&quot;);  mutation.put(&quot;hero&quot;, &quot;wearsCape?&quot;, &quot;true&quot;);  // Create a BatchWriter to the GothamPD table and add your mutation to it.  // Try w/ resources will close for us.  try (BatchWriter writer = client.createBatchWriter(&quot;GothamPD&quot;)) {      writer.addMutation(mutation);  }  // Read and print all rows of the &quot;GothamPD&quot; table.  // Try w/ resources will close for us.  try (ScannerBase scan = client.createScanner(&quot;GothamPD&quot;, Authorizations.EMPTY)) {    System.out.println(&quot;Gotham Police Department Persons of Interest:&quot;);    // A Scanner is an extension of java.lang.Iterable so behaves just like one.    scan.forEach((k, v) -&amp;gt; System.out.printf(&quot;Key : %-50s Value : %sn&quot;, k, v));  }Note: The fully-qualified class name for Accumulo Scanner ororg.apache.accumulo.core.client.Scanner needs to be used due to conflicting issues withJava’s built-in java.util.Scanner. However, to shorten the Accumulo Scanner’s declaration, assignscan to ScannerBase type instead.3) Executing the Accumulo task above outputs:mutation ==&amp;gt; org.apache.accumulo.core.data.Mutation@1Gotham Police Department Persons of Interest:Key : id0001 hero:alias [] 1618926204602 false            Value : BatmanKey : id0001 hero:name [] 1618926204602 false             Value : Bruce WayneKey : id0001 hero:wearsCape? [] 1618926204602 false       Value : truejshell&amp;gt;",
      "url": " /blog/2021/04/21/jshell-accumulo-feature.html",
      "categories": "blog"
    }
    ,
  
    "release-accumulo-2-0-1": {
      "title": "Apache Accumulo 2.0.1",
      "content": "Apache Accumulo 2.0.1 contains bug fixes for 2.0.0.Since 2.0 is a non-LTM release line, and since an LTM release line has not yetbeen made available for 2.x, this patch backports critical bug fixes to 2.0 toaddress security bug CVE-2020-17533 that could affect any existing 2.0.0users. Users that have already migrated to 2.0.0 are urged to upgrade to 2.0.1as soon as possible, and users of 1.10 who wish to upgrade to 2.0 shouldupgrade directly to 2.0.1, bypassing 2.0.0.These release notes are highlights of the changes since 2.0.0. The fulldetailed changes can be seen in the git history. If anything is missing fromthis list, please contact us to have it included.Known IssuesApache Commons VFS was upgraded in #1295 and some users have reportedissues similar to VFS-683. Possible solutions are discussed in #2775.Critical Bug FixesThis release includes critical bug fixes to fix security bugs identified asCVE-2020-17533:  #1828, #1832 Throw exceptions when permission checks fail,and improve test coverage for permissions checksOther Bug Fixes  #1839 Fix AccumuloClient’s builder to prevent it from modifying aprovided Properties object when building a client from PropertiesNote About Newer JDK Versions (11 and later)While work has been done on other branches to better support newer JDKversions, that is not the case for this non-LTM release. Certain non-criticalaspects of this release are known to break with some newer versions of JDK.For example, the version of maven-javadoc-plugin may fail to generate thejavadocs using a newer JDK’s javadoc tool. In addition, this release assumesthe use of the CMS garbage collector in its build tests and in minicluster.Newer JDKs, where CMS has been removed, may fail to execute Accumulo buildtests in this release.Therefore, it is recommended to use JDK 8 or JDK 11 with this release, whichare known to work.Note About ZooKeeper Versions 3.5 and LaterThis release assumes the use of ZooKeeper 3.4. While work has been done onother branches to better support newer ZooKeeper versions (3.5 and later), thisis a targeted release to fix specific bugs and does not include those kinds ofimprovements.Therefore, in order to use this release with ZooKeeper versions 3.5 and later,you may need to edit your default class path, or perform other minor changes towork smoothly with those versions of ZooKeeper. Please contact us if you needassistance working with newer versions of ZooKeeper.UpgradingView the Upgrading Accumulo documentation for guidance.Useful Links  Release VOTE email thread  All Changes since 2.0.0  All tickets related to this releaseThis release does not include other bug fixes from 1.10.0 and 1.10.1, whichwere released after 2.0.0.",
      "url": " /release/accumulo-2.0.1/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-10-1": {
      "title": "Apache Accumulo 1.10.1",
      "content": "AboutApache Accumulo 1.10.1 is a bug fix release of the 1.10 LTM release line.These release notes are highlights of the changes since 1.10.0. The fulldetailed changes can be seen in the git history. If anything is missing fromthis list, please contact us to have it included.Users of 1.10.0 or earlier are urged to upgrade to 1.10.1 as soon as possible,as this is a continuation of the 1.10 LTM release line with critical bug fixesfor security bug CVE-2020-17533. Users are also encouraged to considermigrating to a 2.x version when one that is suitable for their needs becomesavailable.Critical Bug FixesThis release includes critical bug fixes to fix security bugs identified asCVE-2020-17533:  #1830, #1832 Throw exceptions when permission checks fail,and improve test coverage for permissions checks (backport of #1828)Other Bug Fixes  #1716, #1729, #1737 Improvements in tool.sh,including better support for newer ZooKeeper and Hadoop versions  #1829 Improve log message in Delete Cleanup FATE  #1734 Support building native libraries on alpine-based distrosNote About JDK 15Accumulo 1.x assumes the use of the CMS garbage collector in its build testsand in the minicluster code. That garbage collector was removed in newerversions of Java, and the build flags for Java that supported configuring theCMS garbage collector now cause errors if attempted to be used with Java 15 orlater.Therefore, a change was made in 1.10.1’s build to fail fast if attempting tobuild with JDK 15 or later (using JDK 11 or later was already a buildrequirement).If you need to build on JDK 15 or later, and intend to skip tests and don’tintend to use minicluster, you can bypass this build constraint by buildingwith -Denforcer.skip, as a workaround.Useful Links  Release VOTE email thread  All Changes since 1.10.0  All tickets related to this release",
      "url": " /release/accumulo-1.10.1/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-10-0": {
      "title": "Apache Accumulo 1.10.0",
      "content": "AboutApache Accumulo 1.10.0 is a continuation of the 1.x release line, and isessentially the next maintenance release of 1.8/1.9, following the 1.9.3version with some small additional internal improvements. Earlier 1.x versionsare now superseded by this maintenance release, and will no longer bemaintained.The semver minor version number increase (1.9 to 1.10) signals that thisrelease is backwards compatible with previous minor releases (1.8 and 1.9).Rather than API additions, the primary reason for this minor version increaseis due to the decision to make Java 8 the minimum supported Java version (seebelow for more).This release contains contributions from more than 13 contributors from theApace Accumulo community in over 80 commits and 16 months of work since the1.9.3 release. These release notes are highlights of those changes. The fulldetailed changes can be seen in the git history. If anything is missing fromthis list, please contact us to have it included.According to the Long Term Maintenance (LTM) strategy, the intent is tomaintain the 1.10 release line with critical bug and security fixes until oneyear after the next LTM version is released. However, this is anticipated to bethe final 1.x legacy release, so it is not expected to receive any new featuresor significant non-critical updates, so users wanting new features should planto upgrade to a 2.x release, where new feature development is still being done.Users of 1.9.3 or earlier are urged to upgrade to 1.10.0 as soon as it isavailable, as this is a continuation of the 1.9 maintenance line. and toconsider migrating to a 2.x version when a suitable one becomes available.Accumulo 2.0.0 is currently available, and 2.1.0 is anticipated to be the nextLTM release. If you would like to start preparing for 2.1.0 now, one way to dothis is to start building and testing the next version of your software againstAccumulo 2.0.0 because all 2.x releases should be backwards compatible with2.0.0, following semantic versioning.Minimum RequirementsThe versions mentioned here are a guide only. It is not expected that ourconvenience binary tarball will work out-of-the-box with your particularenvironment, and some responsibility is placed on users to properly configureAccumulo, or even patch and rebuild it from source, for their particularenvironment.Please contact us or file a bug report if you have trouble with aspecific version or wish to seek tips. Be prepared to provide details of theproblems you encounter, as well as perform some troubleshooting steps of yourown, in order to get the best response.Java 8Java 8 is now the minimum supported Java version, and it is designed to work onJava 11, as well. To build the project from source, Java 11 or later isrequired. Please contact us if you find any bugs on any Java version.Hadoop 2 or 3This release has been built using Hadoop 2.6.5, and is expected to work withany Hadoop version 2.6.5 or later. It has also been tested with 3.0.3, and isexpected to work with Hadoop 3.0 versions as well. Hadoop 3.1.3, 3.2.1, and3.3.0 have also been tested with this version, and are known to work (with atleast basic functionality) with some class path modifications (specifically,using Guava 27.0-jre instead of the provided 14.0 version).Particular class path pain points are known to be guava, commons-io,commons-vfs2, and possibly other commons libraries.ZooKeeperThis release has been built agains ZooKeeper 3.4.14, the latest 3.4 release. Itis known to work against 3.5 and 3.6 versions as well, when configuredproperly.Major Bug FixesAccumulo GC Bug  #1314, #1318 Eliminate task creation leak caused by the anadditional timed-task created for each accumulo-gc cycleBulk Import Concurrency Bug  #1153 Prevent multiple threads from working on same bulk filePrevent Metadata Corruption  #1309 Prevent cloning of the metadata table, which could lead todata loss during accumulo-gc for either the clone or the originalaccumulo.metadata table  #1310 Improve GC handling of WALs used by root tablet. If the roottablet had WALs, the GC did not consider them during collection  #1379 During GC scans, an error will be thrown if the GC failsconsistency checks; added a check to ensure the last tablet was seenOther Miscellaneous Bug Fixes  #1107 Fix ConcurrentModificationException inHostRegexTableLoadBalancer  #1185 Fixed a bug where we were using an unauthenticated ZooKeeperclient to try to read data with an ACL configured; this was previouslypermitted until ZooKeeper fixed a security bug in their own code, whichrevealed our incorrect ZooKeeper client code  #1371 Fix a bug in our MapReduce code that prevented some users fromreading tables they had valid permissions to read  #1401 Display trace information correctly in accumulo-monitor  #1478 Don’t ignore the instance and zookeepers parameters on thecommand-line when running certain utilities  #1532 Remove need for ANT on classpath  #1555 Fix idempotency bug in importtable  #1644 Retry minor compactions to prevent transient iterator issuesblocking foreverMajor ImprovementsPerformance Enhancements  #990 Avoid multiple threads loading same cache block  #1352 Add an option to configure the metadata action after anaccumulo-gc cycle using a new property instead of a hard-coded compaction          gc.post.metadata.action        #1462, #1526 Temporarily cache the existence check forrecovery WALs, so multiple tablets pointing to the same WAL to avoidexpensive redundant checksIdentifying Busy Tablets  #1291, #1296 Log busy tablets by ingest and query atconfigurable intervals for better hot-spot detection using new properties          tserver.log.busy.tablets.count and      tserver.log.busy.tablets.interval      TServer Startup and Shutdown Protections  #1158 Require a configurable number of servers to be online, up to amax wait time, before assignments begin on startup          master.startup.tserver.avail.min.count      master.startup.tserver.avail.max.wait        #1456 Throttle the number of shutdown requests sent to tservers toprevent cluster self-destruction and give time for triageNew Metrics  #1406 Add GC cycle metrics (file and wal collection) to be reportedvia the hadoop2 metrics. This exposes the gc cycle metrics available in themonitor to external metrics systems and includes run time for the new gc postoperation (compact, flush)          Enable with new property, gc.metrics.enabled                  AccGcCandidates - number of candidates for GC          AccGcDeleted - number of candidates deleted          AccGcErrors - number of deletion errors          AccGcFinished - timestamp of GC cycle finished          AccGcInUse - number of candidates still in use          AccGcPostOpDuration - duration of compact / flush          AccGcRunCycleCount - 1-up cycle count          AccGcStarted - timestamp of GC cycle start          AccGcWalCandidates - number of WAL candidates for collection          AccGcWalDeleted - number of WALs deleted          AccGcWalErrors - number of errors during WAL deletion          AccGcWalFinished - timestamp of WAL collection completion          AccGcWalInUse - number of WALs in use          AccGcWalStarted - timestamp of WAL collection start                    Other Miscellaneous Improvements  #1108 Improve logging when ZooKeeper session expires  #1299 Add optional -t tablename to importdirectory shell command  #1338 Reduce verbose logging of merge operations in Master log  #1475 Option to leave cloned tables offline on creation  #1503 Support ZooKeeper 3.5 (and later), in addition to 3.4Useful Links  Release VOTE email thread  All Changes since 1.9.3  All tickets related to this release",
      "url": " /release/accumulo-1.10.0/",
      "categories": "release"
    }
    ,
  
    "blog-2020-02-26-accumulo-spark-connector-html": {
      "title": "Microsoft MASC, an Apache Spark connector for Apache Accumulo",
      "content": "OverviewMASC provides an Apache Spark native connector for Apache Accumulo to integrate the rich Spark machine learning eco-system with the scalable and secure data storage capabilities of Accumulo.Major Features  Simplified Spark DataFrame read/write to Accumulo using DataSource v2 API  Speedup of 2-5x over existing approaches for pulling key-value data into DataFrame format  Scala and Python support without overhead for moving between languages  Process streaming data from Accumulo without loading it all into Spark memory  Push down filtering with a flexible expression language (JUEL): user can define logical operators and comparisons to reduce the amount of data returned from Accumulo  Column pruning based on selected fields transparently reduces the amount of data returned from Accumulo  Server side inference: ML model inference can run on the Accumulo nodes using MLeap to increase the scalability of AI solutions as well as keeping data in AccumuloUse-casesMASC is advantageous in many use-cases, below we list a few.Scenario 1: A data analyst needs to execute model inference on large amount of data in Accumulo.Benefit: Instead of transferring all the data to a large Spark cluster to score using a Spark model, the connector exports and runs the model on the Accumulo cluster. This reduces the need for a large Spark cluster as well as the amount of data transferred between systems, and can improve inference speeds (&amp;gt;2x speedups observed).Scenario 2: A data scientist needs to train a Spark model on a large amount of data in Accumulo.Benefit: Instead of pulling all the data into a large Spark cluster and restructuring the format to use Spark ML Lib tools, the connector streams data into Spark as a DataFrame reducing time to train and Spark cluster size / memory requirements.Scenario 3: A data analyst needs to perform ad hoc analysis on large amounts of data stored in Accumulo.Benefit: Instead of pulling all the data into a large Spark cluster, the connector prunes rows and columns using pushdown filtering with a flexible expression language.ArchitectureThe Accumulo-Spark connector is composed of two components:  Accumulo server-side iterator performs          column pruning      row-based filtering      MLeap ML model inference and      row assembly using Apache AVRO        Spark DataSource V2          determines the number of Spark tasks based on available Accumulo table splits      translates Spark filter conditions into a JUEL expression      configures the Accumulo iterator      deserializes the AVRO payload      UsageMore detailed documentation on installation and use is available in theConnector documentationDependencies  Java 8  Spark 2.4.3+  Accumulo 2.0.0+JARs available on Maven Central Repository:       Spark DataSource         Accumulo Iterator - Backend for Spark DataSource  Example usefrom configparser import ConfigParserfrom pyspark.sql import types as Tdef get_properties(properties_file):    &quot;&quot;&quot;Read Accumulo client properties file&quot;&quot;&quot;    config = ConfigParser()    with open(properties_file) as stream:        config.read_string(&quot;[top]n&quot; + stream.read())    return dict(config[&#39;top&#39;])properties = get_properties(&#39;/opt/muchos/install/accumulo-2.0.0/conf/accumulo-client.properties&#39;)properties[&#39;table&#39;] = &#39;demo_table&#39; # Define Accumulo table where data will be writtenproperties[&#39;rowkey&#39;] = &#39;id&#39;        # Identify column to use as the key for Accumulo rows# define the schemaschema = T.StructType([  T.StructField(&quot;sentiment&quot;, T.IntegerType(), True),  T.StructField(&quot;date&quot;, T.StringType(), True),  T.StructField(&quot;query_string&quot;, T.StringType(), True),  T.StructField(&quot;user&quot;, T.StringType(), True),  T.StructField(&quot;text&quot;, T.StringType(), True)])# Read from Accumulodf = (spark      .read      .format(&quot;com.microsoft.accumulo&quot;)      .options(**options)  # define Accumulo properties      .schema(schema))     # define schema for data retrieval# Write to Accumuloproperties[&#39;table&#39;] = &#39;output_table&#39;(df .write .format(&quot;com.microsoft.accumulo&quot;) .options(**options) .save())See the demo notebook for more examples.Computational Performance of AI ScenarioSetupThe benchmark setup used a 1,000-node Accumulo 2.0.0 Cluster (16,000 cores) running and a 256-node Spark 2.4.3 cluster (4,096 cores). All nodes used Azure D16s_v3 (16 cores) virtual machines. Fluo-muchos was used to handle Accumulo and Spark cluster deployments and configuration.In all experiments we use the same base dataset which is a collection of Twitter user tweets with labeled sentiment value. This dataset is known as the Sentiment140 dataset (Go, Bhayani, &amp;amp; Huang, 2009). The training data consist of 1.6M samples of tweets, where each tweet has columns indicating the sentiment label, user, timestamp, query term, and text. The text is limited to 140 characters and the overall uncompressed size of the training dataset is 227MB.            sentiment      id      date      query_string      user      text                  0      1467810369      Mon Apr 06 22:19:…      NO_QUERY      _TheSpecialOne_      @switchfoot http:…              0      1467810672      Mon Apr 06 22:19:…      NO_QUERY      scotthamilton      is upset that he …              0      1467810917      Mon Apr 06 22:19:…      NO_QUERY      mattycus      @Kenichan I dived…      To evaluate different table sizes and the impact of splitting the following procedure was used to generate the Accumulo tables:  Prefix id with split keys (e.g. 0000, 0001, …, 1024)  Create Accumulo table and configure splits  Upload prefixed data to Accumulo using Spark and the MASC writer  Duplicate data using custom Accumulo server-side iterator  Validate data partitioningA common machine learning scenario was evaluated using a sentiment model trained using SparkML.To train the classification model, we generated feature vectors from the text of tweets (text column). We used a feature engineering pipeline (a.k.a. featurizer) that breaks the text into tokens, splitting on whitespaces and discarding any capitalization and non-alphabetical characters. The pipeline consisted of  Regex Tokenizer  Hashing Transformer  Logistic RegressionSee the benchmark notebook (Scala) for more details.ResultsThe first set of experiments evaluated data transfer efficiency and ML model inference performance. The chart below shows  Accumulo table split size (1GB, 8GB, 32GB, 64GB)  Total table size (1TB, 10TB, 100TB, 1PB)  Operations          Count: plain count of the data      Inference: Accumulo server-side inference using MLeap      Transfer: Filtering results for 30% data transfer        Time is reported in minutesRemarks  Time is log-scale  Inference was run with and without data transfer to isolate server-side performance.  The smaller each Accumulo table split is, the more splits we have and thus higher parallelization.The second set of experiments highlights the computational performance improvement of using the server-side inference approach compared to running inference on the Spark cluster.Learnings  Accumulo MLeap Server-side inference vs Spark ML results in a 2x improvement  Multi-threading in Spark jobs can be used to fully utilize Accumulo servers          Useful when Spark cluster has less cores than Accumulo      e.g. 8 threads * 2,048 Spark executor = 16,384 Accumulo threads        Unbalanced Accumulo table splits can introduce performance bottlenecksUseful links  Complete Jupyter demo notebook (PySpark) for usage of the Accumulo-Spark connector  Complete Jupyter benchmark notebook (Scala) for usage of the Accumulo-Spark connector  GitHub Repository Microsoft’s contributions for Spark with Apache Accumulo  MLeap - Scala/Java stand-alone model inference for SparkML-based models  SparkML - Spark machine learning library  MASC Maven artifacts          Accumulo Iterator - Backend for Spark DataSource      Spark DataSource      LicenseThis work is publicly available under the Apache License 2.0 on GitHub under Microsoft’s contributions for Apache Spark with Apache Accumulo.ContributionsFeedback, questions, and contributions are welcome!Thanks to contributions from members on the Azure Global Customer Engineering and Azure Government teams.  Anupam Sharma  Arvind Shyamsundar  Billie Rinaldi  Chenhui Hu  Jun-Ki Min  Marc Parisi  Markus Cozowicz  Pavandeep Kalra  Robert Alexander  Scott Graham  Tao WuSpecial thanks to Anca Sarb for promptly assisting with MLeap performance issues.",
      "url": " /blog/2020/02/26/accumulo-spark-connector.html",
      "categories": "blog"
    }
    ,
  
    "blog-2019-12-16-accumulo-proxy-html": {
      "title": "Accumulo Clients in Other Programming Languages",
      "content": "Apache Accumulo has an Accumulo Proxy that allows communication with Accumulo using clients writtenin languages other than Java. This blog post shows how to run the Accumulo Proxy process using Unoand communicate with Accumulo using a Python client.First, clone the Accumulo Proxy repository.git clone https://github.com/apache/accumulo-proxyAssuming you have Uno set up on your machine, configure uno.conf to start the Accumulo Proxyby setting the configuration below:export POST_RUN_PLUGINS=&quot;accumulo-proxy&quot;export PROXY_REPO=/path/to/accumulo-proxyRun the following command to set up Accumulo again. The Proxy will be started after Accumulo runs.uno setup accumuloAfter Accumulo is set up, you should see the following output from uno:Executing post run plugin: accumulo-proxyInstalling Accumulo Proxy at /path/to/fluo-uno/install/accumulo-proxy-2.0.0-SNAPSHOTAccumulo Proxy 2.0.0-SNAPSHOT is running    * view logs at /path/to/fluo-uno/install/logs/accumulo-proxy/Next, follow the instructions below to create a Python 2.7 client that creates an Accumulo tablenamed pythontest and writes data to it:mkdir accumulo-client/cd accumulo-client/pipenv --python 2.7pipenv install thriftpipenv install -e /path/to/accumulo-proxy/src/main/pythoncp /path/to/accumulo-proxy/src/main/python/basic_client.py .# Edit credentials if neededvim basic_client.pypipenv run python2 basic_client.pyVerify that the table was created or data was written using uno ashell or the Accumulo monitor.",
      "url": " /blog/2019/12/16/accumulo-proxy.html",
      "categories": "blog"
    }
    ,
  
    "blog-2019-11-04-checkstyle-import-control-html": {
      "title": "Checking API use",
      "content": "Accumulo follows SemVer across versions with the declaration of a public API.  Code not in the public API should beconsidered unstable, at risk of changing between versions.  The public API packages are listed on the websitebut may not be considered when an Accumulo user writes code.  This blog post explains how to make Mavenautomatically detect usage of Accumulo code outside the public API.The techniques described in this blog post only work for Accumulo 2.0 and later.  Do not use with 1.X versions.Checkstyle PluginFirst add the checkstyle Maven plugin to your pom.&amp;lt;plugin&amp;gt;    &amp;lt;!-- This was added to ensure project only uses Accumulo&#39;s public API --&amp;gt;    &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;    &amp;lt;artifactId&amp;gt;maven-checkstyle-plugin&amp;lt;/artifactId&amp;gt;    &amp;lt;version&amp;gt;3.1.0&amp;lt;/version&amp;gt;    &amp;lt;executions&amp;gt;      &amp;lt;execution&amp;gt;        &amp;lt;id&amp;gt;check-style&amp;lt;/id&amp;gt;        &amp;lt;goals&amp;gt;          &amp;lt;goal&amp;gt;check&amp;lt;/goal&amp;gt;        &amp;lt;/goals&amp;gt;        &amp;lt;configuration&amp;gt;          &amp;lt;configLocation&amp;gt;checkstyle.xml&amp;lt;/configLocation&amp;gt;        &amp;lt;/configuration&amp;gt;      &amp;lt;/execution&amp;gt;    &amp;lt;/executions&amp;gt;  &amp;lt;/plugin&amp;gt;The plugin version is the latest at the time of this post.  For more information see the website forthe Apache Maven Checkstyle Plugin.  The configuration above adds the plugin to check execution goalso it will always run with your build.Create the configuration file specified above: checkstyle.xmlcheckstyle.xml&amp;lt;!DOCTYPE module PUBLIC &quot;-//Puppy Crawl//DTD Check Configuration 1.3//EN&quot; &quot;http://www.puppycrawl.com/dtds/configuration_1_3.dtd&quot;&amp;gt;&amp;lt;module name=&quot;Checker&quot;&amp;gt;  &amp;lt;property name=&quot;charset&quot; value=&quot;UTF-8&quot;/&amp;gt;  &amp;lt;module name=&quot;TreeWalker&quot;&amp;gt;    &amp;lt;!--check that only Accumulo public APIs are imported--&amp;gt;    &amp;lt;module name=&quot;ImportControl&quot;&amp;gt;      &amp;lt;property name=&quot;file&quot; value=&quot;import-control.xml&quot;/&amp;gt;    &amp;lt;/module&amp;gt;  &amp;lt;/module&amp;gt;&amp;lt;/module&amp;gt;This file sets up the ImportControl module.Import Control ConfigurationCreate the second file specified above, import-control.xml and copy the configuration below.  Make sure to replace“insert-your-package-name” with the package name of your project.&amp;lt;!DOCTYPE import-control PUBLIC    &quot;-//Checkstyle//DTD ImportControl Configuration 1.4//EN&quot;    &quot;https://checkstyle.org/dtds/import_control_1_4.dtd&quot;&amp;gt;&amp;lt;!-- This checkstyle rule is configured to ensure only use of Accumulo API --&amp;gt;&amp;lt;import-control pkg=&quot;insert-your-package-name&quot; strategyOnMismatch=&quot;allowed&quot;&amp;gt;    &amp;lt;!-- API packages --&amp;gt;    &amp;lt;allow pkg=&quot;org.apache.accumulo.core.client&quot;/&amp;gt;    &amp;lt;allow pkg=&quot;org.apache.accumulo.core.data&quot;/&amp;gt;    &amp;lt;allow pkg=&quot;org.apache.accumulo.core.security&quot;/&amp;gt;    &amp;lt;allow pkg=&quot;org.apache.accumulo.core.iterators&quot;/&amp;gt;    &amp;lt;allow pkg=&quot;org.apache.accumulo.minicluster&quot;/&amp;gt;    &amp;lt;allow pkg=&quot;org.apache.accumulo.hadoop.mapreduce&quot;/&amp;gt;    &amp;lt;!-- disallow everything else coming from accumulo --&amp;gt;    &amp;lt;disallow pkg=&quot;org.apache.accumulo&quot;/&amp;gt;&amp;lt;/import-control&amp;gt;This file configures the ImportControl module to only allow packages that are declared public API.Hold the lineAdding this to an existing project may expose usage of non public Accumulo API’s. It may take more time than is availableto fix those at first, but do not let this discourage adding this plugin. One possible way to proceed is to allow thecurrently used non-public APIs in a commented section of import-control.xml noting these are temporarily allowed untilthey can be removed. This strategy prevents new usages of non-public APIs while allowing time to work on fixing the current usages of non public APIs.  Also, if you don’t want your project failing to build because of this, you can add &amp;lt;failOnViolation&amp;gt;false&amp;lt;/failOnViolation&amp;gt;to the maven-checkstyle-plugin configuration.",
      "url": " /blog/2019/11/04/checkstyle-import-control.html",
      "categories": "blog"
    }
    ,
  
    "blog-2019-10-15-accumulo-adlsgen2-notes-html": {
      "title": "Using Azure Data Lake Gen2 storage as a data store for Accumulo",
      "content": "Accumulo can store its files in Azure Data Lake Storage Gen2using the ABFS (Azure Blob File System) driver.Similar to S3 blog,the write ahead logs &amp;amp; Accumulo metadata can be stored in HDFS and everything else on Gen2 storageusing the volume chooser feature introduced in Accumulo 2.0. The configurations referred on this blogare specific to Accumulo 2.0 and Hadoop 3.2.0.Hadoop setupFor ABFS client to talk to Gen2 storage, it requires one of the Authentication mechanism listed hereThis post covers Azure Managed Identityformerly known as Managed Service Identity or MSI. This feature provides Azure services with anautomatically managed identity in Azure ADand it avoids the need for credentials or other sensitive information from being stored in codeor configs/JCEKS. Plus, it comes free with Azure AD.At least the following should be added to Hadoop’s core-site.xml on each node.&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.azure.account.auth.type&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;OAuth&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.azure.account.oauth.provider.type&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;org.apache.hadoop.fs.azurebfs.oauth2.MsiTokenProvider&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.azure.account.oauth2.msi.tenant&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;TenantID&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.azure.account.oauth2.client.id&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;ClientID&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;See ABFS docfor more information on Hadoop Azure support.To get hadoop command to work with ADLS Gen2 set thefollowing entries in hadoop-env.sh. As Gen2 storage is TLS enabled by default,it is important we use the native OpenSSL implementation of TLS.export HADOOP_OPTIONAL_TOOLS=&quot;hadoop-azure&quot;export HADOOP_OPTS=&quot;-Dorg.wildfly.openssl.path=&amp;lt;path/to/OpenSSL/libraries&amp;gt; ${HADOOP_OPTS}&quot;To verify the location of the OpenSSL libraries, run whereis libssl commandon the hostAccumulo setupFor each node in the cluster, modify accumulo-env.sh to add Azure storage jars to theclasspath.  Your versions may differ depending on your Hadoop version,following versions were included with Hadoop 3.2.0.CLASSPATH=&quot;${conf}:${lib}/*:${HADOOP_CONF_DIR}:${ZOOKEEPER_HOME}/*:${HADOOP_HOME}/share/hadoop/client/*&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/tools/lib/azure-data-lake-store-sdk-2.2.9.jar&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/tools/lib/azure-keyvault-core-1.0.0.jar&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-azure-3.2.0.jar&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/tools/lib/wildfly-openssl-1.0.4.Final.jar&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/common/lib/jaxb-api-2.2.11.jar&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/common/lib/commons-lang3-3.7.jar&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/common/lib/httpclient-4.5.2.jar&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar&quot;export CLASSPATHInclude -Dorg.wildfly.openssl.path to JAVA_OPTS in accumulo-env.sh as shown below. Thisjava property is an optional performance enhancement for TLS.JAVA_OPTS=(&quot;${ACCUMULO_JAVA_OPTS[@]}&quot;  &#39;-XX:+UseConcMarkSweepGC&#39;  &#39;-XX:CMSInitiatingOccupancyFraction=75&#39;  &#39;-XX:+CMSClassUnloadingEnabled&#39;  &#39;-XX:OnOutOfMemoryError=kill -9 %p&#39;  &#39;-XX:-OmitStackTraceInFastThrow&#39;  &#39;-Djava.net.preferIPv4Stack=true&#39;  &#39;-Dorg.wildfly.openssl.path=/usr/lib64&#39;  &quot;-Daccumulo.native.lib.path=${lib}/native&quot;)Set the following in accumulo.properties and then run accumulo init, but don’t start Accumulo.instance.volumes=hdfs://&amp;lt;name node&amp;gt;/accumuloAfter running Accumulo init we need to configure storing write ahead logs inHDFS.  Set the following in accumulo.properties.instance.volumes=hdfs://&amp;lt;namenode&amp;gt;/accumulo,abfss://&amp;lt;file_system&amp;gt;@&amp;lt;storage_account_name&amp;gt;.dfs.core.windows.net/accumulogeneral.volume.chooser=org.apache.accumulo.server.fs.PreferredVolumeChoosergeneral.custom.volume.preferred.default=abfss://&amp;lt;file_system&amp;gt;@&amp;lt;storage_account_name&amp;gt;.dfs.core.windows.net/accumulogeneral.custom.volume.preferred.logger=hdfs://&amp;lt;namenode&amp;gt;/accumuloRun accumulo init --add-volumes to initialize the Azure DLS Gen2 volume.  Doing thisin two steps avoids putting any Accumulo metadata files in Gen2  during init.Copy accumulo.properties to all nodes and start Accumulo.Individual tables can be configured to store their files in HDFS by setting thetable property table.custom.volume.preferred.  This should be set for themetadata table in case it splits using the following Accumulo shell command.config -t accumulo.metadata -s table.custom.volume.preferred=hdfs://&amp;lt;namenode&amp;gt;/accumuloAccumulo exampleThe following Accumulo shell session shows an example of writing data to Gen2 andreading it back.  It also shows scanning the metadata table to verify the datais stored in Gen2.root@muchos&amp;gt; createtable gen2testroot@muchos gen2test&amp;gt; insert r1 f1 q1 v1root@muchos gen2test&amp;gt; insert r1 f1 q2 v2root@muchos gen2test&amp;gt; flush -w2019-10-16 08:01:00,564 [shell.Shell] INFO : Flush of table gen2test  completed.root@muchos gen2test&amp;gt; scanr1 f1:q1 []    v1r1 f1:q2 []    v2root@muchos gen2test&amp;gt; scan -t accumulo.metadata -c file4&amp;lt; file:abfss://&amp;lt;file_system&amp;gt;@&amp;lt;storage_account_name&amp;gt;.dfs.core.windows.net/accumulo/tables/4/default_tablet/F00000gj.rf []    234,2These instructions will help to configure Accumulo to use Azure’s Data Lake Gen2 Storage along with HDFS. With this setup,we are able to successfully run the continuos ingest test. Going forward, we’ll experiment more on this spacewith ADLS Gen2 and add/update blog as we come along.",
      "url": " /blog/2019/10/15/accumulo-adlsgen2-notes.html",
      "categories": "blog"
    }
    ,
  
    "blog-2019-09-17-erasure-coding-html": {
      "title": "Using HDFS Erasure Coding with Accumulo",
      "content": "HDFS normally stores multiple copies of each file for both performance and durability reasons.The number of copies is controlled via HDFS replication settings, and by default is set to 3. Hadoop 3,introduced the use of erasure coding (EC), which improves durability while decreasing overhead.Since Accumulo 2.0 now supports Hadoop 3, it’s time to take a look at whether usingEC with Accumulo makes sense.  EC Intro  EC Performance  Accumulo Performance with ECEC IntroBy default HDFS achieves durability via block replication.  Usuallythe replication count is 3, resulting in a storage overhead of 200%. Hadoop 3introduced EC as a better way to achieve durability.  More info can befound here.EC behaves much like RAID 5 or 6…for k blocks of data, m blocks ofparity data are generated, from which the original data can be recovered in theevent of disk or node failures (erasures, in EC parlance).  A typical EC scheme is Reed-Solomon 6-3, where6 data blocks produce 3 parity blocks, an overhead of only 50%.  In additionto doubling the available disk space, RS-6-3 is also more faulttolerant…a loss of 3 data blocks can be tolerated, where triple replicationcan only lose two blocks.More storage, better resiliency, so what’s the catch?  One concern isthe time spent calculating the parity blocks.  Unlike replication, where a client writes a block, and then the DataNodes replicatethe data, an EC HDFS client is responsible for computing the parity and sending thatto the DataNodes.  This increases the CPU and network load on the client.  The CPUhit can be mitigated by using Intels ISA-L library, but only on CPUsthat support AVX or AVX2 instructions.  (See EC Myths and EC Introductionfor some interesting claims). In addition, unlike the serial replication I/O path,the EC I/O path is parallel providing greater throughput. In our testing, sequential writes toan EC directory were as much as 3 times faster than a replication directory, and reads were up to 2 times faster.Another side effect of EC is loss of data locality.  For performance reasons, ECdata blocks are striped, so multiple DataNodes must be contacted to read a singleblock of data.  For large sequential reads this is not aproblem, but it can be an issue for small random lookups.  For the latter case,using RS 6-3 with 64KB stripes mitigates some of the random lookup painwithout compromising sequential read/write performance.Important WarningBefore continuing, an important caveat;  the current implementation of EC on Hadoop supports neither hsyncnor hflush.  Both of these operations are silent no-ops (EC limitations).  We discovered this the hardway when a data center power loss resulted in write-ahead log corruption, which werestored in an EC directory.  To avoid this problem ensure allWAL directories use replication.  It’s probably a good idea to keep theaccumulo namespace replicated as well, but we have no evidence to back up that assertion.  As with allthings, don’t test on production data.EC PerformanceTo test EC performance, we created a series of clusters on AWS.  Our Accumulo stack consisted ofHadoop 3.1.1 built with the Intel ISA-L library enabled, Zookeeper 3.4.13, and Accumulo 1.9.3 configuredto work with Hadoop 3 (we did our testing before the official release of Accumulo 2.0). The encodingpolicy is set per-directory using the hdfs command-line tool. To set the encoding policyfor an Accumulo table, first find the table ID (for instance using the Accumulo shell’s“table -l” command), and then from the command line set the policy for the corresponding directoryunder /accumulo/tables.  Note that changing the policy on a directory will set the policy forchild directories, but will not change any files contained within.  To change the policy on an existingAccumulo table, you must first set the encoding policy, and then run a major compaction to rewritethe RFiles for the table.Our first tests were of sequential read and write performance straight to HDFS.  For this test we hada cluster of 32 HDFS nodes (c5.4xlarge AWS instances), 16 Spark nodes (r5.4xlarge),3 zookeepers (r5.xlarge), and 1 master (r5.2xlarge).The first table below shows the results for writing a 1TB file.  The results are the average of three runsfor each of the directory encodings Reed-Solomon (RS) 6-3 with 64KB stripes, RS 6-3 with 1MB stripes,RS 10-4 with 1MB stripes, and the default triple replication.  We also varied the number of concurrentSpark executors, performing tests with 16 executors that did not stress the cluster in any area, and with128 executors which exhausted our network bandwidth allotment of 5 Gbps. As can be seen, in the 16 executorenvironment, we saw greater than a 3X bump in throughput using RS 10-4 with 1MB stripes over triple replication.At saturation, the speed up was still over 2X, which is in line with the results from EC Myths. Also of note,using RS 6-3 with 64KB stripes performed better than the same with 1MB stripes, which is a nice result for Accumulo,as we’ll show later.            Encoding      16 executors      128 executors                  Replication      2.19 GB/s      4.13 GB/s              RS 6-3 64KB      6.33 GB/s      8.11 GB/s              RS 6-3 1MB      6.22 GB/s      7.93 GB/s              RS 10-4 1MB      7.09 GB/s      8.34 GB/s      Our read tests are not as dramatic as those in EC Myths, but still looking good for EC.  Here we show theresults for reading back the 1TB file created in the write test using 16 Spark executors.  In addition tothe straight read tests, we also performed tests with 2 DataNodes disabled to simulate the performance hitof failures which require data repair in the foreground.  Finally, we tested the read performanceafter a background rebuild of the filesystem.  We did this to see if the foreground rebuild orthe loss of 2 DataNodes was the major contributor to any performance degradation.  As can be seen,EC read performance is close to 2X faster than replication, even in the face of failures.            Encoding      32 nodesno failures      30 nodeswith failures      30 nodesno failures                  Replication      3.95 GB/s      3.99 GB/s      3.89 GB/s              RS 6-3 64KB      7.36 GB/s      7.27 GB/s      7.16 GB/s              RS 6-3 1MB      6.59 GB/s      6.47 GB/s      6.53 GB/s              RS 10-4 1MB      6.21 GB/s      6.08 GB/s      6.21 GB/s      Accumulo Performance with ECWhile the above results are impressive, they are not representative of how Accumulo uses HDFS.  For starters,Accumulo sequential I/O is doing far more than just reading or writing files; compression and serialization,for example, place quite a load upon the tablet server CPUs.  An example to illustrate this is shown below.The time in minutes to bulk-write 400 million rows to RFiles with 40 Spark executors is listed for both ECusing RS 6-3 with 1MB stripes and triple replication.  The choice of compressor has a much more profoundeffect on the write times than the choice of underlying encoding for the directory being written to(although without compression EC is much faster than replication).            Compressor      RS 6-3 1MB      Replication      File size (GB)                  gz      2.7      2.7      21.3              none      2.0      3.0      158.5              snappy      1.6      1.6      38.4      Of much more importance to Accumulo performance is read latency. A frequent use case for our group is to obtain anumber of row IDs from an index and then use a BatchScanner to read those individual rows.In this use case, the time to access a single row is far more important than the raw I/O performance.  To testAccumulo’s performance with EC for this use case, we did a series of tests against a 10 billion row table,with each row consisting of 10 columns.  16 Spark executors each performed 10000 queries, where each querysought 10 random rows.  Thus 16 million individual rows were returned in batches of 10.  For each batch of10, the time in milliseconds was captured, and theses times were collected in a histogram of 50ms buckets, witha catch-all bucket for queries that took over 1 second.  For this test we reconfigured our cluster to make useof c5n.4xlarge nodes featuring must faster networking speeds (15 Gbps sustained vs 5 Gbps forc5.4xlarge). Because these nodes are in short supply, we ran with only 16 HDFS nodes (c5n.4xlarge),but still had 16 Spark nodes (also c5n.4xlarge).  Zookeeper and master nodes remained the same.In the table below, we show the min, max, and average times in milliseconds for each batch of 10 acrossfour different encoding policies.  The clear winner here is replication, and the clear loser RS 10-4 with1MB stripes, but RS 6-3 with 64KB stripes is not looking too bad.            Encoding      Min      Avg      Max                  RS 10-4 1MB      40      105      2148              RS 6-3 1MB      30      68      1297              RS 6-3 64KB      23      43      1064              Replication      11      23      731      The above results also hold in the event of errors.  The next table shows the same test, but with 2 DataNodesdisabled to simulate failures that require foreground rebuilds.  Again, replication wins, and RS 10-4 1MBloses, but RS 6-3 64KB remains a viable option.            Encoding      Min      Avg      Max                  RS 10-4 1MB      53      143      3221              RS 6-3 1MB      34      113      1662              RS 6-3 64KB      24      61      1402              Replication      12      26      304      The images below show a plots of the histograms.  The third plot was generated with 14 HDFS DataNodes, but afterall missing data had been repaired.  Again, this was done to see how much of the performance degradation could beattributed to missing data, and how much to simply having less computing power available.ConclusionHDFS with erasure coding has the potential to double your available Accumulo storage, at the cost of a hit inrandom seek times, but a potential increase in sequential scan performance. We will be proposing some changesto Accumulo to make working with EC a bit easier. Our initial thoughts are collected in thisAccumulo dev list post.",
      "url": " /blog/2019/09/17/erasure-coding.html",
      "categories": "blog"
    }
    ,
  
    "blog-2019-09-10-accumulo-s3-notes-html": {
      "title": "Using S3 as a data store for Accumulo",
      "content": "Accumulo can store its files in S3, however S3 does not support the needs ofwrite ahead logs and the Accumulo metadata table. One way to solve this problemis to store the metadata table and write ahead logs in HDFS and everything elsein S3.  This post shows how to do that using Accumulo 2.0 and Hadoop 3.2.0.Running on S3 requires a new feature in Accumulo 2.0, that volume choosers areaware of write ahead logs.Hadoop setupAt least the following settings should be added to Hadoop’s core-site.xml file on each node in the cluster.&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.s3a.access.key&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;KEY&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.s3a.secret.key&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;SECRET&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;!-- without this setting Accumulo tservers would have problems when trying to open lots of files --&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.s3a.connection.maximum&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;128&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;See S3A docsfor more S3A settings.  To get hadoop command to work with s3 set exportHADOOP_OPTIONAL_TOOLS=&quot;hadoop-aws&quot; in hadoop-env.sh.When trying to use Accumulo with Hadoop’s AWS jar HADOOP-16080 wasencountered.  The following instructions build a relocated hadoop-aws jar as awork around.  After building the jar copy it to all nodes in the cluster.mkdir -p /tmp/haws-reloccd /tmp/haws-reloc# get the Maven pom file that builds a relocated jarwget https://gist.githubusercontent.com/keith-turner/f6dcbd33342732e42695d66509239983/raw/714cb801eb49084e0ceef5c6eb4027334fd51f87/pom.xmlmvn package -Dhadoop.version=&amp;lt;your hadoop version&amp;gt;# the new jar will be in targetls target/Accumulo setupFor each node in the cluster, modify accumulo-env.sh to add S3 jars to theclasspath.  Your versions may differ depending on your Hadoop version,following versions were included with Hadoop 3.2.0.CLASSPATH=&quot;${conf}:${lib}/*:${HADOOP_CONF_DIR}:${ZOOKEEPER_HOME}/*:${HADOOP_HOME}/share/hadoop/client/*&quot;CLASSPATH=&quot;${CLASSPATH}:/somedir/hadoop-aws-relocated.3.2.0.jar&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/tools/lib/aws-java-sdk-bundle-1.11.375.jar&quot;# The following are dependencies needed by by the previous jars and are subject to changeCLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/common/lib/jaxb-api-2.2.11.jar&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar&quot;CLASSPATH=&quot;${CLASSPATH}:${HADOOP_HOME}/share/hadoop/common/lib/commons-lang3-3.7jar&quot;export CLASSPATHSet the following in accumulo.properties and then run accumulo init, but don’t start Accumulo.instance.volumes=hdfs://&amp;lt;name node&amp;gt;/accumuloAfter running Accumulo init we need to configure storing write ahead logs inHDFS.  Set the following in accumulo.properties.instance.volumes=hdfs://&amp;lt;name node&amp;gt;/accumulo,s3a://&amp;lt;bucket&amp;gt;/accumulogeneral.volume.chooser=org.apache.accumulo.server.fs.PreferredVolumeChoosergeneral.custom.volume.preferred.default=s3a://&amp;lt;bucket&amp;gt;/accumulogeneral.custom.volume.preferred.logger=hdfs://&amp;lt;namenode&amp;gt;/accumuloRun accumulo init --add-volumes to initialize the S3 volume.  Doing thisin two steps avoids putting any Accumulo metadata files in S3 during init.Copy accumulo.properties to all nodes and start Accumulo.Individual tables can be configured to store their files in HDFS by setting thetable property table.custom.volume.preferred.  This should be set for themetadata table in case it splits using the following Accumulo shell command.config -t accumulo.metadata -s table.custom.volume.preferred=hdfs://&amp;lt;namenode&amp;gt;/accumuloAccumulo exampleThe following Accumulo shell session shows an example of writing data to S3 andreading it back.  It also shows scanning the metadata table to verify the datais stored in S3.root@muchos&amp;gt; createtable s3testroot@muchos s3test&amp;gt; insert r1 f1 q1 v1root@muchos s3test&amp;gt; insert r1 f1 q2 v2root@muchos s3test&amp;gt; flush -w2019-09-10 19:39:04,695 [shell.Shell] INFO : Flush of table s3test  completed.root@muchos s3test&amp;gt; scanr1 f1:q1 []    v1r1 f1:q2 []    v2root@muchos s3test&amp;gt; scan -t accumulo.metadata -c file2&amp;lt; file:s3a://&amp;lt;bucket&amp;gt;/accumulo/tables/2/default_tablet/F000007b.rf []    234,2These instructions were only tested a few times and may not result in a stablesystem. I have run a 24hr test with Accumulo and S3.Is S3Guard needed?I am not completely certain about this, but I don’t think S3Guard is needed forregular Accumulo tables.  There are two reasons I think this is so.  First eachAccumulo user tablet stores its list of files in the metadata table usingabsolute URIs.  This allows a tablet to have files on multiple DFS instances.Therefore Accumulo never does a DFS list operation to get a tablets files, italways uses whats in the metadata table.  Second, Accumulo gives each file aunique name using a counter stored in Zookeeper and file names are neverreused.Things are sligthly different for Accumulo’s metadata.  User tablets storetheir file list in the metadata table.  Metadata tablets store their file listin the root table.  The root table stores its file list in DFS.  Therefore itwould be dangerous to place the root tablet in S3 w/o using S3Guard.  That iswhy these instructions place Accumulo metadata in HDFS. Hopefully thisconfiguration allows the system to be consistent w/o using S3Guard.When Accumulo 2.1.0 is released with the changes made by #1313 for issue#936, it may be possible to store the metadata table in S3 w/oS3Gaurd.  If this is the case then only the write ahead logs would need to bestored in HDFS.",
      "url": " /blog/2019/09/10/accumulo-S3-notes.html",
      "categories": "blog"
    }
    ,
  
    "blog-2019-08-12-why-upgrade-html": {
      "title": "Top 10 Reasons to Upgrade",
      "content": "Accumulo 2.0 has been in development for quite some time now and is packed with new features, bugfixes, performance improvements and redesigned components.  All of these changes bring challengeswhen upgrading your production cluster so you may be wondering… why should I upgrade?My top 10 reasons to upgrade. For all changes see the release notes  Summaries  New Bulk Import  Simplified Scripts and Config  New Monitor  New APIs  Offline creation  Search Documentation  On disk encryption  ZStandard Compression  New Scan ExecutorsSummariesThis feature allows detailed stats about Tables to be written directly into Accumulo files (R-Files).Summaries can be used to make precise decisions about your data. Once configured, summaries become apart of your Tables, so they won’t impact ingest or query performance of your cluster.Here are some example use cases:  A compaction could automatically run if deletes compose more than 25% of the data  An admin could optimize compactions by configuring specific age off of data  An admin could analyze R-File summaries for better performance tuning of a clusterFor more info check out the summary docs for 2.0New Bulk ImportBulk Ingest was completely redone for 2.0.  Previously, Bulk Ingest relied on expensive inspections ofR-Files across multiple Tablet Servers. With enough data, an old Bulk Ingest operation could easilyhold up simpler Table operations and critical compactions of files.The new Bulk Ingest gives the user control over the R-File inspection, allows for offline bulkingesting and provides performance improvements.Simplified Scripts and ConfigMany improvements were done to the scripts and configuration. See Mike’s description of the improvements.New MonitorThe Monitor has been re-written using REST, Javascript and more modern Web Tech.  It is faster,cleaner and more maintainable than the previous version. Here is a screen shot:New APIsConnecting to Accumulo is now easier with a single point of entry for clients. It can now be done witha fluent API, 2 imports and using minimal code:import org.apache.accumulo.core.client.Accumulo;import org.apache.accumulo.core.client.AccumuloClient;try (AccumuloClient client = Accumulo.newClient()          .to(&quot;instance&quot;, &quot;zk&quot;)          .as(&quot;user&quot;, &quot;pass&quot;).build()) {      // use the client      client.tableOperations().create(&quot;newTable&quot;);    }As you can see the client is also closable, which gives developers more control over resources.See the Accumulo entry point javadoc.Key and Mutation have new fluent APIs, which now allow mixing of String and byte[] types.Key newKey = Key.builder().row(&quot;foo&quot;).family(&quot;bar&quot;).build();Mutation m = new Mutation(&quot;row0017&quot;);m.at().family(&quot;001&quot;).qualifier(new byte[] {0,1}).put(&quot;v99&quot;);m.at().family(&quot;002&quot;).qualifier(new byte[] {0,1}).delete();More examples for Key and Mutation.Table creation optionsTables can now be created with splits, which is much faster than creating atable and then adding splits.  Tables can also be created in an offline statenow.  The new bulk import API supports offline tables.  This enables thefollowing method of getting a lot of data into a new table very quickly.  Create offline table with splits  Bulk import into new offline table  Bring table onlineSee the javadoc for NewTableConfiguration and search for methods introduced in 2.0.0 for more information.Search DocumentationNew ability to quickly search documentation on the website. The user manual was completely redonefor 2.0. Check it out here. Users can now quickly search the website across all 2.x documentation.New CryptoOn disk encryption was redone to be more secure and flexible. For an in depth description of how Accumulodoes on disk encryption, see the user manual.  NOTE: This is currently an experimental feature.An experimental feature is considered a work in progress or incomplete and could change.Zstandard compressionSupport for Zstandard compression was added in 2.0.  It has been measured to perform better thangzip (better compression ratio and speed) and snappy (better compression ratio). Checkout Facebook’s github for Zstandard andthe table.file.compress.type property for configuring Accumulo.New Scan ExecutorsUsers now have more control over scans with the new scan executors.  Tables can be configured to utilize thesepowerful new mechanisms using just a few properties, giving user control over things like scan prioritization andbetter cluster resource utilization.For example, a cluster has a bunch of long running scans and one really fast scan.  The long running scans will eat upa majority of the server resources causing the one really fast scan to be delayed.  Scan executors allow an adminto configure the cluster in a way that allows the one fast scan to be prioritized and not have to wait.Checkout some examples in the user guide.",
      "url": " /blog/2019/08/12/why-upgrade.html",
      "categories": "blog"
    }
    ,
  
    "release-accumulo-2-0-0": {
      "title": "Apache Accumulo 2.0.0",
      "content": "Apache Accumulo 2.0.0 contains significant changes from 1.9 and earlierversions. It is the first major release since adopting semver and is theculmination of more than 3 years worth of work by more than 40 contributorsfrom the Accumulo community. The following release notes highlight some of thechanges. If anything is missing from this list, please contact the developersto have it included.Notable ChangesNew API for creating connections to AccumuloA fluent API for creating Accumulo clients was introduced in ACCUMULO-4784 and #634.The Connector and ZooKeeperInstance objects have been deprecated and replaced byAccumuloClient which is created from the Accumulo entry point. The new API also deprecatesClientConfiguration and introduces its own properties file called accumulo-client.propertiesthat ships with the Accumulo tarball. The new API has the following benefits over the old API:  All connection information can be specifed in properties file to create the client. This was notpossible with old API.  The new API does not require ZooKeeperInstance to be created first before creating a client.  The new client is closeable and does not rely on shared static resource management  Clients can be created using a new Java builder, Properties object, or accumulo-client.properties  Clients can now be created with default settings for BatchWriter, Scanner, etc.  Create scanners with default authorizations. #744See the client documentation for more information on how to use the new API.Hadoop 3 Java 8 &amp;amp; 11.Accumulo 2.x expects at least Java 8 and Hadoop 3.  It is built against Java 8and Hadoop 3 and the binary tarball is targeted to work with a Java 8 andHadoop 3 system.  See ACCUMULO-4826,  #531, and ACCUMULO-4299.  Running with Java 11 is also supported, but Java 11 is notrequired.Simplified Accumulo scripts and configuration filesAccumulo’s scripts and configuration were refactored in ACCUMULO-4490 to make Accumuloeasier to use. The number of scripts in the bin directory of the Accumulo release tarballhas been reduced from 20 scripts to the four scripts below:  accumulo - mostly left alone except for improved usage  accumulo-service - manage Accumulo processes as services  accumulo-cluster - manage Accumulo on cluster. Replaces start-all.sh and stop-all.sh  accumulo-util - combines many utility scripts into one script.Read this blog post for more information on this change.New Bulk Import APIA new bulk import API was added in 2.0 that has very different implementation.  This new API supports the following new functionality.  Bulk import to an offline table.  Load plans that specify where files go in a table which avoids opening thefiles for inspection.  Inspection of file on the client side. Inspection of all files is donebefore the FATE operation starts.  This results in less namenode operationsand fail-fast for bad files (no longer need a fail directory).  A new improved algorithm to load files into tablets.  This new algorithmscans the metadata table and makes asynchronous load calls to all tablets.This queues load operations on all tablets at around the same time.  Theasync RPC calls and beforehand inspection make the bulk load FATE operationmuch shorter.The shell command for doing bulk load supports the old and new API.  To use thenew API from the shell simply omit the failure directory argument.For the API, use the new fluent API.See #436, #472, and #570.SummariesSummaries enables continually generatingstatistics about a table with user defined functions.  This feature can informa user about what is in their table and be used by compaction strategies tomake decisions.  For example, using this feature it would be possible tocompact all tablets where deletes are more than 25% of the data. Anotherexample use case is optimizing filtering compactions by enabling smartselection of files with pertinent data. Examples of filtering compactions areage off and removal of non-compliant data.Scan ExecutorsScan executors support prioritizingand dedicating scan resources. Each executor has a configurable number ofthreads and an optional custom prioritizer.  Tables can be configured in aflexible way to dispatch scans to different executors.SPI packageAll new pluggable components introduced in 2.0 were placed under a new SPIpackage.  The SPI package is analyzed by Apilyzer at build time to ensureplugins only use SPI and API types.  This prevents plugins from using internalAccumulo types that are inherently unstable over time.  Plugins created before2.0 do use internal types and are less stable.  The new pluggable interfacesshould be much more stable.Official Accumulo docker image was createdAn official Accumulo docker images was created in ACCUMULO-4706 to makeit easier for users to run Accumulo in Docker. To support running in Docker, a few changes weremade to Accumulo:  The --upload-accumulo-site option was added to accumulo init to set properties in accumulo-site.xmlto Zookeeper during initialization.  The -o &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; option was added to the accumulo command to override configuration that couldnot be set in Zookeeper.Updated and improved Accumulo documentationAccumulo’s documentation has been refactored with the following improvements:  Documentation source now lives in accumulo-website repo so changesare now immediately viewable.  Improved navigation using a new sidebar  Better linking to Javadocs, between documentation pages, and to configuration properties.Accumulo’s documentation was also reviewed and changes were made to improve accuracy and removeout of date documentation.Moved Accumulo Examples to its own repoThe Accumulo examples were moved out the accumulo repo to the accumulo-examples repowhich has the following benefits:  The Accumulo examples are no longer released with Accumulo and can be continuously improved.  The Accumulo API version used by the examples can be updated right before Accumulo is releasedto test for any changes to the API that break semver.Simplified Accumulo logging configurationThe log4j configuration of Accumulo services was improved in ACCUMULO-4588 with the following changes:  Logging is now configured using standard log4j JVM property ‘log4j.configuration’ in accumulo-env.sh.  Tarball ships with fewer log4j config files (3 rather than 6) which are all log4j properties files.  Log4j XML can still be used by editing accumulo-env.sh  Removed auditLog.xml and added audit log configuration to log4j-service properties files  Accumulo conf/ directory no longer has an examples/ directory. Configuration files ship in conf/ and areused by default.  Accumulo monitor by default will bind to 0.0.0.0 but will advertise hostname looked up in Java for logforwarding  Switched to use full hostnames rather than short hostnames for loggingRemoved comparison of Value with byte[] in Value.equals()Replaced the ability to use Value.equals(byte[]) to check if the contents of aValue object was equal to a given byte array in ACCUMULO-4726. To performthat check, you must now use the newly added Value.contentEquals(byte[])method. This corrects the behavior of the equals method so that it conformsto the API contract documented in the javadoc inherited from its superclass.However, it will break any code that was relying on the undocumented and brokenbehavior to compare Value objects with byte arrays. Such comparisons will nowalways return false instead of true, even if the contents are equal.Removed default dynamic reloading classpath directory (lib/ext)In #1179, the default directory for dynamic class reloading (lib/ext)was removed and the default value for the deprecated propertygeneral.dynamic.classpaths was set to blank. This was done as part of a planto phase out class loading behaviors that are tightly coupled to Accumulo, infavor of more user-pluggable class loading features that are easier to maintainseparately from Accumulo’s core code.To continue to use this feature until it is removed, you must set this propertyto a value. However, it is recommended to add your non-dynamic user class pathsto the CLASSPATH environment in accumulo-env.sh instead, or to leverage theper-table context class paths feature, depending on your use case. Forreference, the previous default value was $ACCUMULO_HOME/lib/ext/[^.].*.jar.Other Notable Changes  ACCUMULO-3652 - Replaced string concatenation in log statements with slf4jwhere applicable. Removed tserver TLevel logging class.  ACCUMULO-4449 - Removed ‘slave’ terminology and replaced with ‘tserver’ inmost cases. The former ‘slaves’ config file is now named ‘tservers’. Added checks toscripts to fail if ‘slaves’ file is present.  ACCUMULO-4808 - Can now create table with splits and offline.  Specifying splitsat table creation time can be much faster than adding splits after creation.  ACCUMULO-4463 - Caching is now pluggable.  ACCUMULO-4177 - New built in cache implementation based on TinyLFU.  ACCUMULO-4376 ACCUMULO-4746 - Mutation and Key Fluent APIs allow easy mixing of types.  For example a family of type String and qualifier of type byte[] is much easier to write using this new API.  ACCUMULO-4771 - The Accumulo monitor was completely rewritten.  ACCUMULO-4732 - Specify iterators and locality groups at table creation time.  ACCUMULO-4612 - Use percentages for memory related configuration.  ACCUMULO-1787 - Two tier compaction strategy.  Support compacting small files with snappy and large files with gzip.  #560 - Provide new Crypto interface &amp;amp; impl  #536 - Removed mock Accumulo.  #438 - Added support for ZStandard compression  #404 - Added basic Grafana dashboard example.  #1102 #1100 #1037 - Removed lock contention in different areas.  These locks caused threads working unrelated task to impede each other.  #1033 - Optimized the default compaction strategy.  In some cases the Accumulo would rewrite data O(N^2) times over repeated compactions.  With this change the amount of rewriting is always logarithmic.  Many performance improvements mentioned in the 1.9.X release notes are also available in 2.0.  Scanners close server side sessions on close #813 #905This release also includes bug fixes from 1.9.3 which was released after2.0.0-alpha-1 and 2.0.0-alpha-2.UpgradingView the Upgrading Accumulo documentation for guidance.Useful Links  All tickets on GitHub related to this release",
      "url": " /release/accumulo-2.0.0/",
      "categories": "release"
    }
    ,
  
    "blog-2019-04-24-using-spark-with-accumulo-html": {
      "title": "Using Apache Spark with Accumulo",
      "content": "Apache Spark applications can read from and write to Accumulo tables.  Toget started using Spark with Accumulo, checkout the Spark documentation inthe 2.0 Accumulo user manual. The Spark example application is a good starting pointfor using Spark with Accumulo.",
      "url": " /blog/2019/04/24/using-spark-with-accumulo.html",
      "categories": "blog"
    }
    ,
  
    "release-accumulo-1-9-3": {
      "title": "Apache Accumulo 1.9.3",
      "content": "Apache Accumulo 1.9.3 contains bug fixes for Write Ahead Logs and compaction.Users of 1.9.2 are encouraged to upgrade.  User Manual - In-depth developer and administrator documentation  Javadocs - Accumulo 1.9 API  Examples - Code with corresponding readme files that give step bystep instructions for running example codeNotable ChangesMultiple Fixes for Write Ahead LogsThis release fixes Write Ahead Logs issues that slow or prevent recoveryand in some cases lead to data loss. The fixes reduce the number of WALSreferenced by a tserver, improve error handing, and improve clean up.      Eliminates a race condition that could result in data loss during recovery.If the GC deletes unreferenced WALs from ZK while the master is readingrecovery WALs from ZK, the master may skip WALs it should not, resulting indata loss.  Fixed in #866.        Opening a new WAL in DFS may fail, but still be advertised in ZK. This couldresult in a missing WAL during recovery, preventing tablets from loading.There is no data loss in this case, just WAL references that should not exists.Reported in #949 and fixed in #1005.        tserver failures could result in many empty WALs that unnecessarily slow recovery.This was fixed in #823.        Some write patterns caused tservers to unnecessarily reference a lot of WALs,which could slow any recovery.  In #854 the max WALs referenced waslimited regardless of the write pattern, avoiding long recovery times.        During tablet recovery, filter out logs that do not define the tablet. #881        If a tserver fails sorting, a marker file is written to the recovery directory.This marker prevents any subsequent recovery attempts from succeeding.Fixed by modifying the WAL RecoveryLogReader to handle failed file markers in #961.        Improve performance of serializing mutations to a WAL by avoiding frequent synchronization. #669  Multiple Fixes for Compaction Issues      Stop locking during compaction.  Compactions acquired the tablet lock between eachkey value. This created unnecessary contention with other operations like scan andbulk imports.  The synchronization was removed #1031.        Only re-queue compaction when there is activity. #759  Fix ArrayOutOfBounds error when new files are created (affects all previous versions)If the 7 digit base 36 number used to name files attempted to go to 8 digits,then compactions would fail.  This was fixed in #562.Updated Master Metrics to include FATE metrics.Added master metrics to provide a snapshot of current FATE operations.  The metrics added:  the number of current FATE transactions in progress,  the count of child operations that have occurred on the zookeeper FATE node  a count of zookeeper connection errors when the snapshot is taken.The number of child operations provides a light-weight surrogate for FATE transactionprogression between snapshots. The metrics are controlled with the following properties:  master.fate.metrics.enabled - default to false preserve current metric reporting  master.fate.metrics.min.update.interval - default to 60s - there is a hard limit of 10s.When enabled, the metrics are published to JMX and can optionally be configured using standardhadoop metrics2 configuration files.Fixed issues with Native Maps with libstdc++ 8.2 and higherVersions of libstdc++ 8.2 and higher triggered errors within within the native map code.This release fixes issues #767, #769, #1064, and #1070.Fixed splitting tablets with files and no dataThe split code assumed that if a tablet had files that it had data inthose files.  There are some edge case where this is not true.  Updatedthe split code to handle this #998.Log when a scan waits a long time for files.Accumulo has a configurable limit on the max number of files open in atserver for all scans.  When too many files are open, scans must wait.In #978 and #981 scans that wait too long for files now log a message.Fixed race condition in table existence check.The Accumulo client code that checks if tables exists had a racecondition.  The race was fixed in #768 and #973Support running Mini Accumulo using Java 11Mini Accumulo made some assumptions about classloaders that were nolonger true in Java 11.  This caused Mini to fail in Java 11.  In#924 Mini was updated to work with Java 11, while still workingwith Java 7 and 8.Fixed issue with improperly configured SnappyIf snappy was configured and the snappy libraries were not available then minorcompactions could hang forever.  In #920 and #925 this was fixed and minorcompactions will proceed when a different compression is configured.Handle bad locality group config.Improperly configured locality groups could cause a tablet to becomeinoperative.  This was fixed in #819 and #840.Fixed bulk import race condition.There was a race condition in bulk import that could result in filesbeing imported after a bulk import transaction had completed.  In theworst case these files were already compacted and garbage collected.This would cause a tablet to have a reference to a file that did notexists.  No data would have been lost, but it would cause scans to fail.The race was fixed in #800 and #837Fixed issue with HostRegexTableLoadBalancerThis addresses an issue when using the HostRegexTableLoadBalancerwhen the default pool is empty. The load balancer will not assign the tablets at all.Here, we select a random pool to assign the tablets to. This behavior is on bydefault in the HostRegexTableLoadBalancer but can be disabled viaHostRegexTableLoadBalancer configuration setting table.custom.balancer.host.regex.HostTableLoadBalancer.ALL Fixed in #691 - backported to 1.9 in #710Update to libthrift versionThe packaged, binary  tarball contains updated version of libthrift to version 0.9.3-1 toaddress thrift CVE. Issue #1029UpgradingView the Upgrading Accumulo documentation for guidance.Useful links  Release VOTE email thread  All Changes since 1.9.2  All tickets related to this release",
      "url": " /release/accumulo-1.9.3/",
      "categories": "release"
    }
    ,
  
    "blog-2019-02-28-nosql-day-html": {
      "title": "NoSQL Day 2019",
      "content": "On May 21st in Washington, DC, there will be a one-day community event for Apache Accumulo,HBase, and Phoenix called NoSQL Day. We hope that these three Apache communities can come together to sharestories from the field and learn from one another. This event is being offered by theDataWorks Summit organization, prior to their DataWorks Summit event May 20th through 23rd.At this time, we are looking for speakers, attendees, and sponsors for the event. Forspeakers, we hope to see a wide breadth of subjects and focus, anything from performance,scaling, real-life applications, dev-ops, or best-practices. All speakers are welcome!Abstracts can be submitted here.For attendees, we want to get the best and brightest from each of the respective communitiesbecause the organizers believe we have much to learn from from each other. We’ve tried tokeep costs down to make this approachable for all.Finally, sponsors are the major enabler to provide events like these at low-coststo attendees. If you are interested in a corporate sponsorship, please feel free to contactJosh Elser for more information.",
      "url": " /blog/2019/02/28/nosql-day.html",
      "categories": "blog"
    }
    ,
  
    "release-accumulo-2-0-0-alpha-2": {
      "title": "Apache Accumulo 2.0.0-alpha-2",
      "content": "Apache Accumulo 2.0.0-alpha-2 contains numerous changes since the alpha-2. Thisalpha release is a preview of features coming in 2.0.0. It is being madeavailable for preview, testing, and evaluation of those upcoming features, butis not yet suitable for production use. API, packaging, and other changes maystill occur before a final 2.0.0 release.  User Manual - In-depth developer and administrator documentation  Javadocs - Accumulo 2.0 API (subject to change)Notable Changes  Please see the draft release notes for 2.0.0 for a list of thechanges coming in 2.0.0, many of which are either complete, or nearlycomplete in this alpha release.Since 2.0.0-alpha-1  New Map Reduce API #743 #751 #753 #803  Deprecated existing map reduce API 2465562 #804  #892  Added ability to create scanners with default auths #744  Updated AccumuloClient builder API #792  AccumuloClient was made Closeable #718  Scanners close server side sessions on close #813 #905",
      "url": " /release/accumulo-2.0.0-alpha-2/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-2-0-0-alpha-1": {
      "title": "Apache Accumulo 2.0.0-alpha-1",
      "content": "Apache Accumulo 2.0.0-alpha-1 contains numerous changes since the 1.9. Thisalpha release is a preview of features coming in 2.0.0. It is being madeavailable for preview, testing, and evaluation of those upcoming features, butis not yet suitable for production use. API, packaging, and other changes maystill occur before a final 2.0.0 release.  User Manual - In-depth developer and administrator documentation  Javadocs - Accumulo 2.0 API (subject to change)Notable Changes  Please see the draft release notes for 2.0.0 for a list of thechanges coming in 2.0.0, many of which are either complete, or nearlycomplete in this alpha release.This release also includes bug fixes from 1.9.2 and earlier.",
      "url": " /release/accumulo-2.0.0-alpha-1/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-9-2": {
      "title": "Apache Accumulo 1.9.2",
      "content": "Apache Accumulo 1.9.2 contains fixes for critical write-ahead log bugs.Users of any previous version of 1.8 or 1.9 are encouraged to upgradeimmediately to avoid those issues.  User Manual - In-depth developer and administrator documentation  Javadocs - Accumulo 1.9 API  Examples - Code with corresponding readme files that give step bystep instructions for running example codeNotable ChangesFixes for Critical WAL Bugs (affects versions 1.8.0-1.9.1)Multiple bugs were fixed in 1.9.2 which affects the behavior of the write-aheadlog mechanism. These vary in significance, ranging from moderate to critical.  #537 - (Critical) Since 1.8.0, a bug existed which could cause somewrite-ahead logs to be removed (garbage collected) before Accumulo wasfinished with them. These removed logs could have contained important statetracking information.  Without the state contained in these logs, some datain the remaining logs could have been replayed into a table when not needed.This could have reintroduced deleted data, or introduced duplicate data(which can interfere with combiners).  #538 - (Moderate) A bug was introduced in 1.9.1 which resulted in somefalse positive IllegalStateExceptions to occur, preventing log recovery.  #539 - (Moderate) Since 1.8.0, a race condition existed which could cause a logfile which contains data to be recovered to not be recorded, thus making itinvisible to recovery, if a tserver died within a very small window.  #559 fixes this issue and may also fix a 1.9.1 deadlock caused by the fix for #441.Even if you primarily use bulk ingest, Accumulo’s own metadata tables can beaffected by these bugs, causing unexpected behavior after an otherwise routineand recoverable server failure. As such, these bugs should be a concern to allusers.Fixes for concurrency bugs gathering table information (affects 1.8.0-1.9.1)Bugs were found with the master.status.threadpool.size property. If thisproperty were set to a value other than 1, it could cause 100% CPU, hanging,or ConcurrentModificationExceptions.These bugs were fixed in #546.Caching of file lengthsRFiles stores metadata at the end of file. When opening a rfile Accumuloseeks to the end and reads metadata.  To do this seek the file length is needed.Before opening a file its length is requested from the namenode.  This canadd more load to a busy namenode.  To alleviate this, a small cache of file lengths wasadded in #467.Monitor time unit bugA bug was found in the monitor which displayed time durations (for example,those pertaining to bulk imports) in incorrect time units.This bug was fixed in #553.UpgradingView the Upgrading Accumulo documentation for guidance.Testing  All ITs passed with Hadoop 3.0.0 (hadoop.profile=3)  All ITs passed with Hadoop 2.6.4 (hadoop.profile=2)  Ran 3 continuous ingesters successfully for 24 hours on a 10 node clusterwith agitation and pausing. Verification for all 3 tests was successful.  Ran continuous ingest for 24 hours and verified without agitation on a 10node cluster.  Tested Apache Fluo build and ITs passed against this version.  Ran a single-node cluster with Uno and created a table, ingested data,flushed, compacted, scanned, and deleted the table.Useful Links  All tickets related to this release",
      "url": " /release/accumulo-1.9.2/",
      "categories": "release"
    }
    ,
  
    "blog-2018-07-05-accumulo-summit-html": {
      "title": "Accumulo Summit is on October 15th!",
      "content": "The Fifth Annual Accumulo Summit will be held on October 15, 2018 at the Sheraton Columbia Town Center Hotel in Columbia, MD.This day-long event offers a unique opportunity for attendees to get introduced to Apache Accumulo, sharpen their skillsets, and connect with leading Accumulo users and developers.Have a great idea you’d like to share?  Engineers, architects, and business leaders are encouraged to share their experiences or present a topic that would be of interest to the Accumulo community.  Talks can be submitted  through August 1st.Early bird registration is now open!  Sign up before September 1st to save $50 off the regular admission price.",
      "url": " /blog/2018/07/05/accumulo-summit.html",
      "categories": "blog"
    }
    ,
  
    "release-accumulo-1-9-1": {
      "title": "Apache Accumulo 1.9.1",
      "content": "Apache Accumulo 1.9.1 contains bug fixes for a critical data loss bug. Users of1.8.0, 1.8.1, or 1.9.0 are encouraged to upgrade immediately.  User Manual - In-depth developer and administrator documentation  Javadocs - Accumulo 1.9 API  Examples - Code with corresponding readme files that give step bystep instructions for running example codeNotable ChangesFixes for Critical WAL Data Loss Bugs (affects versions 1.8.0-1.9.0)Accumulo 1.9.1 contains multiple bug fixes for write ahead log recovery. Writeahead log recovery is the process of restoring data that was in memory when atablet server died. These bugs could lead to data loss and are present in1.8.0, 1.8.1, and 1.9.0. Because the bugs can negatively impact Accumulo’smetadata table, even users that mainly use bulk import may be affected. Itis strongly recommended that anyone using 1.8.0 or greater upgradeimmediately. For more information see issues #441 and #449. These issueswere fixed in pull request #443 and #458.The only users who would not be affected by these bugs would be users alreadyrunning Accumulo without the recommended write-ahead logs enabled at all(durability: none), including for the metadata tables. Such users are alreadyrisking data loss when a server fails, but are not subject to any additionalrisk from these bugs, which occur during automated recovery from such failures.Some WAL recovery files were not being properly cleaned upA less serious bug than the above critical bugs was discovered and fixed,pertaining to write-ahead log recovery. This bug involved recovery files notbeing removed properly when no longer required and was fixed in #432.UpgradingView the Upgrading Accumulo documentation for guidance.TestingContinuous ingest with agitation and all integration test were run against thisversion. Continuous ingest was run with 9 nodes for 24 hours followed by asuccessful verification. The integration tests were run against both Hadoop2.6.4 and Hadoop 3.0.0.Useful Links  All tickets related to this release",
      "url": " /release/accumulo-1.9.1/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-9-0": {
      "title": "Apache Accumulo 1.9.0",
      "content": "Apache Accumulo 1.9.0 is a minor release on the 1.x branch. This release wouldbe considered a maintenance release on 1.8 branch except there are some APIadditions which resulted in a new minor release. Users of 1.8.x versions ofAccumulo should upgrade to 1.9.0. There will be no more bug fix releases on the1.8 branch. This release contains changes for nearly a hundred issues. SeeGitHub and JIRA for a list of changes.Below are resources for this release:  User Manual - In-depth developer and administrator documentation.  Javadocs - Accumulo 1.9.0 API  Examples - Code with corresponding readme files that give step bystep instructions for running example code.Notable ChangesDeprecated ClientConfiguration API using commons configIn ACCUMULO-4611, public API in ClientConfiguration using commons configtypes was deprecated to better support Hadoop 3 in the future. New methods wereadded to replace these methods which cause this release to be a 1.9.0 release.These changes allow removal of commons config from Accumulo’s API in 2.0.0.  Ifusing ClientConfiguration, then switching from existing constructors to the newstatic methods create(), fromFile(), or fromMap() will ensure your codeworks in 2.0.0.Performance ImprovementsAccumulo was profiled while running lots of concurrent small scans. During thisexercise these performance bugs were found and fixed: #379, ACCUMULO-4778,ACCUMULO-4779, ACCUMULO-4781, ACCUMULO-4782, ACCUMULO-4788,ACCUMULO-4789, ACCUMULO-4790, ACCUMULO-4797, ACCUMULO-4798,ACCUMULO-4799, ACCUMULO-4800, ACCUMULO-4801, ACCUMULO-4805, andACCUMULO-4809Below are other significant performance improvements in 1.9.0:  ACCUMULO-4636 - System iterator performance improvements  ACCUMULO-4657 - Avoided expensive BulkImport logging  ACCUMULO-4667 - Avoided unnecessary recomputation in LocalityGroupIterator  #410 - Fixed inefficient auths checkFixed upgrade process to set version on all volumesDuring upgrades, only one volume in a multiple HDFS volume was updated with thecorrect version. This would cause all tablet servers to complain and ultimatelyfail. ACCUMULO-4686 fixes this by setting the version on all volumes.Updated Accumulo to work with new releases of GuavaIn ACCUMULO-4702, dependencies on Beta-annotated Guava classes and methodswere removed. While Accumulo still includes Guava 14 in its tarball, it willwork with newer versions of Guava in client code. It has been tested to workwith Guava 23.Updated RFile to prevent very large blocksRFiles now use windowed statistics (ACCUMULO-4669) to prevent very largeblocks. In 1.8.0 a bug was introduced that caused RFile data block sizes togrow very large in the case where key sizes slowly increased.  This could leadto degraded query performance or out of memory exceptions on tablet servers.Allow iterators to yieldIn ACCUMULO-4643 added capability for an iterator to yield control in a seekor next call prior to finding a key/value.  Yielding avoids starvation of otherscans when iterators take a long time to return a key/value. To use thisfeature, implement YieldingKeyValueIterator.Disallow dots (.) in iterator namesIn ACCUMULO-3389, we added a check to prevent iterators from being created byour API which contained the dot (.) character. In some cases, the presence of adot character could be parsed incorrectly as an iterator option rather thanpart of its name. This caused unexpected problems. Iterator names are no longerallowed to contain dots. Any user code doing that will break with anIllegalArgumentException.Various security-related improvements  #417 - Make TLSv1.2 the default for TLS RPC connections  ACCUMULO-2806 - accumulo init sets the correct permissions on /accumuloto 700  ACCUMULO-4587 - use a newer version of JQuery (3.2.1)  ACCUMULO-4660 - sanitized incoming values from HTTP parameters  ACCUMULO-4665 and ACCUMULO-4666 - Kerberos improvements  ACCUMULO-4676 - set the HTTPOnly flags for JSESSSIONID in monitorOther Notable Changes  #403 - Enabled more metrics reporting  ACCUMULO-4528 - Add import/export table info to docs  ACCUMULO-4655 - Added a Response Time column to the monitor  ACCUMULO-4693 - Add process name to metrics  ACCUMULO-4721 - Document rfile-info in the user manualUpgradingView the Upgrading Accumulo documentation for guidance.TestingContinuous ingest, random walk, and all integration test were run against RC1.Randomwalk was run for 2 days with 7 walkers.  Continuous ingest was run with 9nodes for 24 hours followed by a successful verification.Useful Links  All tickets on GitHub related to this release  All tickets on JIRA related to this release",
      "url": " /release/accumulo-1.9.0/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-7-4": {
      "title": "Apache Accumulo 1.7.4",
      "content": "Apache Accumulo 1.7.4 is a maintenance release on the 1.7 version branch. This release contains changes from 46 issues, comprised of bug-fixes,performance improvements, build quality improvements, and more. See Jira or the bottom of this page for a complete list.Below are resources for this release:  User Manual - In-depth developer and administrator documentation.  Javadocs - Accumulo 1.7.4 API  Examples - Code with corresponding readme files that give step by step instructions for running example code.Accumulo follows Semantic Versioning guidelines.  This release is a  “patch version”, which means thatonly backwards compatible bug fixes are introduced in this version. A bug fix is defined as an internal change that fixes incorrect behavior. Usersof any previous 1.7.x release are strongly encouraged to update as soon as possible to benefit from the  bug fixes with very little concern in changeof underlying functionality.  As always, the Accumulo developers take API compatibility very seriously and have invested much time to ensure that wemeet the promises set forth to our users. Users of 1.6 or earlier that are seeking to upgrade to 1.7 should consider 1.7.4 as a starting point.Major ChangesFixed upgrade process to set version on all volumesDuring upgrades, only one volume in a multiple HDFS volume was updated with the correct version. This would cause all tablet serversto complain and ultimately fail. ACCUMULO-4686 fixes this by setting the version on all volumes.Updated Accumulo to work with new releases of GuavaIn ACCUMULO-4702, dependencies on Beta-annotated Guava classes and methods were removed. While Accumulo stillincludes Guava 14 in its tarball, it will work with newer versions of Guava in client code. It has been tested to workwith Guava 23.Updated RFile to prevent very large blocksRFiles now use windowed statistics (ACCUMULO-4669) to prevent very large blocks.  In 1.7.3 a bug was introduced thatcaused RFile data block sizes to grow very large in the case where key sizes slowly increased.  This could leadto degraded query performance or out of memory exceptions on tablet servers.Notable Changes  ACCUMULO-4506 - Add a timeout to a replication RPC call  ACCUMULO-4619 - Add splits hung forever  ACCUMULO-4633 - Added check to prevent division by zero  ACCUMULO-4640 - Accumulo shell is expecting instance.volumens in client.conf instead of accumulo-site.xml  ACCUMULO-4657 - BulkImport Performance Bottleneck  ACCUMULO-4665 - Must use the “real” user for RPCs when Kerberos is enabled  ACCUMULO-4676 - Use HTTPOnly flags in monitor to prevent XSS attacks  ACCUMULO-4776 - Fix advertised host in monitor  ACCUMULO-4777 - Root tablet got spammed with 1.8 million log entries  ACCUMULO-4787 - Close input stream in AccumuloReplicaSystem  ACCUMULO-4809 - Avoid blocking during session clean up  ACCUMULO-4817 - Update build plugins and parent POMUpgradingView the Upgrading Accumulo documentation for guidance.TestingContinuous ingest, random walk, and all integration test were run against RC0.A few bugs were found and fixed before RC1.  Randomwalk was run overnight with4 walkers.  Continuous ingest was run with 9 nodes for 22 hours followed by asuccessful verification.  A Kerberos IT failed because the 1.7.4 pom dependson Hadoop 2.2.0, the IT passed with a newer version of Hadoop.All Changes  ACCUMULO-1972 - Fixed Range constructor  ACCUMULO-3208 - Integration test for the OrIterator and cleanup  ACCUMULO-3283 - Create ColumnFQ only once  ACCUMULO-3827 - Set default store types for monitor ssl to jks  ACCUMULO-4170 - Clarify ClientConfiguration javadocs  ACCUMULO-4365 - Fixes to prevent intermittent failures in ShellServerIT and ConditionalWriterIT  ACCUMULO-4482 - Mention snappy compression in docs  ACCUMULO-4506 - Add a timeout to a replication RPC call  ACCUMULO-4546 - Create default log message for table error  ACCUMULO-4555 - Removes parsing of version string in Version class  ACCUMULO-4576 - Suppress warnings from deprecations  ACCUMULO-4587 - Upgrade Monitor jquery to 3.2.1  ACCUMULO-4591 - Add replication latency metrics  ACCUMULO-4602 - Deleted AssignmentThreadsIT  ACCUMULO-4619 - Add splits hung forever  ACCUMULO-4627 - Add corrupt WAL recovery instructions to user manual  ACCUMULO-4633 - Added check to prevent division by zero  ACCUMULO-4636 - System iterator improvements  ACCUMULO-4640 - Accumulo shell is expecting instance.volumens in client.conf instead of accumulo-site.xml  ACCUMULO-4648 - Update voting text in build.sh script  ACCUMULO-4657 - BulkImport Performance Bottleneck  ACCUMULO-4658 - Cache call to ZooKeeper to get table id map for the duration of the method  ACCUMULO-4660 - Sanitize incoming values from HTTP parameters  ACCUMULO-4662 - Fix ambiguous table reference  ACCUMULO-4665 - Use UGI with real Kerberos credentials  ACCUMULO-4666 - Improve KerberosToken sanity-checks and related doc  ACCUMULO-4669 - Use windowed statistics in RFile  ACCUMULO-4676 - Use HTTPOnly flags in monitor to prevent XSS attacks  ACCUMULO-4682 - Enforce import ordering  ACCUMULO-4684 - Basic schema outline for accumulo:replication  ACCUMULO-4685 - Fix minor warnings and bug in FileRefTest  ACCUMULO-4686 - Fix upgrade process to set version in all volumes.  ACCUMULO-4687 - Clean up some static-analysis warnings  ACCUMULO-4699 - Fixed queued work key in replication  ACCUMULO-4702 - Code fixes to work with Guava 23  ACCUMULO-4713 - Correct handling min and max timestamps  ACCUMULO-4716 - Don’t cache blks over max array size  ACCUMULO-4721 - Document rfile-info in the user manual  ACCUMULO-4741 - Remove minified files in Monitor  ACCUMULO-4774 - Made conditional writer thread into daemon  ACCUMULO-4776 - Fix advertised host in monitor  ACCUMULO-4777 - Root tablet got spammed with 1.8 million log entries  ACCUMULO-4780 - Add overflow check to seq num in CommitSession  ACCUMULO-4781 - Fixed logging performance issue  ACCUMULO-4787 - Close input stream in AccumuloReplicaSystem  ACCUMULO-4809 - Avoid blocking during session cleanup  ACCUMULO-4817 - Update build plugins and parent POM",
      "url": " /release/accumulo-1.7.4/",
      "categories": "release"
    }
    ,
  
    "blog-2018-03-22-view-metrics-in-grafana-html": {
      "title": "View Accumulo metrics in Grafana",
      "content": "The Accumulo monitor provides the easiest way to view Accumulo metrics but it has two limitations:  The overview page is limited to 10 graphs  These graphs only show metrics for the past two hoursThe Accumulo monitor could be improved to be better a visualization tool for metrics but this doesn’t makesense when tools like Grafana, Graphite, and Ganglia exist and Accumulo can report metrics to these toolsusing Hadoop Metrics2. While it’s easy to configure Accumulo to send metrics, the hard part issetting up and configuring a metrics visualization tool (i.e Graphite, Ganglia, Grafana/InfluxDB) to collectand view these metrics.To ease this burden, this post describes how to send Accumulo metrics to InfluxDB, a time series database,and make them viewable in Grafana, a visualization tool.Below is a screenshot of Accumulo metrics in Grafana:Set up metrics using UnoUno can setup and configure InfluxDB/Grafana when it sets up Accumulo using the following command:uno setup accumulo --with-metricsMetrics from Accumulo will be immediately viewable in Grafana at http://localhost:3000/.Set up metrics manually  Follow the standard installation instructions for InfluxDB and Grafana. As for versions,the instructions below were written using InfluxDB v0.9.4.2 and Grafana v2.5.0.  Add the following to your InfluxDB configuration to configure it accept metrics in Graphiteformat from Accumulo. The configuration below contains templates that transform the Graphitemetrics into a format that is usable in InfluxDB.    [[graphite]]  bind-address = &quot;:2003&quot;  enabled = true  database = &quot;accumulo_metrics&quot;  protocol = &quot;tcp&quot;  consistency-level = &quot;one&quot;  separator = &quot;_&quot;  batch-size = 1000  batch-pending = 5  batch-timeout = &quot;1s&quot;  templates = [    &quot;accumulo.*.*.*.*.*.*.* measurement.measurement.measurement.d.e.f.host.measurement&quot;,    &quot;accumulo.*.*.*.*.*.* measurement.measurement.measurement.d.e.host.measurement&quot;,    &quot;accumulo.*.*.*.*.* measurement.measurement.measurement.d.host.measurement&quot;,  ]        Configure the Accumulo configuration file hadoop-metrics2-accumulo.properties to send Graphitemetrics to InfluxDB. Below is example configuration. Remember to replace &amp;lt;INFLUXDB_HOST&amp;gt; withthe actual host.     *.period=30 accumulo.sink.graphite.class=org.apache.hadoop.metrics2.sink.GraphiteSink accumulo.sink.graphite.server_host=&amp;lt;INFLUXDB_HOST&amp;gt; accumulo.sink.graphite.server_port=2003 accumulo.sink.graphite.metrics_prefix=accumulo        Make sure the reporting frequency is set to 30 sec (i.e *.period=30). This is required if you areusing the provided Grafana dashboards that is configured in the next step.    Grafana needs to be configured to load dashboard JSON templates from a directory. Before restartingGrafana, you should copy this Accumulo dashboard template to the dashboards/ directoryconfigured below.     [dashboards.json] enabled = true path = &amp;lt;GRAFANA_HOME&amp;gt;/dashboards        If you restart Grafana, you will see the Accumulo dashboard configured but all of their charts willbe empty unless you have Accumulo running and configured to send data to InfluxDB. When you startsending data, you may need to refresh the dashboard page in the browser to start viewing metrics.",
      "url": " /blog/2018/03/22/view-metrics-in-grafana.html",
      "categories": "blog"
    }
    ,
  
    "blog-2018-03-16-moving-to-github-issues-html": {
      "title": "Migrating to GitHub Issues",
      "content": "Apache Accumulo is migrating from JIRA to GitHub issues. The migration is starting with an initial plan, but it may change as the community uses it.  For a description of the initial operating plan see the issues section of How to Contribute and the triaging issues section of Making a Release.MotivationDoing routine activities in less time is one motivation for the migration.  Below are some examples.  For GitHub a pull request is an issue. Therefore, creating an issue before a pull request is optional. Before the migration, an issue needed to be created in JIRA before creating a pull request. This was cumbersome for small changes.  When browsing commits in GitHub, issue numbers can be clicked.  Discussions on pull request can easily reference issues with simple markdown syntax.  Commits can close issues if the commit message contains “fixes #xyz”MigrationThere is no plan to migrate all existing issues in JIRA. The plan is to only migrate issues that someone is interested in or are actively being worked. Migration is done by linking the JIRA and GitHub issues to each other and then closing the JIRA issue. No new issues should be opened in JIRA.  JIRA will eventually be transitioned to a read only state.",
      "url": " /blog/2018/03/16/moving-to-github-issues.html",
      "categories": "blog"
    }
    ,
  
    "blog-2017-12-12-take-the-accumulo-tour-html": {
      "title": "Take the Accumulo Tour!",
      "content": "Apache Accumulo now has a hands-on tour that introduces users to key Accumulo conceptsby completing a series of programming exercises. Don’t worry! Solutions are provided ifyou get stuck.The tour has a ‘Getting Started’ page that helps you set up your development environment andclone a repository with template code. After you are set up, the tour has exercises that cover thefollowing Accumulo concepts:  Writing and Reading from Accumulo  Data Model  Authorizations  Ranges &amp;amp; Splits  Batch Scanner  Conditional WriterMore exercises (such as one on Accumulo Iterators) will be added in the future so check for updates.",
      "url": " /blog/2017/12/12/take-the-accumulo-tour.html",
      "categories": "blog"
    }
    ,
  
    "blog-2017-07-27-accumulo-summmit-on-october-16-html": {
      "title": "Accumulo Summit is on October 16th!",
      "content": "The fourth annual Accumulo Summit will be held on October 16, 2017 at the Sheraton Hotel in Columbia, MD.Registration is now open! Early bird pricing ends on August 14th.Submit a talk if you would like to speak at the event.",
      "url": " /blog/2017/07/27/accumulo-summmit-on-october-16.html",
      "categories": "blog"
    }
    ,
  
    "blog-2017-06-29-accumulo-documentation-improvements-html": {
      "title": "Documentation Improvements for 2.0",
      "content": "Since Accumulo 1.7, the Accumulo user manual source resided in the source repository as asciidoc. For every release or update to the manual,an HTML document is produced from the asciidoc, committed to the project website repository, and published to the website. This process will remainfor 1.x releases.For 2.0, the source for the user manual was converted to markdown and moved to the website repository. Theupcoming 2.0 documentation has several improvements over the older documentation:  Improved navigation using a new sidebar  Changes to the documentation are now immediately viewable on the website  Better linking to Javadocs and between documentation pages  Documentation style now matches the websiteWhile the 2.0 documentation is viewable, it is not linked to (except by this post) and every page contains a warning that the documentationis for a future release. Each page also links to the documentation for the latest release.It is now much easier to view, edit, and propose changes to the documentation. If you would like to contribute to the documentation for 2.0, viewthe documentation. Each page has an Edit this page link that will take you to GitHub where you can edit the markdown for the page, preview it,and submit a pull request to the website repository. A committer will review your changes so don’t be afraid to contribute!",
      "url": " /blog/2017/06/29/accumulo-documentation-improvements.html",
      "categories": "blog"
    }
    ,
  
    "blog-2017-04-21-introducing-uno-and-muchos-html": {
      "title": "Introducing Uno and Muchos",
      "content": "While Accumulo’s installation instructions are simple, it can be time consuming to install Accumulogiven its requirement on Hadoop and Zookeeper being installed and running. For a one-time productioninstallation, this set up time (which can take up to an hour) is not much of an inconvenience. However, it can become a burdenfor developers who need to frequently set up Accumulo to test code changes, switch between differentversions, or start a fresh instance on a laptop.Uno and Muchos are tools that ease the burden on developers of installing Accumulo and its dependencies.The names of Uno and Muchos indicate their use case. Uno is designed for running Accumulo on a single nodewhile Muchos is designed for running Accumulo on a cluster. While Uno and Muchos will install by default the mostrecent stable release of Accumulo, Hadoop, and Zookeeper, it is easy to configure different versions tomatch a production cluster.The sections below show how to use these tools. For more complete documentation, view their respective GitHubpages.UnoUno is a command line tool that sets up Accumulo on a single machine. It can be installed by cloning theUno git repo.git clone https://github.com/apache/fluo-uno.gitcd fluo-unoUno works out of the box but it can be customized by modifying conf/uno.conf.First, download the Accumulo, Hadoop, and Zookeeper tarballs from Apache by using the command below:./bin/uno fetch accumuloThe fetch command places all tarballs in the downloads/ directory. Uno can be configured (in conf/uno.conf)to build an Accumulo tarball from a local git repo when fetch is called.After downloading tarballs, the command below sets up Accumulo, Hadoop &amp;amp; Zookeeper in the install/ directory../bin/uno setup accumuloAccumulo, Hadoop, &amp;amp; Zookeeper are now ready to use. You can view the Accumulo monitor athttp://localhost:9995. You can configure your shell using the command below:eval &quot;$(./bin/uno env)&quot;Run uno stop accumulo to cleanly stop your cluster and uno start accumulo to start it again.If you need a fresh cluster, you can run uno setup accumulo again. To kill your cluster, run uno kill.MuchosMuchos is a command line tool that launches an AWS EC2 cluster with Accumulo set up on it. It is installed bycloning its git repo.git clone https://github.com/apache/fluo-muchos.gitcd fluo-muchosBefore using Muchos, create muchos.props in conf/ and edit it for your AWS environment.cp conf/muchos.props.example conf/muchos.propsvim conf/muchos.propsNext, run the command below to launch a cluster in AWS.muchos launch -c myclusterAfter launching the cluster, set up Accumulo on it using the following command.muchos setupUse muchos ssh to ssh to the cluster and muchos terminate to terminate all EC2 nodes when you are finished.ConclusionUno and Muchos automate installing Accumulo for development and testing. While not recommended for productionuse at this time, Muchos is a great reference for running Accumulo in production. System administrators canreference the Ansible code in Muchos to automate management of their own clusters.",
      "url": " /blog/2017/04/21/introducing-uno-and-muchos.html",
      "categories": "blog"
    }
    ,
  
    "release-accumulo-1-7-3": {
      "title": "Apache Accumulo 1.7.3",
      "content": "Apache Accumulo 1.7.3 is a maintenance release on the 1.7 version branch. This release contains changes from 79 issues, comprised of bug-fixes,performance improvements, build quality improvements, and more. SeeJIRA for a complete list.Below are resources for this release:  User Manual : In-depth developer and administrator documentation.  Javadocs  : Accumulo 1.7.3 API  Examples : Code with corresponding readme files that give step by step instructions for running example code.Accumulo follows Semantic Versioning guidelines.  This release is a“patch version”, which  means that only backwards compatible bug fixes are introduced in this version. A bug fix is defined asan internal change that fixes incorrect behavior. Users of any previous 1.7.x release are strongly encouraged to update as soon as possible to benefit from thebug fixes with very little concern in change of underlying functionality.  As always, the Accumulo developers take API compatibility very seriouslyand have invested much time to ensure that we meet the promises set forth to our users. Users of 1.6 or earlier that are seeking to upgrade to 1.7 shouldconsider 1.7.3 as a starting point.Major ChangesTablet Server Performance ImprovementACCUMULO-4458 mitigated some contention on the Hadoop configuration instance backing the XML configsread for SiteConfiguration. This should improve overall Tablet Server performance.Synchronization issue with deep copies of sourcesDeep copies of iterator sources were not thread safe and threw exceptions, mostly down in the ZlibDecompressor library. The real bug was in theBoundedRangeFileInputStream. The read() method synchronizes on the underlying FSDataInputStream, however the available() method did not.See ACCUMULO-4391.System permission bug in Thrift ProxyThe Accumulo Proxy lacked support for the following system permissions:  System.CREATE_NAMESPACE  System.DROP_NAMESPACE  System.ALTER_NAMESPACE  System.OBTAIN_DELEGATION_TOKENWithout these permissions, proxy users would get an AccumuloException if attemptingany of those operations.  Fixed in ticket ACCUMULO-4519.HostRegexTableLoadBalancer used stale informationThe HostRegexTableLoadBalancer maintains an internal mapping of tablet server pools and tablet server status. It was updated at a configurable intervalinitially as an optimization. Unfortunately it had the negative side effect of providing the assignment and balance operations with stale information.This lead to a constant shuffling of tablets. The configuration property was removed so that assign/balance methods get updated information every time.See ACCUMULO-4576.Modify TableOperations online to check for table stateThe table operations online operation executes as a fate operation. If a transaction lock for the table is currently held, this operation will block evenif no action is needed. ACCUMULO-4574 changes the behavior of the online operation to a NOOP if thetable is already in the requested state. This returns immediately without queuing a fate operation.Other Notable Changes  ACCUMULO-4600 Shell does not fall back to accumulo-site.xml when on classpath  ACCUMULO-4597 NPE from RFile PrintInfo when RF has more than 1,000 column families.  ACCUMULO-4488 Fix gap in user manual on Kerberos for clients  ACCUMULO-2724 CollectTabletStats had multiple -t parameter  ACCUMULO-4431 Log what random is chosen for a tserver.  ACCUMULO-4549 Remove duplicate init functions in TabletBalancer  ACCUMULO-4467 Random Walk broken because of unmet dependency on commons-math  ACCUMULO-4578 Cancel compaction FATE operation does not release namespace lock  ACCUMULO-4505 Shell still reads accumulo-site.xml when using Zookeeper CLI options  ACCUMULO-4535 HostRegexTableLoadBalancer fails with NullPointerException  ACCUMULO-4575 Concurrent table delete operations leave orphan fate transaction locksUpgradingThe recommended way to upgrade from a prior 1.7.x release is to stop Accumulo, upgrade to 1.7.3 and then start 1.7.3.When upgrading, there is a known issue if the upgrade fails due to outstanding FATEoperations, see ACCUMULO-4496 The work around if this situation is encountered:  Start tservers  Start shell  Run fate print to list all  If completed, just delete with fate delete  Start masters once there are no more fate operationsIf any of the FATE operations are not complete, you should rollback the upgrade and troubleshoot completing them with your prior version.When performing an upgrade between major versions, the upgrade is one-way, therefore it is important that you do not have any outstandingFATE operations before starting the upgrade.From 1.6 to 1.7Upgrades from 1.6 to 1.7 are be possible with little effort as no changes were made at the data layer and RPC changeswere made in a backwards-compatible way. The recommended way is to stop Accumulo 1.6, perform the Accumulo upgrade to1.7, and then start 1.7. Like previous versions, after 1.7.0 is started on a 1.6 instance, a one-time upgrade willhappen by the Master which will prevent a downgrade back to 1.6. Upgrades are still one way. Upgrades from versionsprior to 1.6 to 1.7 should follow the below path to 1.6 and then perform the upgrade to 1.7 – direct upgrades to 1.7for versions other than 1.6 are untested.After upgrading to 1.7.0, users will notice the addition of a replication table in the accumulo namespace. Thistable is created and put offline to avoid any additional maintenance if the data-center replication feature is notin use.Existing configuration files from 1.6 should be compared against the examples provided in 1.7. The 1.6 configurationfiles should all function with 1.7 code, but you will likely want to include a new file (hadoop-metrics2-accumulo.properties)to enable the new metrics subsystem. Read the section on Hadoop Metrics2 in the Administration chapter of the Accumulo User Manual.For each of the other new features, new configuration properties exist to support the feature. Refer to the addedsections in the User Manual for the feature for information on how to properly configure and use the new functionality.TestingEach unit and functional test only runs on a single node, while the RandomWalk and Continuous Ingest tests run on any number of nodes. Agitation refers to randomly restarting Accumulo processes and Hadoop Datanode processes, and, in HDFS High-Availability instances, forcing NameNode failover.            OS/Environment      Hadoop      Nodes      ZooKeeper      HDFS HA      Tests                  CentOS 7, openJDK 1.8 from CentOS yum repo, EC2; 1 leader m3.xlarge, 8 workers d2.xlarge      2.7.3      9      3.4.9      No      24 HR Continuous Ingest without Agitation              CentOS 7, openJDK 1.8 from CentOS yum repo, EC2; 1 leader m3.xlarge, 8 workers d2.xlarge      2.7.3      9      3.4.9      No      24 HR Continuous Ingest Agitation      ",
      "url": " /release/accumulo-1.7.3/",
      "categories": "release"
    }
    ,
  
    "blog-2017-03-21-happy-anniversary-accumulo-html": {
      "title": "Happy Anniversary Accumulo",
      "content": "This month, Apache Accumulo is celebrating five years as a top-level projectat the Apache Software Foundation.To celebrate, we got a cake!The community would like to thank everyone that has been a part of Accumuloover the past years and to those involved before that during its time in the ASF Incubator.As always, we look forward to continued activity from everyone. Happy Accumul-ating!",
      "url": " /blog/2017/03/21/happy-anniversary-accumulo.html",
      "categories": "blog"
    }
    ,
  
    "blog-2017-03-06-security-performance-implications-html": {
      "title": "Security Performance Implications",
      "content": "The purpose of this two part series was to measure the performance impact ofvarious security configurations on a cluster running Apache Accumulo’scontinuous ingest suite. The tests were performed using Amazon WebServices (AWS), Hortonworks Data Platform 2.4 and Accumulo 1.7. Each ofthe five different security settings in Accumulo 1.7 was tested includingno security, SSL, and SASL with Kerberos authentication for the three qualityof protection levels (auth, auth-int, auth-conf).  KDC was MIT.  HDFS wasconfigured to use Kerberos for authentication and had service levelauthorization on. Other than that, no other security settings (HTTPS, RPCprotection, data transfer encryption, etc) were enabled.  Timely was aseparate, single node HDFS/Zookeeper/Accumulo instance.IntroAll runs utilized the continuous ingest suite that ships with Accumulo (astandard method to measure performance in Accumulo).  It generates randomgraph data and inserts it into Accumulo, creatinga long linked list of entries.  Part 1 was run with just continuous ingest.Based on the test results, there was a measurable performance impact as each additional security configuration was put in place.MethodologyWe ran 5 tests, one for each security configuration.  Each iteration of each test inserted 2 billion entries.  Batch writers were configured with 500K max memto artificially inflate the overall write overhead. This was performed on asmall cluster on AWS.Each test used one of the following security configurations:  No security - Default  Two way SSL  Kerberos/SASL with auth          auth is just Kerberos authentication between client and server.  Each end of the RPC definitively knows who the other is.        Kerberos/SASL with auth-int          Builds on auth, also providing message integrity checks of the data going across the wire. You also know that the message you received was not altered.        Kerberos/SASL with auth-conf          Builds on auth-int, also providing confidentiality of the message that was sent to prevent others from reading it (aka wire-encryption).      For each test, five iterations were run to obtain a min, max, and mediantime elapsed at each security configuration. After each iteration,Hadoop, and Zookeeper processes were restarted, Accumulo tables arewiped clean and tables are recreated. In addition, pagecache, dentriesand inodes are dropped by issuing a ‘3’ command on/proc/sys/vm/drop_caches to ensure that the OS is not caching things to diskthat might affect the benchmark. The following sequence was performedbetween iterations:  Bring down Accumulo  Bring down Zookeeper  Bring down Hadoop  Run sync command  Drop OS cache  Bring up Hadoop  Bring up Zookeeper  Bring up Accumulo  Drop tables  Create tablesFor each iteration, the results were stored, fed into Timely, and viewed with Grafana.Since the runs were executed sequentially, the start epochs for each run did not align.To mitigate, the entries for each run were insertedwith the same relative epoch for convenient comparison in Grafana.The table configurations for Accumulo remain the same throughout thedifferent iterations and security levels. The Accumulo siteconfigurations differ only due to the different settings for thesecurity level configurations.EnvironmentIn order to perform the testing, a small AWS cluster was setup using 14hosts on EC2. Two i2.xlarge instances were used as master nodes and eightd2.xlarge instances were used for workers. In addition, two c4.4xlargeinstances were used for ingesters, one m4.2xlarge instance was used forTimely, and one m4.xlarge instance was used for Apache Ambari. A logicaldiagram of the setup is depicted below:Figure 1 - Cluster Layout, Roles, and Instance Types on AWS.The types of nodes and their function are given below:            Node Type      AWS EC2 Type      EC2 Type Details      Quantity                  Ingest Nodes      c4.4xlarge      16 core, 30 GB RAM      2              Worker Node      d2.xlarge      4 cores, 30.5 GB RAM, 3x2T GB HD      8              Master Node      i2.xlarge      4 cores, 30.5 GB RAM, 1x800GB SSD      2              Admin Node      m4.xlarge      4 cores, 16 GB RAM      1              Timely Node      m4.2xlarge      8 cores, 32 GB RAM      1      Table 1 – AWS Instance Types, Role, Details, and QuantitiesResultsThe median, max, and min of the milliseconds elapsedtime of all iterations for each test is displayed below. The percentage changecolumns compare the Median, Max, and Min respectively from the nosecurity level to each security configuration (e.g. no security Medianvs. auth-int Median, no security Max vs. auth-int Max).            Security Level      Median      Standard Deviation      Max      Min      % Change (nosec Median vs. Median)      % Change (nosec Max vs. Max)      % Change (nosec Min vs. Min)      Delta from Previous Level (Median)                  no security      7829394      139340      8143035      7764309      0.00%      0.00%      0.00%      0.00%              ssl      8292760      87012      8464060      8204955      5.92%      3.94%      5.68%      5.92%              auth      8859552      134109      9047971      8657618      13.16%      11.11%      11.51%      6.83%              auth-int      9500737      155968      9753424      9282371      21.34%      19.78%      19.55%      7.24%              auth-conf      9479635      170823      9776580      9282189      21.08%      20.06%      19.55%      -0.22%      Table 2 – Summarized Time Elapsed for Each Security LevelPlotsBelow are some snapshots of *stats.out elements via Grafana that were insertedinto Timely with the same relative start time.  Each graph represents a fieldin the output generated by ContinuousStatsCollectorTABLE_RECS(Number of records in the continuous ingest table.  Down sample=1m, aggregate=avg)TOTAL_INGEST(Ingest rate for Accumulo instance.  Down sample=5m, aggregate=avg)AVG_FILES/TABLET(Average number of files per Accumulo tablet.  Down sample=1m, aggregate=avg)ACCUMULO_FILES(Total number of files for Accumulo.  Down sample=1m, aggregate=avg)As can be seen in the plots above, the different security settings haverelatively consistent, discernible median run characteristics.  The bigdip in each TOTAL_INGEST coincides with a large number of majorcompactions, a rate decrease for TABLE_RECS, and a decrease inAVG_FILES/TABLET.Final ThoughtsThe biggest performancehits to run duration median (compared to default security) were ~21% forauth-int and auth-conf.  Interesting to note that SSL’s median run duration waslower than all SASL configs and that auth-conf’s was lower than auth-int.Initial  speculation for these oddities revolved around theThrift serverimplementations, but the Thrift differences will not explain the auth-conf/intdisparity since both utilize TThreadPoolServer.  It was certainly unexpected that theaddition of wire encryption would yield a faster median run duration.  This resultprompted, as a sanity check, sniffing the net traffic (in a contrived examplenot during a timed run) in both auth-conf and auth-int to ensure that the messagecontents were actually obfuscated in auth-conf (they were) and not obfuscated inauth-int (they weren’t).Future WorkPart 2 of this series will consist of the same continuous ingest loads andconfigurations with the addition of a query load on the system.",
      "url": " /blog/2017/03/06/security-performance-implications.html",
      "categories": "blog"
    }
    ,
  
    "release-accumulo-1-8-1": {
      "title": "Apache Accumulo 1.8.1",
      "content": "Apache Accumulo 1.8.1 is a maintenance release on the 1.8 version branch. Thisrelease contains changes from more then 40 issues, comprised of bug-fixes,performance improvements, build quality improvements, and more. SeeJIRA for a complete list.Below are resources for this release:  User Manual  Javadocs  ExamplesIn the context of Accumulo’s Semantic Versioning guidelines,this is a “minor version”. This means that new APIs have been created, somedeprecations may have been added, but no deprecated APIs have been removed.Code written against 1.7.x should work against 1.8.0 – binary compatibilityhas been preserved with one exception of an already-deprecated Mock Accumuloutility class. As always, the Accumulo developers take API compatibilityvery seriously and have invested much time to ensure that we meet the promises set forth to our users.Major ChangesProblem with scans right after minor compactionA bug was found when 2 or more concurrent scans run on a tablet thathas just undergone minor compaction. The minor compaction threadwrites the in-memory map to a local temporary rfile and tries toswitch the current iterators to use it instead of the native map. Theiterator code in the scan thread may also switch itself to use the localtemporary rfile it if notices it before the minor compaction threadsperforms the switch. The bug happened shortly after the switch whenone of the iterator threads will get a NegativeArraySizeException.See ACCUMULO-4483 for more info.Tablet Server Performance ImprovementACCUMULO-4458 mitigated some contention on the Hadoopconfiguration instance backing the XML configs read for SiteConfiguration.This should improve overall Tablet Server performance.Synchronization issue with deep copies of sourcesDeep copies of iterator sources were not thread safe and threwexceptions, mostly down in the ZlibDecompressor library.  The real bugwas in the BoundedRangeFileInputStream.  The read() methodsynchronizes on the underlying FSDataInputStream, however theavailable() method did not.   See ACCUMULO-4391.System permission bug in Thrift ProxyThe Accumulo Proxy lacked support for the following system permissions:  System.CREATE_NAMESPACE  System.DROP_NAMESPACE  System.ALTER_NAMESPACE  System.OBTAIN_DELEGATION_TOKENTicket is ACCUMULO-4519.Shell compaction file selection options can blockThe block happens when the tablet lock is held.  The tablet lock ismeant to protect changes to the tablets internal metadata, andblocking operations should not occur while this lock is held.  Thecompaction command has options to select files based on somecriteria, some of which required blocking operations.  This issue isfixed in ACCUMULO-4572.HostRegexTableLoadBalancer used stale informationThe HostRegexTableLoadBalancer maintains an internal mapping of tabletserver pools and tablet server status. It was updated at aconfigurable interval initially as an optimization. Unfortunately ithad the negative side effect of providing the assignment and balanceoperations with stale information.  This lead to a constant shufflingof tablets.  The configuration property was removed so thatassign/balance methods get updated information every time.  SeeACCUMULO-4576.Modify TableOperations online to check for table stateThe table operations online operation executes as a fateoperation. If a transaction lock for the table is currently held,this operation will block even if no action is needed.ACCUMULO-4574 changes the behavior of the onlineoperation to a NOOP if the table is already in the requested state.This returns immediately without queuing a fate operation.Other Notable Changes  ACCUMULO-4488 Fix gap in user manual on Kerberos for clients  ACCUMULO-2724 CollectTabletStats had multiple -t parameter  ACCUMULO-4431 Log what random is chosen for a tserver.  ACCUMULO-4494 Include column family seeks in the Iterator Test Harness  ACCUMULO-4549 Remove duplicate init functions in TabletBalancer  ACCUMULO-4467 Random Walk broken because of unmet dependency on commons-math  ACCUMULO-4578 Cancel compaction FATE operation does not release namespace lock  ACCUMULO-4505 Shell still reads accumulo-site.xml when using Zookeeper CLI options  ACCUMULO-4535 HostRegexTableLoadBalancer fails with NullPointerException  ACCUMULO-4575 Concurrent table delete operations leave orphan fate transaction locksUpgradingUpgrades from 1.7 to 1.8 are possible with little effort as no changes were made at the data layer and RPC changeswere made in a backwards-compatible way. The recommended way is to stop Accumulo 1.7, perform the Accumulo upgrade to1.8, and then start 1.8. Like previous versions, after 1.8 is started on a 1.7 instance, a one-time upgrade willhappen by the Master which will prevent a downgrade back to 1.7. Upgrades are still one way. Upgrades from versionsprior to 1.7 to 1.8 should follow the below path to 1.7 and then perform the upgrade to 1.8 – direct upgrades to 1.8for versions other than 1.7 are untested.Existing configuration files from 1.7 should be compared against the examples provided in 1.8. The 1.7 configurationfiles should all function with 1.8 code, but you will likely want to include changes found in the1.8.0 release notes and these release notes for 1.8.1.For upgrades from prior to 1.7, follow the upgrade instructions to 1.7 first.TestingEach unit and functional test only runs on a single node, while the RandomWalkand Continuous Ingest tests run on any number of nodes. Agitation refers torandomly restarting Accumulo processes and Hadoop Datanode processes, and, inHDFS High-Availability instances, forcing NameNode failover.            OS/Environment      Hadoop      Nodes      ZooKeeper      HDFS HA      Tests                  CentOS7/openJDK1.8.0_121/EC2; 1 m3.xlarge leader, 8 d2.xlarge workers      2.7.3      9      3.4.9      No      24 HR Continuous Ingest without Agitation.              CentOS7/openJDK1.8.0_121/EC2; 1 m3.xlarge leader, 8 d2.xlarge workers      2.7.3      9      3.4.9      No      24 HR Continuous Ingest with Agitation.      ",
      "url": " /release/accumulo-1.8.1/",
      "categories": "release"
    }
    ,
  
    "blog-2016-12-19-running-on-fedora-25-html": {
      "title": "Running Accumulo on Fedora 25",
      "content": "Apache Accumulo has been available in Fedora since F20. Recently, the Fedorapackages have been updated to Accumulo version 1.6.6 and have made someimprovements to the default configuration and launch scripts to provide a goodout-of-box experience. This post will discuss the basic setup procedures forrunning Accumulo in the latest version, Fedora 25.Prepare the systemWARNING: Before you start, be sure you’ve got plenty of free disk space.Otherwise, you could run into this bug or see other problems.These instructions will assume you’re using Fedora 25, fully up-to-date (sudodnf --refresh upgrade).Install packagesFedora provides a meta-package to install Accumulo and all of its dependencies.It’s a good idea to install the JDK, so you’ll have access to the jpscommand, and tuned for setting system performance tuning parameters from aprofile. It’s also a good idea to ensure the optional hadoop native librariesare installed, and you have a good editor (replace vim with your preferrededitor):sudo dnf install accumulo java-1.8.0-openjdk-devel tuned vim hadoop-common-nativeIt is possible to install only a specific Accumulo service. For the single nodesetup, almost everything is needed. For the multi-node setup, it might makemore sense to be selective about which you choose to install on each node (forexample, to only install accumulo-tserver).Set up tuned(Optional) tuned can optimize your server settings, adjusting things likeyour vm.swappiness. To set up tuned, do:sudo systemctl start tuned.service     # start servicesudo tuned-adm profile network-latency # pick a good profilesudo tuned-adm active                  # verify the selected profilesudo systemctl enable tuned.service    # auto-start on rebootsSet up ZooKeeperYou’ll need to set up ZooKeeper, regardless of whether you’ll be running asingle node or many. So, let’s create its configuration file (the defaults arefine):sudo cp /etc/zookeeper/zoo_sample.cfg /etc/zookeeper/zoo.cfgNow, let’s start ZooKeeper (and set it to run on reboot):sudo systemctl start zookeeper.servicesudo systemctl enable zookeeper.serviceNote that the default port for ZooKeeper is 2181. Remember the hostname ofthe node where ZooKeeper is running, referred to as &amp;lt;zk-dns-name&amp;gt; later.Running a single nodeConfigure AccumuloTo run on a single node, you don’t need to run HDFS. Accumulo can use the localfilesystem as a volume instead. By default, it uses /tmp/accumulo. Let’schange that to something which will survive a reboot:sudo vim /etc/accumulo/accumulo-site.xmlChange the value of the instance.volumes property from file:///tmp/accumuloto file:///var/tmp/accumulo in the configuration file (or another preferredlocation).While you are editing the Accumulo configuration file, you should also changethe default instance.secret from DEFAULT to something else. You can alsochange the credentials used by the tracer service now, too. If you use theroot user, you’ll have to set its password to the same one you’ll use laterwhen you initialize Accumulo. If you use another user name, you’ll have tocreate that user later.Configure Hadoop clientHadoop’s default local filesystem handler isn’t very good at ensuring files arewritten to disk when services are stopped. So, let’s use a better filesystemimplementation for file:// locations. This implementation may not be asrobust as a full HDFS instance, but it’s more reliable than the default. Eventhough you’re not going to be running HDFS, the Hadoop client code used inAccumulo can still be configured by modifying Hadoop’s configuration file:sudo vim /etc/hadoop/core-site.xmlAdd a new property:  &amp;lt;property&amp;gt;    &amp;lt;name&amp;gt;fs.file.impl&amp;lt;/name&amp;gt;    &amp;lt;value&amp;gt;org.apache.hadoop.fs.RawLocalFileSystem&amp;lt;/value&amp;gt;  &amp;lt;/property&amp;gt;Initialize AccumuloNow, initialize Accumulo. You’ll need to do this as the accumulo user,because the Accumulo services run as the accumulo user. This user is createdautomatically by the RPMs if it doesn’t exist when the RPMs are installed. Ifyou already have a user and/or group by this name, it will probably not be aproblem, but be aware that this user will have permissions for the serverconfiguration files. To initialize Accumulo as a specific user, use sudo -u:sudo -u accumulo accumulo initAs expected, this command will fail if ZooKeeper is not running, or if thedestination volume (file:///var/tmp/accumulo) already exists.Start Accumulo servicesNow that Accumulo is initialized, you can start its services:sudo systemctl start accumulo-{master,tserver,gc,tracer,monitor}.serviceEnable the commands to start at boot:sudo systemctl enable accumulo-{master,tserver,gc,tracer,monitor}.serviceRunning multiple nodesAmazon EC2 setupFor a multi-node setup, the authors tested these instructions with a Fedora 25Cloud AMI on Amazon EC2 with the following characteristics:  us-east-1 availability zone  ami-e5757bf2 (latest in us-east-1 at time of writing)  HVM virtualization type  gp2 disk type  64GB EBS root volume (no additional storage)  m4.large and m4.xlarge instance types (tested on both)  3 nodesFor this setup, you should have a name service configured properly. Forconvenience, we used the EC2 provided internal DNS, with internal IP addresses.Make sure the nodes can communicate with each other using these names. Ifyou’re using EC2, this means making sure they are in the same security group,and the security group has an inbound rule for “All traffic” with the sourceset to itself (sg-xxxxxxxx).The default user is fedora for the Fedora Cloud AMIs. For the bestexperience, don’t forget to make sure they are fully up-to-date (sudo dnf--refresh upgrade).Configure and run HadoopConfiguring HDFS is the primary difference between the single and multi-nodesetup. For both Hadoop and Accumulo, you can edit the configuration files onone machine, and copy them to the others.Pick a server to be the NameNode and identify its DNS name,(&amp;lt;namenode-dns-name&amp;gt;). Edit Hadoop’s configuration to set the defaultfilesystem name to this location:sudo vim /etc/hadoop/core-site.xmlSet the value for the property fs.default.name tohdfs://&amp;lt;namenode-dns-name&amp;gt;:8020.Distribute copies of the changed configuration files to each node.Now, format the NameNode. You’ll need to do this as the hdfs user on theNameNode instance:sudo -u hdfs hdfs namenode -formatOn the NameNode, start the NameNode service and enable it on reboot:sudo systemctl start hadoop-namenode.servicesudo systemctl enable hadoop-namenode.serviceOn each DataNode, start the DataNode service:sudo systemctl start hadoop-datanode.servicesudo systemctl enable hadoop-datanode.serviceConfigure and run AccumuloUpdate Accumulo’s configuration to use this HDFS filesystem:sudo vim /etc/accumulo/accumulo-site.xmlChange the value of the instance.volumes tohdfs://&amp;lt;namenode-dns-name&amp;gt;:8020/accumulo in the configuration file. Don’tforget to also change the default instance.secret and the trace user’scredentials, if necessary. Also, since you will have multiple nodes, you cannotuse localhost:2181 for ZooKeeper, so set instance.zookeeper.host to&amp;lt;zk-dns-name&amp;gt;:2181.Distribute copies of the changed configuration files to each node.With HDFS now running, make sure Accumulo has permission to create itsdirectory in HDFS, and initialize Accumulo:sudo -u hdfs hdfs dfs -chmod 777 /sudo -u accumulo accumulo initAfter Accumulo has created its directory structure, you can change thepermissions for the root back to what they were:sudo -u hdfs hdfs dfs -chmod 755 /Note: we only choose to do the above because this is a developer/testingenvironment. Temporarily changing ownership of HDFS is not recommended forthe root of HDFS.Now, you can start Accumulo.On the NameNode, start all the Accumulo services and enable on reboot:sudo systemctl start accumulo-{master,tserver,gc,tracer,monitor}.servicesudo systemctl enable accumulo-{master,tserver,gc,tracer,monitor}.serviceOn each DataNode, start just the tserver and enable it on reboot:sudo systemctl start accumulo-tserver.servicesudo systemctl enable accumulo-tserver.serviceWatching and using AccumuloRun the shellRun a shell as Accumulo’s root user (the instance name and root password arethe ones you selected during the initialize step above:accumulo shell -u root -zh &amp;lt;zk-dns-name&amp;gt;:2181 -zi &amp;lt;instanceName&amp;gt;View the monitor pagesYou should also be able to view the NameNode monitor page and the Accumulomonitor pages. If you are running this in EC2, you can view these over an SSHtunnel using the NameNode’s public IP address. If you didn’t give this node apublic IP address, you can allocate one in EC2 and associate it with this node:ssh -L50070:localhost:50070 -L50095:localhost:50095 &amp;lt;user&amp;gt;@&amp;lt;host&amp;gt;Replace &amp;lt;user&amp;gt; with your username (probably fedora if using the FedoraAMI), and &amp;lt;host&amp;gt; with the public IP or hostname for your EC2 instance. Now,in your local browser, you should be able to navigate to these addresses inyour localhost: Hadoop monitor (http://localhost:50070) and Accumulomonitor (http://localhost:50095).Debugging commandsCheck the status of a service:sudo systemctl status &amp;lt;ServiceName&amp;gt;.serviceCheck running Java processes:sudo jps -mlCheck the system logs for a specific service within the last 10 minutes:sudo journalctl -u &amp;lt;ServiceName&amp;gt; --since &#39;10 minutes ago&#39;Check listening ports:sudo netstat -tlnpCheck DNS name for a given IP address:getent hosts &amp;lt;ipaddress&amp;gt; # ORhostname -APerform forward and reverse DNS lookups:sudo dnf install bind-utilsdig +short &amp;lt;hostname&amp;gt;     # forward DNS lookupdig +short -x &amp;lt;ipaddress&amp;gt; # reverse DNS lookupFind the instance ID for your instance name:zkCli.sh -server &amp;lt;host&amp;gt;:2181     # replace &amp;lt;host&amp;gt; with your ZooKeeper server DNS name&amp;gt; get /accumulo/instances/&amp;lt;name&amp;gt; # replace &amp;lt;name&amp;gt; with your instance name&amp;gt; quitIf the NameNode is listening on the loopback address, you’ll probably need torestart the service manually, as well as any Accumulo services which failed.This is a known issue with Hadoop:sudo systemctl restart hadoop-namenode.serviceSome helpful rpm commands:rpm -q -i &amp;lt;installed-package-name&amp;gt;              # to see info about an installed packagerpm -q -i -p &amp;lt;rpm-file-name&amp;gt;                    # to see info about an rpm filerpm -q --provides &amp;lt;installed-package-name&amp;gt;      # see what a package providesrpm -q --requires &amp;lt;installed-package-name&amp;gt;      # see what a package requiresrpm -q -l &amp;lt;installed-package-name&amp;gt;              # list package filesrpm -q --whatprovides &amp;lt;file&amp;gt;                    # find rpm which owns &amp;lt;file&amp;gt;rpm -q --whatrequires &#39;mvn(groupId:artifactId)&#39; # find rpm which requires maven coordsHelping outFeel free to get involved with the Fedora or Fedora EPEL(for RHEL/CentOS users) packaging. Contact the Fedora maintainers (user atfedoraproject dot org) for the Accumulo packages to see how you can helppatching bugs, adapting the upstream packages to the Fedora packagingstandards, testing updates, maintaining dependency packages, and more.",
      "url": " /blog/2016/12/19/running-on-fedora-25.html",
      "categories": "blog"
    }
    ,
  
    "blog-2016-11-16-simpler-scripts-and-config-html": {
      "title": "Simpler scripts and configuration coming in Accumulo 2.0.0",
      "content": "For the upcoming 2.0.0 release, Accumulo’s scripts and configuration were refactoredto make Accumulo easier to use. While Accumulo’s documentation (i.e. the usermanual and INSTALL.md) were updated with any changes that were made, this blog post providesa summary of the changes.Fewer scriptsBefore 2.0.0, the bin/ directory of Accumulo’s binary tarball contained about 20 scripts:$ ls accumulo-1.8.0/bin/accumulo             build_native_library.sh  generate_monitor_certificate.sh  start-here.sh    stop-server.shaccumulo_watcher.sh  check-slaves             LogForwarder.sh                  start-server.sh  tdown.shbootstrap_config.sh  config-server.sh         start-all.sh                     stop-all.sh      tool.shbootstrap_hdfs.sh    config.sh                start-daemon.sh                  stop-here.sh     tup.shThe number of scripts made it difficult to know which scripts to use.  If you added the bin/ directory to yourPATH, it could add unnecessary commands to your PATH or cause commands to be overridden due generic names(like ‘start-all.sh’). The number of scripts were reduced by removing scripts that are no longer used and combiningscripts with similar functionality.Starting with 2.0.0, Accumulo will only have 4 scripts in its bin/ directory:$ ls accumulo-2.0.0/bin/accumulo  accumulo-cluster  accumulo-service  accumulo-utilBelow are some notes on this change:  The ‘accumulo’ script was mostly left alone except for improved usage.  The ‘accumulo-service’ script was created to manage Accumulo processes as services  The ‘accumulo-cluster’ command was created to manage Accumulo on cluster and replaces ‘start-all.sh’ and ‘stop-all.sh’.  The ‘accumulo-util’ command combines many utility scripts such as ‘build_native_library.sh’, ‘tool.sh’, etc into one script.Less configurationBefore 2.0.0, Accumulo’s conf/ directory looked like the following (after creating initial config filesusing ‘bootstrap_config.sh’):$ ls accumulo-1.8.0/conf/accumulo-env.sh          auditLog.xml  generic_logger.properties            masters                    slavesaccumulo-metrics.xml     client.conf   generic_logger.xml                   monitor                    templatesaccumulo.policy.example  examples      hadoop-metrics2-accumulo.properties  monitor_logger.properties  tracersaccumulo-site.xml        gc            log4j.properties                     monitor_logger.xmlWhile all of these files have a purpose, many are only used in rare situations. For Accumulo 2.0, the ‘conf/’directory now only contains a minimum set of configuration files needed to run Accumulo.$ ls accumulo-2.0.0/conf/accumulo-env.sh  accumulo-site.xml  client.conf  log4j-monitor.properties  log4j.properties  log4j-service.properties  templatesThe Accumulo tarball does contain host files (i.e ‘tservers’, ‘monitor’, etc) by default as these files are only required bythe ‘accumulo-cluster’ command. However, the script has a command to generate them.$ ./bin/accumulo-cluster create-configAny less common configuration files can still be found in conf/templates.Better usageBefore 2.0.0, the ‘accumulo’ command had a limited usage:$ ./accumulo-1.8.0/bin/accumuloaccumulo admin | check-server-config | classpath | create-token | gc | help | info | init | jar &amp;lt;jar&amp;gt; [&amp;lt;main class&amp;gt;] args |  login-info | master | minicluster | monitor | proxy | rfile-info | shell | tracer | tserver | version | zookeeper | &amp;lt;accumulo class&amp;gt; argsFor 2.0.0, all ‘accumulo’ commands were given a short description and organized into the groups.  Below isthe full usage. It should be noted that usage is limited until the ‘accumulo-env.sh’ configuration file iscreated in conf/ by the accumulo create-config command.$ ./accumulo-2.0.0/bin/accumulo helpUsage: accumulo &amp;lt;command&amp;gt; [-h] (&amp;lt;argument&amp;gt; ...)  -h   Prints usage for specified commandCore Commands:  init                           Initializes Accumulo  shell                          Runs Accumulo shell  classpath                      Prints Accumulo classpath  version                        Prints Accumulo version  admin                          Executes administrative commands  info                           Prints Accumulo cluster info  help                           Prints usage  &amp;lt;main class&amp;gt; args              Runs Java &amp;lt;main class&amp;gt; located on Accumulo classpathProcess Commands:  gc                             Starts Accumulo garbage collector  master                         Starts Accumulo master  monitor                        Starts Accumulo monitor  minicluster                    Starts Accumulo minicluster  proxy                          Starts Accumulo proxy  tserver                        Starts Accumulo tablet server  tracer                         Starts Accumulo tracer  zookeeper                      Starts Apache Zookeeper instanceAdvanced Commands:  check-server-config            Checks server config  create-token                   Creates authentication token  login-info                     Prints Accumulo login info  rfile-info                     Prints rfile infoThe new ‘accumulo-service’ and ‘accumulo-cluster’ commands also have informative usage.$ ./accumulo-2.0.0/bin/accumulo-serviceUsage: accumulo-service &amp;lt;service&amp;gt; &amp;lt;command&amp;gt;Services:  gc          Accumulo garbage collector  monitor     Accumulo monitor  master      Accumulo master  tserver     Accumulo tserver  tracer      Accumulo tracerCommands:  start       Starts service  stop        Stops service  kill        Kills service$ ./accumulo-2.0.0/bin/accumulo-clusterUsage: accumulo-cluster &amp;lt;command&amp;gt; (&amp;lt;argument&amp;gt; ...)Commands:  create-config       Creates cluster config  start               Starts Accumulo cluster  stop                Stops Accumulo cluster  start-non-tservers  Starts all services except tservers  start-tservers      Starts all tservers on cluster  stop-tservers       Stops all tservers on cluster  start-here          Starts all services on this node  stop-here           Stops all services on this nodeThis post was updated on March 24, 2017 to reflect changes to Accumulo 2.0",
      "url": " /blog/2016/11/16/simpler-scripts-and-config.html",
      "categories": "blog"
    }
    ,
  
    "blog-2016-11-02-durability-performance-html": {
      "title": "Durability Performance Implications",
      "content": "OverviewAccumulo stores recently written data in a sorted in memory map.  Before data isadded to this map, it’s written to an unsorted write ahead log(WAL).  In thecase when a tablet server dies, the recently written data is recovered from theWAL.When data is written to Accumulo the following happens :  Client sends a batch of mutations to a tablet server  Tablet server does the following :          Writes mutation to tablet servers’ WAL      Sync or flush tablet servers’ WAL      Adds mutations to sorted in memory map of each tablet.      Reports success back to client.      The sync/flush step above moves data written to the WAL from memory to disk.Write ahead logs are stored in HDFS. HDFS supports two ways of forcing data todisk for an open file : hsync and hflush.HDFS Sync/Flush DetailsWhen hflush is called on a WAL, it does not guarantee data is on disk.  Itonly guarantees that data is in OS buffers on each datanode and on its way to disk.As a result calls to hflush are very fast.  If a WAL is replicated to 3 datanodes then data may be lost if all three machines reboot or die.  If the datanodeprocess dies, then data loss will not happen because the data was in OS bufferswaiting to be written to disk.  The machines have to reboot or die for data loss tooccur.In order to avoid data loss in the event of reboot, hsync can be called.  Thiswill ensure data is written to disk on all datanodes before returning.  Whenusing hsync for the WAL, if Accumulo reports success to a user it means thedata is on disk.  However hsync is much slower than hflush and the way it’simplemented exacerbates the problem.  For example hflush make take 1ms andhsync may take 50ms.  This difference will impact writes to Accumulo and canbe mitigated in some situations with larger buffers in Accumulo.HDFS keeps checksum data internally by default.  Datanodes store checksum datain a separate file in the local filesystem.  This means when hsync is calledon a WAL, two files must be synced on each datanode.  Syncing two files doublesthe time. To make matters even worse, when the two files are synced the localfilesystem metadata is also synced.  Depending on the local filesystem and itsconfiguration, syncing the metadata may or may not take time.  In the worstcase, we need to wait for four sync operations at the local filesystem level oneach datanode. One thing I am not sure about, is if these sync operations occurin parallel on the replicas on different datanodes.  If anyone can answer thisquestion, please let us know on the dev list. The following pointers showwhere sync occurs in the datanode code.  BlockReceiver.flushOrSync() calls ReplicaOutputStreams.syncDataOut() and ReplicaOutputStreams.syncChecksumOut() when isSync is true.  The methods in ReplicaOutputStreams call FileChannel.force(true) whichsynchronously flushes data and filesystem metadata.If files were preallocated (this would avoid syncing local filesystem metadata)and checksums were stored in-line, then 1 sync could be done instead of 4.Configuring WAL flush/sync in Accumulo 1.6Accumulo 1.6.0 only supported hsync and this caused performanceproblems.  In order to offer better performance, the option toconfigure hflush was added in 1.6.1.  Thetserver.wal.sync.method configuration option was added to supportthis feature.  This was a tablet server wide option that applied to everythingwritten to any table.Group CommitEach Accumulo tablet server has a single WAL.  When multiple clients sendmutations to a tablet server at around the same time, the tablet sever may groupall of this into a single WAL operation.  It will do this instead of writing andsyncing or flushing each client’s mutations to the WAL separately.  Doing thisincrease throughput and lowers average latency for clients.Configuring WAL flush/sync in Accumulo 1.7+Accumulo 1.7.0 introduced table.durability, a new per table propertyfor configuring durability.  It also stopped using the tserver.wal.sync.methodproperty.  The table.durability property has the following four legal values.This property defaults to the most durable option which is sync.  none : Do not write to WAL  log  : Write to WAL, but do not sync  flush : Write to WAL and call hflush  sync : Write to WAL and call hsyncIf multiple writes arrive at around the same time with different durabilitysettings, then the group commit code will choose the most durable.  This cancause one tables settings to slow down writes to another table.  Basically, onetable that is set to sync can impact the entire system.In Accumulo 1.6, it was easy to make all writes use hflush because there wasonly one tserver setting.  Getting everything to use flush in 1.7 and latercan be a little tricky because by default the Accumulo metadata table is set touse sync.  The following shell commands show this. The first command setstable.durability=flush as a system wide default for all tables.  However, themetadata table is still set to sync, because it has a per table override forthat setting.  This override is set when Accumulo is initialized.  To get thistable to use flush, the per table override must be deleted.  After deletingthose properties, the metadata tables will inherit the system wide setting.root@uno&amp;gt; config -s table.durability=flushroot@uno&amp;gt; createtable fooroot@uno foo&amp;gt; config -t foo -f table.durability-----------+---------------------+----------------------------------------------SCOPE      | NAME                | VALUE-----------+---------------------+----------------------------------------------default    | table.durability .. | syncsystem     |    @override ...... | flush-----------+---------------------+----------------------------------------------root@uno&amp;gt; config -t accumulo.metadata -f table.durability-----------+---------------------+----------------------------------------------SCOPE      | NAME                | VALUE-----------+---------------------+----------------------------------------------default    | table.durability .. | syncsystem     |    @override ...... | flushtable      |    @override ...... | sync-----------+---------------------+----------------------------------------------root@uno&amp;gt; config -t accumulo.metadata -d table.durabilityroot@uno&amp;gt; config -t accumulo.metadata -f table.durability-----------+---------------------+----------------------------------------------SCOPE      | NAME                | VALUE-----------+---------------------+----------------------------------------------default    | table.durability .. | syncsystem     |    @override ...... | flush-----------+---------------------+----------------------------------------------In short, executing the following commands will make all writes use flush(assuming no other tables or namespaces have been specifically set to sync).config -s table.durability=flushconfig -t accumulo.metadata -d table.durabilityconfig -t accumulo.root -d table.durabilityEven with these settings adjusted, minor compactions could still force hsyncto be called in 1.7.0 and 1.7.1.  This was fixed in 1.7.2 and 1.8.0.  See the1.7.2 release notes and ACCUMULO-4112 for more details.In addition to the per table durability setting, a per batch writer durabilitysetting was also added in 1.7.0.  SeeBatchWriterConfig.setDurability(…).  This means any client couldpotentially cause a hsync operation to occur, even if the system isconfigured to use hflush.Improving the situationThe more granular durability settings introduced in 1.7.0 can cause someunexpected problems.  ACCUMULO-4146 suggests one possible way to solve theseproblems with Per-durability write ahead logs.",
      "url": " /blog/2016/11/02/durability-performance.html",
      "categories": "blog"
    }
    ,
  
    "release-accumulo-1-6-6": {
      "title": "Apache Accumulo 1.6.6",
      "content": "Apache Accumulo 1.6.6 is a maintenance release on the 1.6 version branch. Thisrelease contains changes from more than 40 issues, comprised of bug-fixes,performance improvements, build quality improvements, and more. SeeJIRA for a complete list.Below are resources for this release:  User Manual  Javadocs  ExamplesUsers of any previous 1.6.x release are strongly encouraged to update as soonas possible to benefit from the improvements with very little concern in changeof underlying functionality.As of this release, active development has ceased for the 1.6 release line, sousers should consider upgrading to a newer, actively maintained version whenthey can. While the developers may release another 1.6 version to address asevere issue, there’s a strong possibility that this will be the last 1.6release. That would also mean that this will be the last Accumulo version tosupport Java 6 and Hadoop 1.HighlightsWrite-Ahead Logs can be prematurely deletedThere were cases where the Accumulo Garbage Collector may inadvertently deletea WAL for a tablet server that it has erroneously determined to be down,causing data loss. This has been corrected. See ACCUMULO-4157for additional detail.Upgrade to Commons-VFS 2.1UPDATE (20161202): This change was reverted prior to releasing 1.6.6,because it broke the build with Hadoop 1. Hadoop 1 support was dropped in 1.7.0and later, so builds were not affected in those branches. It is still possibleto Apache Commons VFS 2.1, but you may need backport the patch. SeeACCUMULO-3470 for details.Upgrading to Apache Commons VFS 2.1 fixes several issues with classloading outof HDFS. For further detail see ACCUMULO-4146. Additionalfixes to a potential HDFS class loading deadlock situation were made inACCUMULO-4341.Native Map failed to increment mutation count properlyThere was a bug (ACCUMULO-4148) where multiple put calls withidentical keys and no timestamp would exhibit different behaviour depending onwhether native maps were enabled or not. This behaviour would result in hiddenmutations with native maps, and has been corrected.Open WAL files could prevent DataNode decommissionAn improvement was introduced to allow a max age before WAL files would beautomatically rolled. Without a max age, they could stay open for writingindefinitely, blocking the Hadoop DataNode decommissioning process. For moreinformation, see ACCUMULO-4004.Remove unnecessary copy of cached RFile index blocksAccumulo maintains an cache for file blocks in-memory as a performanceoptimization. This can be done safely because Accumulo RFiles are immutable,thus their blocks are also immutable. There are two types of these blocks:index and data blocks. Index blocks refer to the b-tree style index inside ofeach Accumulo RFile, while data blocks contain the sorted Key-Value pairs. Inprevious versions, when Accumulo extracted an Index block from the in-memorycache, it would copy the data. ACCUMULO-4164 removes thisunnecessary copy as the contents are immutable and can be passed by reference.Ensuring that the Index blocks are not copied when accessed from the cache is abig performance gain at the file-access level.Analyze Key-length to avoid choosing large Keys for RFile Index blocksAccumulo’s RFile index blocks are made up of a Key which exists in the file andpoints to that specific location in the corresponding RFile data block. Thus,the size of the RFile index blocks is largely dominated by the size of the Keyswhich are used by the index. ACCUMULO-4314 is an improvementwhich uses statistics on the length of the Keys in the Rfile to avoid choosingKeys for the index whose length is greater than three standard deviations forthe RFile. By choosing smaller Keys for the index, Accumulo can access theRFile index faster and keep more Index blocks cached in memory. Initial testsshowed that with this change, the RFile index size was nearly cut in half.Gson version bumpDue to an upstream bug with Gson 2.2.2, we’ve bumped our bundleddependency (ACCUMULO-4345) to version 2.2.4. Please take noteof this when you upgrade, if you were using the version shipped with Accumulo,and were relying on the buggy behavior in the previous version in your owncode.Minor performance improvements.A performance issue was identified and corrected(ACCUMULO-1755) where the BatchWriter would block calls toaddMutation while looking up destination tablet server metadata. The writer hasbeen fixed to allow both operations in parallel.Other Notable Changes  ACCUMULO-4155 No longer publish javadoc for non-public APIto website. (Still available in javadoc jars in maven)  ACCUMULO-4334 Ingest rates reported through JMX did notmatch rates reported by Monitor.  ACCUMULO-4335 Error conditions that result in a Halt shouldensure non-zero process exit code.TestingEach unit and functional test only runs on a single node, while the RandomWalkand Continuous Ingest tests run on any number of nodes. Agitation refers torandomly restarting Accumulo processes and Hadoop Datanode processes, and, inHDFS High-Availability instances, forcing NameNode failover.            OS/Environment      Hadoop      Nodes      ZooKeeper      HDFS HA      Tests                  CentOS 7      1.2.1      1      3.3.6      No      Unit tests and Integration Tests              CentOS 7      2.2.0      1      3.3.6      No      Unit tests and Integration Tests      ",
      "url": " /release/accumulo-1.6.6/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-8-0": {
      "title": "Apache Accumulo 1.8.0",
      "content": "Apache Accumulo 1.8.0 is a significant release that includes many importantmilestone features which expand the functionality of Accumulo. These includefeatures related to security, availability, and extensibility. Over350 JIRA issues were resolved in this version. This includes over200 bug fixes and 71 improvements and 4 new features. See JIRAfor the complete list.Below are resources for this release:  User Manual  Javadocs  ExamplesIn the context of Accumulo’s Semantic Versioning guidelines,this is a “minor version”. This means that new APIs have been created, somedeprecations may have been added, but no deprecated APIs have been removed.Code written against 1.7.x should work against 1.8.0 – binary compatibilityhas been preserved with one exception of an already-deprecated Mock Accumuloutility class. As always, the Accumulo developers take API compatibilityvery seriously and have invested much time to ensure that we meet the promises set forth to our users.Major ChangesSpeed up WAL roll oversPerformance of writing mutations is improved by refactoring thebookkeeping required for Write-Ahead Log (WAL) files and by creating astandby WAL for faster switching when the log is full. This was asubstantial refactor in the way WALs worked, but smoothes overallingest performance in addition to provides a increase in write speedas shown by the simple test below. The top entry is beforeACCUMULO-3423 and the bottom graph is after therefactor.User level API for RFilePreviously the only public API available to write RFiles was via the AccumuloFileOutputFormat. There was no way to read RFiles in the publicAPI. ACCUMULO-4165 exposes a brand new public API for reading and writing RFiles as well as cleans up some of the internal APIs.Suspend Tablet assignment for rolling restartsWhen a tablet server dies, Accumulo attempted to reassign the tablets as quickly as possible to maintain availability.A new configuration property table.suspend.duration (with a default of zero seconds) now controls how long to wait before reassigninga tablet from a dead tserver. The property is configurable via theAccumulo shell, so you can set it, do a rolling restart, and thenset it back to 0. A new state as introduced, TableState.SUSPENDED to support this feature. By default, metadata tabletreassignment is not suspended, but that can also be changed with the master.metadata.suspendable property that is false bydefault. Root tablet assignment can not be suspended. See ACCUMULO-4353 for more info.Run multiple Tablet Servers on one nodeACCUMULO-4328 introduces the capability of running multiple tservers on a single node. This is intended for nodes with a largeamounts of memory and/or disk. This feature is disabled by default. There are several related tickets: ACCUMULO-4072, ACCUMULO-4331and ACCUMULO-4406. Note that when this is enabled, the names of the log files change. Previous log file names were defined in thegeneric_logger.xml as ${org.apache.accumulo.core.application}_{org.apache.accumulo.core.ip.localhost.hostname}.log.The files will now include the instance id after the application with${org.apache.accumulo.core.application}_${instance}_${org.apache.accumulo.core.ip.localhost.hostname}.log.For example: tserver_host.domain.com.log will become tserver_1_host.domain.log when multiple TabletServersare run per host. The same change also applies to the debug logs provided in the example configurations. The lognames do not change if this feature is not used.Rate limiting Major CompactionsMajor Compactions can significantly increase the amount of load onTabletServers. ACCUMULO-4187 restricts the rate at which data isread and written when performing major compactions. This has a directeffect on the IO load caused by major compactions with a similareffect on the CPU utilization. This behavior is controlled by a newproperty tserver.compaction.major.throughput with a defaults of 0Bwhich disables the rate limiting.Table SamplingQueryable sample data was added by ACCUMULO-3913.  This allows users to configure a pluggablefunction to generate sample data.  At scan time, the sample data can optionally be scanned.Iterators also have access to sample data.  Iterators can access all data and sample data, thisallows an iterator to use sample data for query optimizations.  The new user level RFile APIsupports writing RFiles with sample data for bulk import.A simple configurable sampler function is included with Accumulo.  This sampler uses hashing andcan be configured to use a subset of Key fields.  For example if it was desired to have entire rowsin the sample, then this sampler would be configured to hash+mod the row.   Then when a row isselected for the sample, all of its columns and all of its updates will be in the sample data.Another scenario is one in which a document id is in the column qualifier.  In this scenario, onewould either want all data related to a document in the sample data or none.  To achieve this, thesample could be configured to hash+mod on the column qualifier.  See the sample Readmeexample and javadocs on the new APIs for more information.For sampling to work, all tablets scanned must have pre-generated sample data that was generated inthe same way.  If this is not the case then scans will fail.  For existing tables, samples can begenerated by configuring sampling on the table and compacting the table.Upgrade to Apache Thrift 0.9.3Accumulo relies on Apache Thrift to implement remote procedure callsbetween Accumulo services. Ticket ACCUMULO-4077updates our dependency to 0.9.3. See theApache Thrift 0.9.3 Release Notes for details onthe changes to Thrift.  NOTE: The Thrift 0.9.3 Java library is notcompatible other versions of Thrift. Applications running against Accumulo1.8 must use Thrift 0.9.3. Different versions of Thrift on the classpathwill not work.Iterator Test HarnessUsers often write a new iterator without fully understanding its limits and lifetime. Previously, Accumulo didnot provide any means in which a user could test iterators to catch common issues that only become apparentin multi-node production deployments. ACCUMULO-626 provides a framework and a collection of initial testswhich can be used to simulate common issues with Iterators that only appear in production deployments. This testharness can be used directly by users as a supplemental tool to unit tests and integration tests with MiniAccumuloCluster.Please see the Accumulo User Manual chapter on Iterator Testing for more informationDefault port for Monitor changed to 9995Previously, the default port for the monitor was 50095. You will need to update your links to point to port 9995. The defaultport for the GC process was also changed from 50091 to 9998, although this an RPC port used internally and automatically discovered.These default ports were changed because the previous defaults fell in the Linux Ephemeral port range. This means that the operatingsystem, when a port in this range was unused, would allocate this port for dynamic network communication. This has the side-effect oftemporal bind issues when trying to start these services (as the operatingsystem might have allocated them elsewhere). By moving thesedefaults out of the ephemeral range, we can guarantee that the Monitor and GCwill reliably start. These values are still configurable by settingmonitor.port.clientand gc.port.client in the accumulo-site.xml.Other Notable Changes  ACCUMULO-1055 Configurable maximum file size for merging minor compactions  ACCUMULO-1124 Optimization of RFile index  ACCUMULO-2883 API to fetch current tablet assignments  ACCUMULO-3871 Support for running integration tests in MapReduce  ACCUMULO-3920 Deprecate the MockAccumulo class and remove usage in our tests  ACCUMULO-4339 Make hadoop-minicluster optional dependency of accumulo-minicluster  ACCUMULO-4318 BatchWriter, ConditionalWriter, and ScannerBase now extend AutoCloseable  ACCUMULO-4326 Value constructor now accepts Strings (and Charsequences)  ACCUMULO-4354 Bump dependency versions to include gson, jetty, and sl4j  ACCUMULO-3735 Bulk Import status page on the monitor  ACCUMULO-4066 Reduced time to processes conditional mutations.  ACCUMULO-4164 Reduced seek time for cached data.TestingEach unit and functional test only runs on a single node, while the RandomWalkand Continuous Ingest tests run on any number of nodes. Agitation refers torandomly restarting Accumulo processes and Hadoop Datanode processes, and, inHDFS High-Availability instances, forcing NameNode failover.            OS/Environment      Hadoop      Nodes      ZooKeeper      HDFS HA      Tests                  CentOS7/openJDK7/EC2; 3 m3.xlarge leaders, 8 d2.xlarge workers      2.6.4      11      3.4.8      No      24 HR Continuous Ingest without Agitation.              CentOS7/openJDK7/EC2; 3 m3.xlarge leaders, 8 d2.xlarge workers      2.6.4      11      3.4.8      No      16 HR Continuous Ingest with Agitation.              CentOS7/openJDK7/OpenStack VMs (16G RAM 2cores 2disk3; 1 leader, 5 workers      HDP 2.5 (Hadoop 2.7)      7      HDP 2.5 (ZK 3.4)      No      24 HR Continuous Ingest without Agitation.              CentOS7/openJDK7/OpenStack VMs (16G RAM 2cores 2disk3; 1 leader, 5 workers      HDP 2.5 (Hadoop 2.7)      7      HDP 2.5 (ZK 3.4)      No      24 HR Continuous Ingest with Agitation.      ",
      "url": " /release/accumulo-1.8.0/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-7-2": {
      "title": "Apache Accumulo 1.7.2",
      "content": "Apache Accumulo 1.7.2 is a maintenance release on the 1.7 version branch. Thisrelease contains changes from more than 150 issues, comprised of bug-fixes,performance improvements, build quality improvements, and more. SeeJIRA for a complete list.Below are resources for this release:  User Manual  Javadocs  ExamplesUsers of any previous 1.7.x release are strongly encouraged to update as soonas possible to benefit from the improvements with very little concern in changeof underlying functionality. Users of 1.6 or earlier that are seeking toupgrade to 1.7 should consider 1.7.2 as a starting point.HighlightsWrite-Ahead Logs can be prematurely deletedThere were cases where the Accumulo Garbage Collector may inadvertently delete a WAL for a tablet server that it has erroneously determined to be down, causing data loss. This has been corrected. See ACCUMULO-4157 for additional detail.Upgrade to Commons-VFS 2.1Upgrading to Apache Commons VFS 2.1 fixes several issues with classloading out of HDFS. For further detail see ACCUMULO-4146. Additional fixes to a potential HDFS class loading deadlock situation were made in ACCUMULO-4341.Native Map failed to increment mutation count properlyThere was a bug (ACCUMULO-4148) where multiple put calls with identical keys and no timestamp would exhibit different behaviour depending on whether native maps were enabled or not. This behaviour would result in hidden mutations with native maps, and has been corrected.Open WAL files could prevent DataNode decommissionAn improvement was introduced to allow a max age before WAL files would be automatically rolled. Without a max age, they could stay open for writing indefinitely, blocking the Hadoop DataNode decommissioning process. For more information, see ACCUMULO-4004.Remove unnecessary copy of cached RFile index blocksAccumulo maintains an cache for file blocks in-memory as a performance optimization. This can be done safely because Accumulo RFiles are immutable, thus their blocks are also immutable. There are two types of these blocks: index and data blocks. Index blocks refer to the b-tree style index inside of each Accumulo RFile, while data blocks contain the sorted Key-Value pairs. In previous versions, when Accumulo extracted an Index block from the in-memory cache, it would copy the data. ACCUMULO-4164 removes this unnecessary copy as the contents are immutable and can be passed by reference. Ensuring that the Index blocks are not copied when accessed from the cache is a big performance gain at the file-access level.Analyze Key-length to avoid choosing large Keys for RFile Index blocksAccumulo’s RFile index blocks are made up of a Key which exists in the file and points to that specific location in the corresponding RFile data block. Thus, the size of the RFile index blocks is largely dominated by the size of the Keys which are used by the index. ACCUMULO-4314 is an improvement which uses statistics on the length of the Keys in the Rfile to avoid choosing Keys for the index whose length is greater than three standard deviations for the RFile. By choosing smaller Keys for the index, Accumulo can access the RFile index faster and keep more Index blocks cached in memory. Initial tests showed that with this change, the RFile index size was nearly cut in half.Minor performance improvements.Tablet servers would previously always hsync at the start of a minor compaction, causing delays in the write pipeline. These additional syncs were determined to provide no additional durability guarantees and have been removed. See ACCUMULO-4112 for additional detail.A performance issue was identified and corrected (ACCUMULO-1755) where the BatchWriter would block calls to addMutation while looking up destination tablet server metadata. The writer has been fixed to allow both operations in parallel.Other Notable Changes  ACCUMULO-3923 bootstrap_hdfs.sh script would copy incorrect jars to hdfs.  ACCUMULO-4146 Avoid copy of RFile Index Blocks when already in cache.  ACCUMULO-4155 No longer publish javadoc for non-public API to website. (Still available in javadoc jars in maven)  ACCUMULO-4173 Provide balancer to balance table within subset of hosts.  ACCUMULO-4334 Ingest rates reported through JMX did not match rates reported by Monitor.  ACCUMULO-4335 Error conditions that result in a Halt should ensure non-zero process exit code.TestingEach unit and functional test only runs on a single node, while the RandomWalkand Continuous Ingest tests run on any number of nodes. Agitation refers torandomly restarting Accumulo processes and Hadoop Datanode processes, and, inHDFS High-Availability instances, forcing NameNode failover.            OS/Environment      Hadoop      Nodes      ZooKeeper      HDFS HA      Tests                  CentOS 7; EC2 m3.xlarge, d2.xlarge workers      2.6.3      9      3.4.8      No      24 HR Continuous Ingest with and without Agitation.              CentOS 6: EC2 m3.2xlarge      2.6.1      1      3.4.5      No      Unit tests and Integration Tests      ",
      "url": " /release/accumulo-1.7.2/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-7-1": {
      "title": "Apache Accumulo 1.7.1",
      "content": "Apache Accumulo 1.7.1 is a maintenance release on the 1.7 version branch. Thisrelease contains changes from more than 150 issues, comprised of bug-fixes,performance improvements, build quality improvements, and more. SeeJIRA for a complete list.Below are resources for this release:  User Manual  Javadocs  ExamplesUsers of any previous 1.7.x release are strongly encouraged to update as soonas possible to benefit from the improvements with very little concern in changeof underlying functionality. Users of 1.6 or earlier that are seeking toupgrade to 1.7 should consider 1.7.1 as a starting point.HighlightsSilent data-loss via bulk imported filesA user recently reported that a simple bulk-import application wouldoccasionally lose some records. Through investigation, it was found that whenbulk imports into a table failed the initial assignment, the logic thatautomatically retries the imports was incorrectly choosing the tablets toimport the files into. ACCUMULO-3967 contains more informationon the cause and identification of the bug. The data-loss condition would onlyaffect entire files. If records from a file exist in Accumulo, it is stillguaranteed that all records within that imported file were successful.As such, users who have bulk import applications using previous versions ofAccumulo should verify that all of their data was correctly ingested intoAccumulo and immediately update to Accumulo 1.7.1 (This is the same bug thatwas fixed in 1.6.4, so you won’t be affected if you’re running 1.6.4 or newer).Queued Compactions Not RunningFound and fixed a bug (ACCUMULO-4016) in which some queuedcompactions would never run if the number of files changed while the tablet wasqueued.Kerberos Ticket RenewalsA bug was fixed which caused Accumulo clients and services to fail to check and(if necessary) renew their Kerberos credentials. This would eventually lead tothese components failing to properly authenticate until they were restarted.(ACCUMULO-4069)Updated commons-collectionThe bundled commons-collection library was updated from version 3.2.1 to 3.2.2because of a reported vulnerability in that library.(ACCUMULO-4056)Faster Processing of Conditional MutationsImproved ConditionalMutation processing time by a factor of 3.(ACCUMULO-4066)Slow GC While Bulk ImportingFound and worked around an issue where lots of bulk imports creating many newfiles would significantly impair the Accumulo GC service, and possibly preventit from running to completion entirely. (ACCUMULO-4021)Unnoticed Per-table Configuration UpdatesFixed a bug which caused tablet servers to not notice changes to the per-tableconstraints, under some circumstances. (ACCUMULO-3859)TabletServers kill themselves on CentOS7Reduced the aggressiveness with which Accumulo Tablet Servers preemptivelykilled themselves when a local filesystem switched to read-only (indicating apossible failure). To reduce false positives, such as those which can occurwith systemd’s extra cgroup mounts in CentOS7, an additional check was added toensure that tablet servers would only kill themselves if an ext- orxfs-formatted disk switched to read-only. (ACCUMULO-4080)Improvements in Locating Client Configuration FileFixed some unexpected error messages related to settingACCUMULO_CLIENT_CONF_PATH, and improved the detection of the client.conf file ifACCUMULO_CLIENT_CONF_PATH was set to a directory containing client.conf.(ACCUMULO-4026,ACCUMULO-4027)Transient ZooKeeper disconnect causes FATE threads to exitZooKeeper clients are expected to handle the situation where they becomedisconnected from the ZooKeeper server and must wait to be reconnectedbefore continuing ZooKeeper operations.The dedicated threads running inside the Accumulo Master process for FATEactions had the potential unexpectedly exit in this disconnected state.This caused a scenario where all future FATE-based operations wouldbe blocked until the Accumulo Master process was restarted. (ACCUMULO-4060)Incorrect management of certain Apache Thrift RPCsAccumulo relies on Apache Thrift to implement remote procedure calls betweenAccumulo services. Accumulo’s use of Thrift uncovered an unfortunate situationwhere a special RPC (a “oneway” call) would leave unwanted data on the underlyingThrift connection. After this extra data was left on connection, all subsequent RPCsre-using that connection would fail with “out of sequence response” error messages.Accumulo would be left in a bad state until the mishandled connections were releasedor Accumulo services were restarted. (ACCUMULO-4065)Other Notable Changes  ACCUMULO-3509 Fixed some lock contention in TabletServer, preventing resource cleanup  ACCUMULO-3734 Fixed quote-escaping bug in VisibilityConstraint  ACCUMULO-4025 Fixed cleanup of bulk load fate transactions  ACCUMULO-4098,ACCUMULO-4113 Fixed widespread misuse of ByteBufferTestingEach unit and functional test only runs on a single node, while the RandomWalkand Continuous Ingest tests run on any number of nodes. Agitation refers torandomly restarting Accumulo processes and Hadoop Datanode processes, and, inHDFS High-Availability instances, forcing NameNode failover.            OS/Environment      Hadoop      Nodes      ZooKeeper      HDFS HA      Tests                  CentOS 7.1 w/Oracle JDK8 on EC2 (1 m3.xlarge, 8 d2.xlarge)      2.6.3      9      3.4.6      No      Random walk (All.xml) 24-hour run, saw ACCUMULO-3794 and ACCUMULO-4151.              CentOS 7.1 w/Oracle JDK8 on EC2 (1 m3.xlarge, 8 d2.xlarge)      2.6.3      9      3.4.6      No      21 hr run of CI w/ agitation, 23.1B entries verified.              CentOS 7.1 w/Oracle JDK8 on EC2 (1 m3.xlarge, 8 d2.xlarge)      2.6.3      9      3.4.6      No      24 hr run of CI w/o agitation, 23.0B entries verified; saw performance issues outlined in comment on ACCUMULO-4146.              CentOS 6.7 (OpenJDK 7), Fedora 23 (OpenJDK 8), and CentOS 7.2 (OpenJDK 7)      2.6.1      1      3.4.6      No      All unit tests and ITs pass with -Dhadoop.version=2.6.1; Kerberos ITs had a problem with earlier versions of Hadoop      ",
      "url": " /release/accumulo-1.7.1/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-6-5": {
      "title": "Apache Accumulo 1.6.5",
      "content": "Apache Accumulo 1.6.5 is a maintenance release on the 1.6 version branch. Thisrelease contains changes from 55 issues, comprised of bug-fixes, performanceimprovements, build quality improvements, and more. See JIRA for acomplete list.Below are resources for this release:  User Manual  Javadocs  ExamplesUsers of any previous 1.6.x release are strongly encouraged to update as soon aspossible to benefit from the improvements with very little concern in change ofunderlying functionality. Users of 1.4 or 1.5 that are seeking to upgrade to 1.6should consider 1.6.5 as a starting point.Outstanding Known IssuesBe aware that a small documentation bug exists with the compact command in theshell (ACCUMULO-4138). The documentation for the begin row andend row should be described as exclusive and inclusive, respectively, ratherthan the incorrect description of both being inclusive.HighlightsQueued Compactions Not RunningFound and fixed a bug (ACCUMULO-4016) in which some queuedcompactions would never run if the number of files changed while the tablet wasqueued.Faster Processing of Conditional MutationsImproved ConditionalMutation processing time by a factor of 3.(ACCUMULO-4066)Slow GC While Bulk ImportingFound and worked around an issue where lots of bulk imports creating many newfiles would significantly impair the Accumulo GC service, and possibly preventit from running to completion entirely. (ACCUMULO-4021)Improvements in Locating Client Configuration FileFixed some unexpected error messages related to settingACCUMULO_CLIENT_CONF_PATH, and improved the detection of the client.conf file ifACCUMULO_CLIENT_CONF_PATH was set to a directory containing client.conf.(ACCUMULO-4026,ACCUMULO-4027)Transient ZooKeeper disconnect causes FATE threads to exitZooKeeper clients are expected to handle the situation where they becomedisconnected from the ZooKeeper server and must wait to be reconnectedbefore continuing ZooKeeper operations.The dedicated threads running inside the Accumulo Master process for FATEactions had the potential unexpectedly exit in this disconnected state.This caused a scenario where all future FATE-based operations wouldbe blocked until the Accumulo Master process was restarted. (ACCUMULO-4060)Incorrect management of certain Apache Thrift RPCsAccumulo relies on Apache Thrift to implement remote procedure calls betweenAccumulo services. Accumulo’s use of Thrift uncovered an unfortunate situationwhere a special RPC (a “oneway” call) would leave unwanted data on the underlyingThrift connection. After this extra data was left on connection, all subsequent RPCsre-using that connection would fail with “out of sequence response” error messages.Accumulo would be left in a bad state until the mishandled connections were releasedor Accumulo services were restarted. (ACCUMULO-4065)Other Notable Changes  ACCUMULO-3509 Fixed some lock contention in TabletServer, preventing resource cleanup  ACCUMULO-3734 Fixed quote-escaping bug in VisibilityConstraint  ACCUMULO-4025 Fixed cleanup of bulk load fate transactions  ACCUMULO-4070 Fixed Kerberos ticket renewal for all Accumulo services  ACCUMULO-4098,ACCUMULO-4113 Fixed widespread misuse of ByteBufferTestingEach unit and functional test only runs on a single node, while the RandomWalkand Continuous Ingest tests run on any number of nodes. Agitation refers torandomly restarting Accumulo processes and Hadoop Datanode processes, and, inHDFS High-Availability instances, forcing NameNode failover.            OS      Hadoop      Nodes      ZooKeeper      HDFS HA      Tests                  CentOS 7.1      2.6.3      9      3.4.6      No      Random walk (All.xml) 18-hour run (2 failures, both conflicting operations on same table in Concurrent test)              CentOS 7.1      2.6.3      6      3.4.6      No      Continuous ingest with agitation (2B entries)              CentOS 6.7      2.2.0 and 1.2.1      1      3.3.6      No      All unit and integration tests              CentOS 7.1 (Oracle JDK8)      2.6.3      9      3.4.6      No      Continuous ingest with agitation (24hrs, 32B entries verified) on EC2 (1 m3.xlarge leader; 8 d2.xlarge workers)      ",
      "url": " /release/accumulo-1.6.5/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-6-4": {
      "title": "Apache Accumulo 1.6.4",
      "content": "Apache Accumulo 1.6.4 is a maintenance release on the 1.6 version branch.This release contains changes from 21 issues, comprised of bug-fixes,performance improvements and better test cases. See JIRA for acomplete list.Below are resources for this release:  User Manual  Javadocs  ExamplesUsers of any previous 1.6.x release are strongly encouraged to update as soon aspossible to benefit from the improvements with very little concern in changeof underlying functionality. Users of 1.4 or 1.5 that are seeking to upgradeto 1.6 should consider 1.6.4 as a starting point.Silent data-loss via bulk imported filesA user recently reported that a simple bulk-import application would occasionallylose some records. Through investigation, it was found that when bulk imports intoa table failed the initial assignment, the logic that automatically retries theimports was incorrectly choosing the tablets to import the files into. ACCUMULO-3967contains more information on the cause and identification of the bug. The data-losscondition would only affect entire files. If records from a file exist in Accumulo,it is still guaranteed that all records within that imported file were successful.As such, users who have bulk import applications using previous versions of Accumuloshould verify that all of their data was correctly ingested into Accumulo andimmediately update to Accumulo 1.6.4.Other bug fixes  ACCUMULO-3979 Fixed an issue where the BulkImporter failedwith an error message “QUERY_METADATA already started”.  ACCUMULO-3965 The listscans shell command did not containthe scanId attribute for currently running scans.  ACCUMULO-3946 Verified that all user-facing operations containedappropriate audit messages.  ACCUMULO-3977 Isolated scans with Iterators in use incorrectlyfail around invocation of deepCopy.  ACCUMULO-3905 RowDeletingIterator functions incorrectly whencolumns are provided by the client. This restores intended functionality withoutthe need for a workaround.  ACCUMULO-3959 ACCUMULO-3934 Multiple documentationimprovements to BatchScanner.TestingEach unit and functional test only runs on a single node, while the RandomWalkand Continuous Ingest tests run on any number of nodes. Agitation refers torandomly restarting Accumulo processes and Hadoop Datanode processes, and, inHDFS High-Availability instances, forcing NameNode failover.            OS      Hadoop      Nodes      ZooKeeper      HDFS HA      Tests                  Amazon Linux 2014.09      2.6.0      20      3.4.5      No      ContinuousIngest w/ verification w/ and w/o agitation (37B entries)      ",
      "url": " /release/accumulo-1.6.4/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-5-4": {
      "title": "Apache Accumulo 1.5.4",
      "content": "Apache Accumulo 1.5.4 is one more bug-fix release for the 1.5 series. Like 1.5.3 before it, this release contains avery small changeset when considering the normal size of changes in a release.Below are resources for this release:  User Manual  Javadocs  ExamplesThis release contains no changes to the public API. As such, there are no concernsfor the compatibility of user code running against 1.5.3. All users are encourage to upgradeimmediately without concern of stability and compatibility.A full list of changes is available via CHANGES.We’d like to thank all of the committers and contributors which had a part inmaking this release, from code contributions to testing. Everyone’s efforts aregreatly appreciated.Correctness BugsSilent data-loss via bulk imported filesA user recently reported that a simple bulk-import application would occasionally lose some records. Through investigation,it was found that when bulk imports into a table failed the initial assignment, the logic that automatically retriesthe imports was incorrectly choosing the tablets to import the files into. ACCUMULO-3967 containsmore information on the cause and identification of the bug. The data-loss condition would only affect entire files.If records from a file exist in Accumulo, it is still guaranteed that all records within that imported file weresuccessful.As such, users who have bulk import applications using previous versions of Accumulo should verify that all of theirdata was correctly ingested into Accumulo and immediately update to Accumulo 1.5.4.Thanks to Edward Seidl for reporting this bug to us!Server-side auditing changesThanks to James Mello for reporting and providing the fixes to the following server-side auditing issues.Incorrect audit initializationIt was observed that the implementation used to audit user API requests on Accumulo server processeswas not being correctly initialized which caused audit messages to never be generated. This was rectifiedin ACCUMULO-3939.Missing audit implementationsIt was also observed that some server-side API implementations did not include audit messages which resultedin an incomplete historical picture on what operations a user might have invoked. The missing audits (and thosethat were added) are described in ACCUMULO-3946.TestingEach unit and functional test only runs on a single node, while the RandomWalkand Continuous Ingest tests run on any number of nodes. Agitation refers torandomly restarting Accumulo processes and Hadoop DataNode processes, and, inHDFS High-Availability instances, forcing NameNode fail-over.            OS      Hadoop      Nodes      ZooKeeper      HDFS High-Availability      Tests                  OSX      2.6.0      1      3.4.5      No      Unit and Functional Tests              Centos 6.5      2.7.1      6      3.4.5      No      Continuous Ingest and Verify (10B entries), Randomwalk (24hrs)      ",
      "url": " /release/accumulo-1.5.4/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-6-3": {
      "title": "Apache Accumulo 1.6.3",
      "content": "Apache Accumulo 1.6.3 is a maintenance release on the 1.6 version branch.This release contains changes from over 63 issues, comprised of bug-fixes,performance improvements and better test cases. See JIRA for acomplete list.Below are resources for this release:  User Manual  Javadocs  ExamplesUsers of 1.6.0, 1.6.1, and 1.6.2 are strongly encouraged to update as soon aspossible to benefit from the improvements with very little concern in changeof underlying functionality. Users of 1.4 or 1.5 that are seeking to upgradeto 1.6 should consider 1.6.3 as a starting point.Fixed BatchWriter hold time errorIn previous releases, a BatchWriter could fail with aMutationsRejectedException with server errors. If inspection of the tserverlogs showed HoldTimeoutException was the cause, the workaround was toincrease the value of general.rpc.timeout. Changing this setting is nolonger necessary as this bug was fixed by ACCUMULO-2388.Severe bug fixes  ACCUMULO-3597 Fixed a deadlock where a table flush andmetadata tablet load were waiting on each other.  This was a rare bug. If itoccurred it could impact the availability of Accumulo as most Accumulooperations depend on metadata tablets.  ACCUMULO-3709 Fixed a potential data loss bug whereAccumuloOutputFormat close did not rethrow exception.  ACCUMULO-3745 Fixed a deadlock in SourceSwitchingIteratorthat occurred when using custom iterators that called deepCopy.  This bugwould cause scans to hang indefinitely until the offending tserver was killed.  ACCUMULO-3859 Fixed a race condition that could prevent tableconstraints from ever loading for a Tablet. It is likely to only affect userswhen the constraint is first added to a table.Notable bug fixes  ACCUMULO-3589 du in Shell does not check table existence.  ACCUMULO-3692 Offline’ing a table disabled subsequent balancing.  ACCUMULO-3696 Tracing could queue too many traces  ACCUMULO-3718 Fixed a bug that prevented a Mutation frombeing created in Scala.  ACCUMULO-3747 Thrashing tablet servers would be removed from the Monitor’s Dead Server list.  ACCUMULO-3750 Fixed an issue where the Master would perpetuallyfail when there was a bad instance.secret setting.  ACCUMULO-3784 Fixed a bug in getauths Shell command where ittreated visibilities that differed only in case as the same.  ACCUMULO-3796 Added documentation about turning off zonereclaim.  ACCUMULO-3880 Fixed an issue where malformed configuration causedTabletServers to shutdown.  ACCUMULO-3890 Fixed a performance issue with CredentialProvider. Informationstored in the CredentialProvider was not cached which resulted in repeatedly reading thefile from HDFS which can degrade HDFS performance.Known IssuesDuring testing HDFS-8406 was encountered which is summarized by write-ahead log recoverythat was never completed due to an inability to recover the HDFS lease on the WAL. To work aroundthis issue, the following steps can be done:  Locate block for walog whose lease can not be recovered.  Copy block into HDFS as temp file TMP_WALOG  Delete the walog whose lease can not be recovered.  Move TMP_WALOG to the filename of the walog deleted in the previous step.Using the fetchColumns() method on a scanner in conjunction with custom iterators thatadd column families in their seek() method can lead to unexpected behavior.  SeeACCUMULO-3905 for more details.  In that issue javadoc updates were made,but the updates did not make it into 1.6.3.TestingEach unit and functional test only runs on a single node, while the RandomWalkand Continuous Ingest tests run on any number of nodes. Agitation refers torandomly restarting Accumulo processes and Hadoop Datanode processes, and, inHDFS High-Availability instances, forcing NameNode failover.            OS      Hadoop      Nodes      ZooKeeper      HDFS HA      Tests                  Amazon Linux 2014.09      2.6.0      20      3.4.5      No      24hr ContinuousIngest w/ verification w/ and w/o agitation              Amazon Linux 2014.09      2.6.0      20      3.4.5      No      24hr Randomwalk w/o agitation              Centos 6.5      2.7.1      6      3.4.5      No      Continuous Ingest and Verify (6B entries)              Centos 6.6      2.2.0      6      3.4.5      No      All integration test passed.  Some needed to be run a 2nd time.      ",
      "url": " /release/accumulo-1.6.3/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-5-3": {
      "title": "Apache Accumulo 1.5.3",
      "content": "Apache Accumulo 1.5.3 is a bug-fix release for the 1.5 series. It is likely to be the last1.5 release, with development shifting towards newer release lines. We recommend upgradingto a newer version to continue to get bug fixes and new features.Below are resources for this release:  User Manual  Javadocs  ExamplesIn the context of Accumulo’s Semantic Versioning guidelines,this is a “patch version”. This means that there should be no public API changes. Anychanges which were made were done in a backwards-compatible manner. Code thatruns against 1.5.2 should run against 1.5.3.We’d like to thank all of the committers and contributors which had a part inmaking this release, from code contributions to testing. Everyone’s efforts aregreatly appreciated.Security ChangesSSLv3 disabled (POODLE)Many Accumulo services were capable of enabling wire encryption usingSSL connectors. To be safe, ACCUMULO-3316 disables the problematic SSLv3 version by default which waspotentially susceptible to the man-in-the-middle attack. ACCUMULO-3317 also disables SSLv3 in the monitor,so it will not accept SSLv3 client connections, when running it with https.Notable Bug FixesSourceSwitchingIterator DeadlockAn instance of SourceSwitchingIterator, the Accumulo iterator which transparently manageswhether data for a tablet read from memory (the in-memory map) or disk (HDFS after a minorcompaction), was found deadlocked in a production system.This deadlock prevented the scan and the minor compaction from ever successfully completingwithout restarting the tablet server. ACCUMULO-3745 fixes the inconsistent synchronizationinside of the SourceSwitchingIterator to prevent this deadlock from happening in the future.The only mitigation of this bug was to restart the tablet server that is deadlocked.Table flush blocked indefinitelyWhile running the Accumulo RandomWalk distributed test, it was observed that all activity inAccumulo had stopped and there was an offline Accumulo metadata table tablet. The system firsttried to flush a user tablet, but the metadata table was not online (likely due to the agitationprocess which stops and starts Accumulo processes during the test). After this call, a call toload the metadata tablet was queued but could not complete until the previous flush call. Thus,a deadlock occurred.This deadlock happened because the synchronous flush call could not complete before the loadtablet call completed, but the load tablet call couldn’t run because of connection caching weperform in Accumulo’s RPC layer to reduce the quantity of sockets we need to create to send data.ACCUMULO-3597 prevents this deadlock by forcing the use of a non-cached connection for the RPCmessage requesting a metadata tablet to be loaded.While this feature does result in additional network resources to be used, the concern is minimalbecause the number of metadata tablets is typically very small with respect to the total number oftablets in the system.The only mitigation of this bug was to restart the tablet server that is hung.RPC Connections not cachedIt was observed that the underlying connection for invoking RPCs were not actually being cached,despite it being requested that they should be cached. While this did not result in a noticedperformance impact, it was deficiency. ACCUMULO-3574 ensures that connections are cached whenit is requested that they are.Deletes on Apache Thrift Proxy API ignoredA user noted that when trying to specify a delete using the Accumulo Thrift Proxy, the deletewas treated as an update. ACCUMULO-3474 fixes the Proxy server such that deletes are properlyrespected as specified by the client.Other ChangesOther changes for this version can be found in JIRA.TestingEach unit and functional test only runs on a single node, while the RandomWalkand Continuous Ingest tests run on any number of nodes. Agitation refers torandomly restarting Accumulo processes and Hadoop DataNode processes, and, inHDFS High-Availability instances, forcing NameNode fail-over.During testing, multiple Accumulo developers noticed some stability issueswith HDFS using Apache Hadoop 2.6.0 when restarting Accumulo processes andHDFS datanodes. The developers investigated these issues as a part of thenormal release testing procedures, but were unable to find a definitive causeof these failures. Users are encouraged to followACCUMULO-2388 if they wish to follow any future developments.One possible workaround is to increase the general.rpc.timeout in theAccumulo configuration from 120s to 240s.            OS      Hadoop      Nodes      ZooKeeper      HDFS High-Availability      Tests                  Gentoo      2.6.0      1      3.4.5      No      Unit and Integration Tests              Centos 6.5      2.7.1      6      3.4.5      No      Continuous Ingest and Verify      ",
      "url": " /release/accumulo-1.5.3/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-7-0": {
      "title": "Apache Accumulo 1.7.0",
      "content": "Apache Accumulo 1.7.0 is a significant release that includes many importantmilestone features which expand the functionality of Accumulo. These includefeatures related to security, availability, and extensibility. Nearly 700 JIRAissues were resolved in this version. Approximately two-thirds were bugs andone-third were improvements.Below are resources for this release:  User Manual  Javadocs  ExamplesIn the context of Accumulo’s Semantic Versioning guidelines,this is a “minor version”. This means that new APIs have been created, somedeprecations may have been added, but no deprecated APIs have been removed.Code written against 1.6.x should work against 1.7.0, likely binary-compatiblebut definitely source-compatible. As always, the Accumulo developers take API compatibilityvery seriously and have invested much time to ensure that we meet the promises set forth to our users.Major ChangesUpdated Minimum RequirementsApache Accumulo 1.7.0 comes with an updated set of minimum requirements.  Java7 is required. Java6 support is dropped.  Hadoop 2.2.0 or greater is required. Hadoop 1.x support is dropped.  ZooKeeper 3.4.x or greater is required.Client Authentication with KerberosKerberos is the de-facto means to provide strong authentication across Hadoopand other related components. Kerberos requires a centralized key distributioncenter to authentication users who have credentials provided by anadministrator. When Hadoop is configured for use with Kerberos, all users mustprovide Kerberos credentials to interact with the filesystem, launch YARNjobs, or even view certain web pages.While Accumulo has long supported operating on Kerberos-enabled HDFS, it stillrequired Accumulo users to use password-based authentication to authenticatewith Accumulo. ACCUMULO-2815 added support for allowingAccumulo clients to use the same Kerberos credentials to authenticate toAccumulo that they would use to authenticate to other Hadoop components,instead of a separate user name and password just for Accumulo.This authentication leverages Simple Authentication and Security Layer(SASL) and GSSAPI to support Kerberos authentication over theexisting Apache Thrift-based RPC infrastructure that Accumulo employs.These additions represent a significant forward step for Accumulo, bringingits client-authentication up to speed with the rest of the Hadoop ecosystem.This results in a much more cohesive authentication story for Accumulo thatresonates with the battle-tested cell-level security and authorization modelalready familiar to Accumulo users.More information on configuration, administration, and application of Kerberosclient authentication can be found in the Kerberos chapter of theAccumulo User Manual.Data-Center ReplicationIn previous releases, Accumulo only operated within the constraints of asingle installation. Because single instances of Accumulo often consist ofmany nodes and Accumulo’s design scales (near) linearly across many nodes, itis typical that one Accumulo is run per physical installation or data-center.ACCUMULO-378 introduces support in Accumulo to automaticallycopy data from one Accumulo instance to another.This data-center replication feature is primarily applicable to users wishingto implement a disaster recovery strategy. Data can be automatically copiedfrom a primary instance to one or more other Accumulo instances. In contrastto normal Accumulo operation, in which ingest and query are stronglyconsistent, data-center replication is a lazy, eventually consistentoperation. This is desirable for replication, as it prevents additionallatency for ingest operations on the primary instance. Additionally, theimplementation of this feature can sustain prolonged outages between theprimary instance and replicas without any administrative overhead.The Accumulo User Manual contains a new chapter on replicationwhich details the design and implementation of the feature, explains how userscan configure replication, and describes special cases to consider whenchoosing to integrate the feature into a user application.User-Initiated Compaction StrategiesPer-table compaction strategies were added in 1.6.0 to provide custom logic todecide which files are involved in a major compaction. In 1.7.0, the abilityto specify a compaction strategy for a user-initiated compaction was added inACCUMULO-1798. This allows surgical compactions on a subsetof tablet files. Previously, a user-initiated compaction would compact allfiles in a tablet.In the Java API, this new feature can be accessed in the following way:Connection conn = ...CompactionStrategyConfig csConfig = new CompactionStrategyConfig(strategyClassName).setOptions(strategyOpts);CompactionConfig compactionConfig = new CompactionConfig().setCompactionStrategy(csConfig);connector.tableOperations().compact(tableName, compactionConfig)In ACCUMULO-3134, the shell’s compact command was modifiedto enable selecting which files to compact based on size, name, and path.Options were also added to the shell’s compaction command to allow settingRFile options for the compaction output. Setting the output options could beuseful for testing. For example, one tablet to be compacted using snappycompression.The following is an example shell command that compacts all files less than10MB, if the tablet has at least two files that meet this criteria. If atablet had a 100MB, 50MB, 7MB, and 5MB file then the 7MB and 5MB files wouldbe compacted. If a tablet had a 100MB and 5MB file, then nothing would be donebecause there are not at least two files meeting the selection criteria.compact -t foo --min-files 2 --sf-lt-esize 10MThe following is an example shell command that compacts all bulk importedfiles in a table.compact -t foo --sf-ename I.*These provided convenience options to select files execute using a specializedcompaction strategy. Options were also added to the shell to specify anarbitrary compaction strategy. The option to specify an arbitrary compactionstrategy is mutually exclusive with the file selection and file creationoptions, since those options are unique to the specialized compaction strategyprovided. See compact --help in the shell for the available options.API ClarificationThe declared API in 1.6.x was incomplete. Some important classes likeColumnVisibility were not declared as Accumulo API. Significant work was doneunder ACCUMULO-3657 to correct the API statement and clean upthe API to be representative of all classes which users are intended tointeract with. The expanded and simplified API statement is in theREADME.In some places in the API, non-API types were used. Ideally, public APImembers would only use public API types. A tool called APILyzerwas created to find all API members that used non-API types. Many of theviolations found by this tool were deprecated to clearly communicate that anon-API type was used. One example is a public API method that returned aclass called KeyExtent. KeyExtent was never intended to be in the publicAPI because it contains code related to Accumulo internals. KeyExtent andthe API methods returning it have since been deprecated. These were replacedwith a new class for identifying tablets that does not expose internals.Deprecating a type like this from the API makes the API more stable while alsomaking it easier for contributors to change Accumulo internals withoutimpacting the API.The changes in ACCUMULO-3657 also included an Accumulo APIregular expression for use with checkstyle. Starting with 1.7.0, projectsbuilding on Accumulo can use this checkstyle rule to ensure they are onlyusing Accumulo’s public API. The regular expression can be found in theREADME.Performance ImprovementsConfigurable Threadpool Size for AssignmentsDuring start-up, the Master quickly assigns tablets to Tablet Servers. However,Tablet Servers load those assigned tablets one at a time. In 1.7, the serverswill be more aggressive, and will load tablets in parallel, so long as they donot have mutations that need to be recovered.ACCUMULO-1085 allows the size of the threadpool used in the Tablet Serversfor assignment processing to be configurable.Group-Commit Threshold as a Factor of Data SizeWhen ingesting data into Accumulo, the majority of time is spent in thewrite-ahead log. As such, this is a common place that optimizations are added.One optimization is known as “group-commit”. When multiple clients arewriting data to the same Accumulo tablet, it is not efficient for each of themto synchronize the WAL, flush their updates to disk for durability, and thenrelease the lock. The idea of group-commit is that multiple writers can queuethe write for their mutations to the WAL and then wait for a sync that willsatisfy the durability constraints of their batch of updates. This has adrastic improvement on performance, since many threads writing batchesconcurrently can “share” the same fsync.In previous versions, Accumulo controlled the frequency in which thisgroup-commit sync was performed as a factor of the number of clients writingto Accumulo. This was both confusing to correctly configure and alsoencouraged sub-par performance with few write threads.ACCUMULO-1950 introduced a new configuration propertytserver.total.mutation.queue.max which defines the amount of data that isqueued before a group-commit is performed in such a way that is agnostic ofthe number of writers. This new configuration property is much easier toreason about than the previous (now deprecated) tserver.mutation.queue.max.Users who have set tserver.mutation.queue.max in the past are encouragedto start using the new tserver.total.mutation.queue.max property.Other improvementsBalancing Groups of TabletsBy default, Accumulo evenly spreads each table’s tablets across a cluster. Insome situations, it is advantageous for query or ingest to evenly spreadsgroups of tablets within a table. For ACCUMULO-3439, a newbalancer was added to evenly spread groups of tablets to optimize performance.This blog post provides more details about when and whyusers may desire to leverage this feature..User-specified DurabilityAccumulo constantly tries to balance durability with performance. Guaranteeingdurability of every write to Accumulo is very difficult in amassively-concurrent environment that requires high throughput. One commonarea of focus is the write-ahead log, since it must eventually call fsync onthe local filesystem to guarantee that data written is durable in the face ofunexpected power failures. In some cases where durability can be sacrificed,either due to the nature of the data itself or redundant power supplies,ingest performance improvements can be attained.Prior to 1.7, a user could only configure the level of durability forindividual tables. With the implementation of ACCUMULO-1957,the durability can be specified by the user when creating a BatchWriter,giving users control over durability at the level of the individual writes.Every Mutation written using that BatchWriter will be written with theprovided durability. This can result in substantially faster ingest rates whenthe durability can be relaxed.waitForBalance APIWhen creating a new Accumulo table, the next step is typically adding splitsto that table before starting ingest. This can be extremely important since atable without any splits will only be hosted on a single tablet server andcreate a ingest bottleneck until the table begins to naturally split. Addingmany splits before ingesting will ensure that a table is distributed acrossmany servers and result in high throughput when ingest first starts.Adding splits to a table has long been a synchronous operation, but theassignment of those splits was asynchronous. A large number of splits could beprocessed, but it was not guaranteed that they would be evenly distributedresulting in the same problem as having an insufficient number of splits.ACCUMULO-2998 adds a new method to InstanceOperations whichallows users to wait for all tablets to be balanced. This method lets userswait until tablets are appropriately distributed so that ingest can be run atfull-bore immediately.Hadoop Metrics2 SupportAccumulo has long had its own metrics system implemented using Java MBeans.This enabled metrics to be reported by Accumulo services, but consumption byother systems often required use of an additional tool like jmxtrans to readthe metrics from the MBeans and send them to some other system.ACCUMULO-1817 replaces this custom metrics system Accumulowith Hadoop Metrics2. Metrics2 has a number of benefits, the most common ofwhich is invalidating the need for an additional process to send metrics tocommon metrics storage and visualization tools. With Metrics2 support,Accumulo can send its metrics to common tools like Ganglia and Graphite.For more information on enabling Hadoop Metrics2, see the MetricsChapter in the Accumulo User Manual.Distributed Tracing with HTraceHTrace has recently started gaining traction as a standalone project,especially with its adoption in HDFS. Accumulo has long had distributedtracing support via its own “Cloudtrace” library, but this wasn’t intended foruse outside of Accumulo.ACCUMULO-898 replaces Accumulo’s Cloudtrace code with HTrace.This has the benefit of adding timings (spans) from HDFS into Accumulo spansautomatically.Users who inspect traces via the Accumulo Monitor (or another system) will beginto see timings from HDFS during operations like Major and Minor compactions whenrunning with at least Apache Hadoop 2.6.0.VERSIONS file present in binary distributionIn the pre-built binary distribution or distributions built by users from theofficial source release, users will now see a VERSIONS file present in thelib/ directory alongside the Accumulo server-side jars. Because the createdtarball strips off versions from the jar file names, it can require extra workto actually find what the version of each dependent jar (typically inspectingthe jar’s manifest).ACCUMULO-2863 adds a VERSIONS file to the lib/ directorywhich contains the Maven groupId, artifactId, and verison (GAV) information foreach jar file included in the distribution.Per-Table Volume ChooserThe VolumeChooser interface is a server-side extension point that allows usertables to provide custom logic in choosing where its files are written whenmultiple HDFS instances are available. By default, a randomized volume chooserimplementation is used to evenly balance files across all HDFS instances.Previously, this VolumeChooser logic was instance-wide which meant that it wouldaffect all tables. This is potentially undesirable as it might unintentionallyimpact other users in a multi-tenant system. ACCUMULO-3177introduces a new per-table property which supports configuration of aVolumeChooser. This ensures that the implementation to choose how HDFSutilization happens when multiple are available is limited to the expectedsubset of all tables.Table and namespace custom propertiesIn order to avoid errors caused by mis-typed configuration properties, Accumulo was strict about which configuration propertiescould be set. However, this prevented users from setting arbitrary properties that could be used by custom balancers, compactionstrategies, volume choosers, and iterators. Under ACCUMULO-2841, the ability to set arbitrary table andnamespace properties was added. The properties need to be prefixed with table.custom..  The changes made inACCUMULO-3177 and ACCUMULO-3439 leverage this new feature.Notable Bug FixesSourceSwitchingIterator DeadlockAn instance of SourceSwitchingIterator, the Accumulo iterator whichtransparently manages whether data for a tablet read from memory (thein-memory map) or disk (HDFS after a minor compaction), was found deadlockedin a production system.This deadlock prevented the scan and the minor compaction from eversuccessfully completing without restarting the tablet server.ACCUMULO-3745 fixes the inconsistent synchronization insideof the SourceSwitchingIterator to prevent this deadlock from happening in thefuture.The only mitigation of this bug was to restart the tablet server that isdeadlocked.Table flush blocked indefinitelyWhile running the Accumulo RandomWalk distributed test, it was observed thatall activity in Accumulo had stopped and there was an offline Accumulometadata table tablet. The system first tried to flush a user tablet, but themetadata table was not online (likely due to the agitation process which stopsand starts Accumulo processes during the test). After this call, a call toload the metadata tablet was queued but could not complete until the previousflush call. Thus, a deadlock occurred.This deadlock happened because the synchronous flush call could not completebefore the load tablet call completed, but the load tablet call couldn’t runbecause of connection caching we perform in Accumulo’s RPC layer to reduce thequantity of sockets we need to create to send data.ACCUMULO-3597 prevents this deadlock by forcing the use of anon-cached connection for the RPC message requesting a metadata tablet to beloaded.While this feature does result in additional network resources to be used, theconcern is minimal because the number of metadata tablets is typically verysmall with respect to the total number of tablets in the system.The only mitigation of this bug was to restart the tablet server that is hung.TestingEach unit and functional test only runs on a single node, while the RandomWalkand Continuous Ingest tests run on any number of nodes. Agitation refers torandomly restarting Accumulo processes and Hadoop DataNode processes, and, inHDFS High-Availability instances, forcing NameNode fail-over.During testing, multiple Accumulo developers noticed some stability issueswith HDFS using Apache Hadoop 2.6.0 when restarting Accumulo processes andHDFS datanodes. The developers investigated these issues as a part of thenormal release testing procedures, but were unable to find a definitive causeof these failures. Users are encouraged to followACCUMULO-2388 if they wish to follow any future developments.One possible workaround is to increase the general.rpc.timeout in theAccumulo configuration from 120s to 240s.            OS      Hadoop      Nodes      ZooKeeper      HDFS HA      Tests                  Gentoo      N/A      1      N/A      No      Unit and Integration Tests              Gentoo      2.6.0      1 (2 TServers)      3.4.5      No      24hr CI w/ agitation and verification, 24hr RW w/o agitation.              Centos 6.6      2.6.0      3      3.4.6      No      24hr RW w/ agitation, 24hr CI w/o agitation, 72hr CI w/ and w/o agitation              Amazon Linux      2.6.0      20 m1large      3.4.6      No      24hr CI w/o agitation      ",
      "url": " /release/accumulo-1.7.0/",
      "categories": "release"
    }
    ,
  
    "blog-2015-04-06-replicating-data-across-accumulo-clusters-html": {
      "title": "Replicating data across Accumulo clusters",
      "content": "Originally posted at https://blogs.apache.org/accumulo/entry/data_center_replicationTraditionally, Apache Accumulo can only operate within the confines of a single physical location. The primary reason for this restriction is that Accumulo relies heavily on Apache ZooKeeper for distributed lock management and some distributed state. Due to the consistent nature of ZooKeeper and its protocol, it doesn’t handle wide-area networks (WAN) well. As such, Accumulo suffers the same problems operating over a WAN.Data-Center Replication is a new feature, to be included in the upcoming Apache Accumulo 1.7.0, which aims to address the limitation of Accumulo to one local-area network (LAN). The implementation makes a number of decisions with respect to consistency and available which aim to avoid the normal “local” operations of the primary Accumulo instance. That is to say, replication was designed in such a way that enabling the feature on an instance should not affect the performance of that system. However, this comes at a cost of consistency across all replicas. Replication from one instance to others is performed lazily. Succinctly, replication in Accumulo can be described as an eventually-consistent system and not a strongly-consistent system (an Accumulo instance is strongly-consistent).Because replication is performed lazily, this implies that the data to replicate must be persisted in some shape until the actual replication takes place. This is done using Accumulo’s write-ahead log (WAL) files for this purpose. The append-only nature of these files make them obvious candidates for reuse without the need to persist the data in another form for replication. The only necessary changes internally to Accumulo to support this is changing the conditions that the Accumulo garbage collector will delete WAL files. Using WAL files also has the benefit of making HDFS capacity the limiting factor in how “lazy” replication can be. This means that the amount of time replication can be offline or stalled is only limited by the amount of extra HDFS space available which is typically ample.TerminologyBefore getting into details on the feature, it will help to define some basic terminology. Data in Accumulo is replicated from a “primary” Accumulo instance to a “peer” Accumulo instance. Each instance here is a normal Accumulo instance – each instance is only differentiated by a few new configuration values. Users ingest data into the primary instance, and that data will eventually be replicated to a peer. Each instance requires a unique name to identify itself among all Accumulo instances replicating with each other. Replication from a primary to a peer is defined on a per-table basis – that is, the configuration states that tableA on the primary will be replicated to tableB on the peer. A primary can have multiple peers defined, e.g. tableA on the primary can will be replicated to tableB on peer1 and tableC on peer2. OverviewInternally, replication is comprised of a few components to make up the user-facing feature: the management of data ingested on the primary which needs to be replicated, the assignment of replication work within the primary, the execution of that work within the primary to send the data to a peer, and the application of the data to the appropriate table within the peer.State Management on PrimaryThe most important state to manage for replication is the tracking the data that was ingested in the primary. This is what ensures that all of the data will be eventually replicated to the necessary peer(s). This state is kept in both the Accumulo metadata table and a new table in the accumulo namespace: replication. Through the use of an Accumulo Combiner on these tables, updates to the replication state are simple updates to the replication table. This makes management of the state machine across all of the nodes within the Accumulo instance extremely simple. For example, TabletServers reporting that data was ingested into a write-ahead log, the Master preparing data to be replicated and the TabletServer reporting that data has been replicated to the peer are all updates to the replication table.To “seed” the state machine, TabletServers first write to the metadata table at the end of a minor compaction. The Master will read records from the metadata table and add them to the replication table. Each Key-Value pair in the replication table represents a WAL’s current state within the replication “state machine” with different column families representing different states. For example, one column family represents the status of a WAL file being replicated to a specific peer while a different column family represents the status of a WAL file being replicated to all necessary peers.The Master is the primary driver of this state machine, reading the replication table and making the necessary updates repeatedly. This allows the Master to maintain a constant amount of memory with respect to the amount of data that needs to be replicated. The only limitation on persisted state for replication is the size of the replication table itself and the amount of space the WAL files on HDFS consume.RPC from primary to peerLike the other remote procedure calls in Accumulo, Apache Thrift is used to make RPCs from the primary Accumulo instance to a peer instance. The purpose of these methods is to send the relevant data from a WAL file to the peer. The Master advertises units of replication work, a WAL file that needs to be replicated to a single peer, and all TabletServers in the primary instance will try to reserve, and then perform, that work. ZooKeeper provides this feature to us with very little code in Accumulo.Once a TabletServer obtains the work, it will read through the WAL file extracting updates only for the table in this unit of work and send the updates across the wire to a TabletServer in the peer. The TabletServer on the primary asks the active Master in the peer for a TabletServer to communicate with. As such, ignoring some very quick interactions with the Master, RPC for replication is primarily a TabletServer to TabletServer operation which means that replication should scale in performance with respect to the number of available TabletServers on the primary and peer.The amount of data read from a WAL and sent to the peer per RPC is a configurable parameter defaulting to 50MB. Increasing the amount of data read at a time will have a large impact on the amount of memory consumed by a TabletServer when using replication, so take care when altering this property. It is also important to note that the Thrift server used for the purposes of replication is completely separate from the thrift server used by clients. Replication and the client service servers will not compete against one another for RPC resources.Replay of data on peerAfter a TabletServer on the primary invokes an RPC to a TabletServer on the peer, but before that RPC completes, the TabletServer on the peer must apply the updates it received to the local table. The TabletServer on the peer constructs a BatchWriter and simply applies the updates to the table. In the event of an error in writing the data, the RPC will return in error and it will be retried by a TabletServer on the primary. As such, in these failure conditions, it is possible that data will be applied on the peer multiple times. The use of Accumulo Combiners on tables used being replicated is nearly always a bad idea which will result in inconsistencies between the primary and replica.Because there are many TabletServers, each with their own BatchWriter, potential throughput for replication on the peer should be equivalent to the ingest throughput observed by clients normally ingesting data uniformly into Accumulo.Complex replication configurationsSo far, we’ve only touched on configurations which have a single primary and one to many peers; however, the feature allows multiple primary instances in addition to multiple peers. This primary-primary configuration allows data to be replicated in both directions instead of just one. This can be extended even further to allow replication between a trio of instances: primaryA replicates to primaryB which replicates to primaryC which replicates to primaryA. This aspect is supported by including provenance of which systems an update was seen inside of each Mutation. In “cyclic” replication setups, this prevents updates from being replicated indefinitely.Supporting these cycles allows for different collections of users to access physically separated instances and eventually see the changes made by other groups. For example, consider two instance of Accumulo, one in New York City and another San Francisco. Users on the west coast can use the San Francisco instance while users on the east coast can use the instance in New York. With the two instances configured to replicate to each other, data created by east coast users will eventually be seen by west coast users and vice versa.Conclusion and future workThe addition of the replication feature fills a large gap in the architecture of Accumulo where the system does not easily operate across WANs. While strong consistency between a primary and a peer is sacrificed, the common case of using replication for disaster recovery favors availability of the system over strong consistency and has the added benefit of not significantly impacting the ingest performance on the primary instance. Replication provides active backup support while enabling Accumulo to automatically share data between instances across large physical distances.One interesting detail about the implementation of this feature is that the code which performs replication between two Accumulo instances, the AccumuloReplicaSystem, is pluggable via the ReplicaSystem interface. It is reasonable to consider other implementations which can automatically replicate data from Accumulo to other systems for purposes of backup or additional query functionality through other data management systems. For example, Accumulo could be used to automatically replicate data to other indexing systems such as Lucene or even relational databases for advanced query functionality. Certain implementations of the ReplicaSystem could perform special filtering to limit the set of columns replicated to certain systems resulting in a subset of the complete dataset stored in one Accumulo instance without forcing clients to write the data to multiple systems. Each of these considerations are only theoretical at this point; however, the potential for advancement is definitely worth investigating.",
      "url": " /blog/2015/04/06/replicating-data-across-accumulo-clusters.html",
      "categories": "blog"
    }
    ,
  
    "blog-2015-03-20-balancing-groups-of-tablets-html": {
      "title": "Balancing Groups of Tablets",
      "content": "Originally posted at https://blogs.apache.org/accumulo/entry/balancing_groups_of_tabletsAccumulo has a pluggable tablet balancer that decides where tablets should be placed. Accumulo’s default configuration spreads each tables tablets evenly and randomly across the tablet servers. Each table can configure a custom balancer that does something different.For some applications to perform optimally, sub-ranges of a table need to be spread evenly across the cluster. Over the years I have run into multiple use cases for this situation. The latest use case was bad performance on the Fluo Stress Test. This test stores a tree in an Accumulo table and creates multiple tablets for each level in the tree. In parallel, the test reads data from one level and writes it up to the next level. Figure 1 below shows an example of tablet servers hosting tablets for different levels of the tree. Under this scenario if many threads are reading data from level 2 and writing up to level 1, only Tserver 1 and Tserver 2 will be utilized. So in this scenario 50% of the tablet servers are idle.Figure 1[ACCUMULO-3439][accumulo-3949] remedied this situation with the introduction of the GroupBalancer and RegexGroupBalancer which will be available in Accumulo 1.7.0. These balancers allow a user to arbitrarily group tablets. Each group defined by the user will be evenly spread across the tablet servers. Also, the total number of groups on each tablet server is minimized. As tablets are added or removed from the table, the balancer will migrate tablets to satisfy these goals.  Much of the complexity in the GroupBalancer code comes from trying to minimize the number of migrations needed to reach a good state.A GroupBalancer could be configured for the table in figure 1 in such a way that it grouped tablets by level. If this were done, the result may look like Figure 2 below. With this tablet to tablet server mapping, many threads reading from level 2 and writing data up to level 1 would utilize all of the tablet servers yielding better performance.Figure 2README.rgbalancer provides a good example of configuring and using the RegexGroupBalancer. If a regular expression can not accomplish the needed grouping, then a grouping function can be written in Java. Extend GroupBalancer to write a grouping function in java. RegexGroupBalancer provides a good example of how to do this.When using a GroupBalancer, how Accumulo automatically splits tablets must be kept in mind. When Accumulo decides to split a tablet, it chooses the shortest possible row prefix from the tablet data that yields a good split point. Therefore its possible that a split point that is shorter than what is expected by a GroupBalancer could be chosen. The best way to avoid this situation is to pre-split the table such that it precludes this possibility.The Fluo Stress test is a very abstract use case. A more concrete use case for the group balancer would be using it to ensure tablets storing geographic data were spread out evenly. For example consider GeoWave’s Accumulo Persistence Model. Tablets could be balanced such that bins related to different regions are spread out evenly. For example tablets related to each continent could be assigned a group ensuring data related to each continent is evenly spread across the cluster. Alternatively, each Tier could spread evenly across the cluster.",
      "url": " /blog/2015/03/20/balancing-groups-of-tablets.html",
      "categories": "blog"
    }
    ,
  
    "release-accumulo-1-6-2": {
      "title": "Apache Accumulo 1.6.2",
      "content": "Apache Accumulo 1.6.2 is a maintenance release on the 1.6 version branch.This release contains changes from over 150 issues, comprised of bug-fixes, performanceimprovements and better test cases. Apache Accumulo 1.6.2 is the first release since thecommunity has adopted Semantic Versioning which means that all changes to the public APIare guaranteed to be made without adding to or removing from the public API. This ensuresthat client code that runs against 1.6.1 is guaranteed to run against 1.6.2 and vice versa.Below are resources for this release:  User Manual  Javadocs  ExamplesUsers of 1.6.0 or 1.6.1 are strongly encouraged to update as soon as possible to benefit fromthe improvements with very little concern in change of underlying functionality. Users of 1.4 or 1.6are seeking to upgrade to 1.6 should consider 1.6.2 the starting point over 1.6.0 or 1.6.1.Notable Bug FixesOnly first ZooKeeper server is usedIn constructing a ZooKeeperInstance, the user provides a comma-separated list of addresses for ZooKeeperservers. 1.6.0 and 1.6.1 incorrectly truncated the provided list of ZooKeeper servers used to the first. Thiswould cause clients to fail when the first ZooKeeper server in the list became unavailable and not properlyload balance requests to all available servers in the quorum. ACCUMULO-3218 fixes the parsing ofthe ZooKeeper quorum list to use all servers, not just the first.Incorrectly handled ZooKeeper exceptionUse of ZooKeeper’s API requires very careful exception handling as some thrown exceptions from the ZooKeeperAPI are considered “normal” and must be retried by the client. In 1.6.1, Accumulo improved its handling ofthese “expected failures” to better insulate calls to ZooKeeper; however, the wrapper which sets data to a ZNodeincorrectly handled all cases. ACCUMULO-3448 fixed the implementation of ZooUtil.putData(...) to handlethe expected error conditions correctly.scanId is not set in ActiveScanThe ActiveScan class is the returned object by InstanceOperations.listScans. This class represents a“scan” running on Accumulo servers, either from a Scanner or BatchScanner. The ActiveScan classis meant to represent all of the information that represents the scan and can be useful to administratorsor DevOps-types to observe and act on scans which are running for excessive periods of time. ACCUMULO-2641fixes ActiveScan to ensure that the internal identifier scanId is properly set.Table state change doesn’t wait when requestedAn Accumulo table has two states: ONLINE and OFFLINE. An offline table in Accumulo consumes no TabletServerresources, only HDFS resources, which makes it useful to save infrequently used data. The Accumulo methods providedto transition a state from ONLINE to OFFLINE and vice versa did not respect the wait=true parameterwhen set. ACCUMULO-3301 fixes the underlying implementation to ensure that when wait=true is provided,the method will not return until the table’s state transition has fully completed.KeyValue doesn’t implement hashCode() or equals()The KeyValue class is an implementation of Entry&amp;lt;Key,Value&amp;gt; which is returned by the classes likeScanner and BatchScanner. ACCUMULO-3217 adds these methods which ensure that the returned Entry&amp;lt;Key,Value&amp;gt;operates as expected with HashMaps and HashSets.Potential deadlock in TabletServerInternal to the TabletServer, there are methods to construct instances of configuration objects for tablesand namespaces. The locking on these methods was not correctly implemented which created the possibility tohave concurrent requests to a TabletServer to deadlock. ACCUMULO-3372 found this problem while performingbulk imports of RFiles into Accumulo. Additional synchronization was added server-side to prevent this deadlockfrom happening in the future.The DateLexicoder incorrectly serialized Dates prior 1970The DateLexicode, a part of the Lexicoders classes which implement methods to convert common type primitivesinto lexicographically sorting Strings/bytes, incorrectly converted Date objects for dates prior to 1970.ACCUMULO-3385 fixed the DateLexicoder to correctly (de)serialize data Date objects. For users withdata stored in Accumulo using the broken implementation, the following can be performed to read the old data.  Lexicoder lex = new ULongLexicoder();  for (Entry&amp;lt;Key, Value&amp;gt; e : scanner) {    Date d = new Date(lex.decode(TextUtil.getBytes(e.getKey().getRow())));    // ...  }Reduce MiniAccumuloCluster failures due to random port allocationsMiniAccumuloCluster has had issues where it fails to properly start due to the way it attempts to choosea random, unbound port on the local machine to start the ZooKeeper and Accumulo processes. Improvements havebeen made, including retry logic, to withstand a few failed port choices. The changes made by ACCUMULO-3233and the related issues should eliminate sporadic failures users of MiniAccumuloCluster might have observed.Tracer doesn’t handle trace table state transitionThe Tracer is an optional Accumulo server process that serializes Spans, elements of a distributed trace,to the trace table for later inspection and correlation with other Spans. By default, the Tracer writesto a “trace” table. In earlier versions of Accumulo, if this table was put offline, the Tracer would failto write new Spans to the table when it came back online. ACCUMULO-3351 ensures that the Tracer processwill resume writing Spans to the trace table when it transitions to online after being offline.Tablet not major compactingIt was noticed that a system performing many bulk imports, there was a tablet with hundreds of files whichwas not major compacting nor was scheduled to be major compacted. ACCUMULO-3462 identified as fixserver-side which would prevent this from happening in the future.YARN job submission fails with Hadoop-2.6.0Hadoop 2.6.0 introduced a new component, the TimelineServer, which is a centralized metrics service designedfor other Hadoop components to leverage. MapReduce jobs submitted via accumulo and tool.sh failed torun the job because it attempted to contact the TimelineServer and Accumulo was missing a dependency onthe classpath to communicate with the TimelineServer. ACCUMULO-3230 updates the classpath in the exampleconfiguration files to include the necessary dependencies for the TimelineServer to ensure that YARN jobsubmission operates as previously.Performance ImprovementsUser scans can block root and metadata table scansThe TabletServer provides a feature to limit the number of open files as a resource management configuration.To perform a scan against a normal table, the metadata and root table, when not cached, need to be consultedfirst. With a sufficient number of concurrent scans against normal tables, adding to the open file count,scans against the metadata and root tables could be blocked from running because no files can be opened.This prevents other system operations from happening as expected. ACCUMULO-3297 fixes the internal semaphoreused to implement this resource management to ensure that root and metadata table scans can proceed.Other improvementsLimit available ciphers for SSL/TLSSince Apache Accumulo 1.5.2 and 1.6.1, the POODLE man-in-the-middle attack was found which exploits a client’sability to fallback to the SSLv3.0 protocol. The main mitigation strategy was to prevent the use of old ciphers/protocolswhen using SSL connectors. In Accumulo, both the Apache Thrift RPC servers and Jetty server for the Accumulomonitor have the ability to enable SSL. ACCUMULO-3316 is the parent issue which provides new configurationproperties in accumulo-site.xml which can limit the accepted ciphers/protocols. By default, insecure or out-datedprotocols have been removed from the default set in order to protect users by default.DocumentationDocumentation was added to the Administration chapter for moving from a Non-HA Namenode setup to an HA Namenode setup.New chapters were added for the configuration of SSL and for summaries of Implementation Details (initially describingFATE operations). A section was added to the Configuration chapter for describing how to arrive at optimal settingsfor configuring an instance with native maps.TestingEach unit and functional test only runs on a single node, while the RandomWalk and Continuous Ingest tests runon any number of nodes. Agitation refers to randomly restarting Accumulo processes and Hadoop Datanode processes,and, in HDFS High-Availability instances, forcing NameNode failover.            OS      Hadoop      Nodes      ZooKeeper      HDFS HA      Tests                  Gentoo      N/A      1      N/A      No      Unit and Integration Tests              Mac OSX      N/A      1      N/A      No      Unit and Integration Tests              Fedora 21      N/A      1      N/A      No      Unit and Integration Tests              CentOS 6      2.6      20      3.4.5      No      ContinuousIngest w/ verification w/ and w/o agitation (31B and 21B entries, respectively)      ",
      "url": " /release/accumulo-1.6.2/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-6-1": {
      "title": "Apache Accumulo 1.6.1",
      "content": "Apache Accumulo 1.6.1 is a maintenance release on the 1.6 version branch.This release contains changes from over 175 issues, comprised of bug-fixes, performanceimprovements and better test cases.Below are resources for this release:  User Manual  Javadocs  ExamplesAs this is a maintenance release, Apache Accumulo 1.6.1 has no client API incompatibilitiesover Apache Accumulo 1.6.0. Users of 1.6.0 are strongly encouraged to update as soon aspossible to benefit from the improvements.New users are encouraged to use this release over 1.6.0 or any other older releases. Forinformation about improvements since Accumulo 1.5, see the 1.6.0 release notes.Performance ImprovementsApache Accumulo 1.6.1 includes a number of performance-related fixes over previous versions.Many of these improvements were also included in the recently released Apache Accumulo 1.5.2.Write-Ahead Log sync performanceThe Write-Ahead Log (WAL) files are used to ensure durability of updates made to Accumulo.A sync is called on the file in HDFS to make sure that the changes to the WAL are persistedto disk, which allows Accumulo to recover in the case of failure. ACCUMULO-2766 fixedan issue where an operation against a WAL would unnecessarily wait for multiple syncs, slowingdown the ingest on the system.Minor-Compactions not aggressive enoughOn a system with ample memory provided to Accumulo, long hold-times were observed whichblocks the ingest of new updates. Trying to free more server-side memory by running minorcompactions more frequently increased the overall throughput on the node. These changeswere made in ACCUMULO-2905.HeapIterator optimizationIterators, a notable feature of Accumulo, are provided to users as a server-side programmingconstruct, but are also used internally for numerous server operations. One of these system iteratoris the HeapIterator which implements a PriorityQueue of other Iterators. One way this iterator isused is to merge multiple files in HDFS to present a single, sorted stream of Key-Value pairs. ACCUMULO-2827introduces a performance optimization to the HeapIterator which can improve the speed of theHeapIterator in common cases.Write-Ahead log sync implementationIn Hadoop-2, two implementations of sync are provided: hflush and hsync. Both of thesemethods provide a way to request that the datanodes write the data to the underlyingmedium and not just hold it in memory (the fsync syscall). While both of these methodsinform the Datanodes to sync the relevant block(s), hflush does not wait for acknowledgementfrom the Datanodes that the sync finished, where hsync does. To provide the most reliable system“out of the box”, Accumulo defaults to hsync so that your data is as secure as possible ina variety of situations (notably, unexpected power outages).The downside is that performance tends to suffer because waiting for a sync to disk is a veryexpensive operation. ACCUMULO-2842 introduces a new system property, tserver.wal.sync.method,that lets users to change the HDFS sync implementation from hsync to hflush. Using hflush insteadof hsync may result in about a 30% increase in ingest performance.For users upgrading from Hadoop-1 or Hadoop-0.20 releases, hflush is the equivalent of howsync was implemented in these older versions of Hadoop and should give comparable performance.Other improvementsUse of Hadoop CredentialProvidersApache Hadoop 2.6.0 introduced a new API aimed at providing ways to separate sensitive valuesfrom being stored in plaintext as a part of HADOOP-10607. Accumulo has had two sensitiveconfiguration properties stored in accumulo-site.xml for every standard installation: instance.secretand trace.token.property.password. If either of these properties are compromised, it could lead tounwanted access of Accumulo. ACCUMULO-2464 modifies Accumulo so that it can stored any sensitiveconfiguration properties in a Hadoop CredentialProvider. With sensitive values removed from accumulo-site.xml,it can be shared without concern and security can be focused solely on the CredentialProvider.Notable Bug FixesAdd configurable maximum frame size to Apache Thrift proxyThe Thrift proxy server was subject to memory exhaustion, typicallydue to bad input, where the server would attempt to allocate a very largebuffer and die in the process. ACCUMULO-2658 introduces a configurationparameter, like ACCUMULO-2360, to prevent this error.Offline tables can prevent tablet balancingBefore 1.6.1, when a table with many tablets was created, ingested into, andtaken offline, tablet balancing may have stopped. This would happen if therewere tablet migrations for the table, because the migrations couldn’t occur.The balancer will not run when there are outstanding migrations; therefore, asystem could become unbalanced. ACCUMULO-2694 introduces a fix to ensurethat offline tables do not block balancing and improves the server-sidelogging.MiniAccumuloCluster process managementMiniAccumuloCluster had a few issues which could cause deadlock or a method thatnever returns. Most of these are related to management of the Accumulo processes(ACCUMULO-2764, ACCUMULO-2985, and ACCUMULO-3055).IteratorSettings not correctly serialized in RangeInputSplitThe Writable interface methods on the RangeInputSplit class accidentally omittedcalls to serialize the IteratorSettings configured for the Job. ACCUMULO-2962fixes the serialization and adds some additional tests.Constraint violation causes hung scansA failed bulk import transaction had the ability to create an infinitely retryingloop due to a constraint violation. This directly prevents scans from completing,but will also hang compactions. ACCUMULO-3096 fixes the issue so that theconstraint no longer hangs the entire system.Unable to upgrade cleanly from 1.5When upgrading a table from 1.5.1 to 1.6.0, a user experienced an error where the tablenever came online. ACCUMULO-2974 fixes an issue from the change of file referencesstored as absolute paths instead of relative paths in the Accumulo metadata table.Guava dependency changedACCUMULO-3100 lowered the dependency on Guava from 15.0 to 14.0.1. This dependencynow matches what Hadoop is depending on for the 2.x.y version line. Depending on a newerversion of Guava introduces many issues stemming from deprecated classes in use by Hadoopwhich have been removed. While installations of Accumulo will likely work as expected withnewer versions of Guava on the classpath (because the Hadoop processes will have their ownclasspath), use of MiniDfsClusters with the new Guava version will result in errors.Users can attempt to use a newer version of Guava on the Accumulo server classpath; however,the success is dependent on Hadoop client libraries not using (missing) Guava methods internally.Scanners eat InterruptedExceptionScanners previously consumed InterruptedExceptions and did not exit after. In multi-threadedenvironments, this is very problematic as there is no means to stop the Scanner from reading data.ACCUMULO-3030 fixes the Scanner so that interrupts are observed and the Scanner exits as expected.DocumentationThe following documentation updates were made:  ACCUMULO-2767  ACCUMULO-2796  ACCUMULO-2919  ACCUMULO-3008  ACCUMULO-2874  ACCUMULO-2821  ACCUMULO-3097  ACCUMULO-3097TestingEach unit and functional test only runs on a single node, while the RandomWalk and Continuous Ingest tests runon any number of nodes. Agitation refers to randomly restarting Accumulo processes and Hadoop Datanode processes,and, in HDFS High-Availability instances, forcing NameNode failover.            OS      Hadoop      Nodes      ZooKeeper      HDFS HA      Tests                  Gentoo      Apache 2.6.0-SNAPSHOT      2      Apache 3.4.5      No      Unit and Functional Tests, ContinuousIngest w/ verification (2B entries)              CentOS 6      Apache 2.3.0      20      Apache 3.4.5      No      24/hr RandomWalk, ContinuousIngest w/ verification w/ and w/o agitation (17B entries), 24hr Randomwalk test      ",
      "url": " /release/accumulo-1.6.1/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-5-2": {
      "title": "Apache Accumulo 1.5.2",
      "content": "Apache Accumulo 1.5.2 is a maintenance release on the 1.5 version branch.This release contains changes from over 100 issues, comprised of bug fixes(client side and server side), new test cases, and updated Hadoop supportcontributed by over 30 different contributors and committers.Below are resources for this release:  User Manual  Javadocs  ExamplesAs this is a maintenance release, Apache Accumulo 1.5.2 has no client APIincompatibilities over Apache Accumulo 1.5.0 and 1.5.1 and requires no manual upgradeprocess. Users of 1.5.0 or 1.5.1 are strongly encouraged to update as soon as possibleto benefit from the improvements.Users who are new to Accumulo are encouraged to use a 1.6 release as opposedto the 1.5 line as development has already shifted towards the 1.6 line. For thosewho cannot or do not want to upgrade to 1.6, 1.5.2 is still an excellent choiceover earlier versions in the 1.5 line.Performance ImprovementsApache Accumulo 1.5.2 includes a number of performance-related fixes over previous versions.Write-Ahead Log sync performanceThe Write-Ahead Log (WAL) files are used to ensure durability of updates made to Accumulo.A sync is called on the file in HDFS to make sure that the changes to the WAL are persistedto disk, which allows Accumulo to recover in the case of failure. ACCUMULO-2766 fixedan issue where an operation against a WAL would unnecessarily wait for multiple syncs, slowingdown the ingest on the system.Minor-Compactions not aggressive enoughOn a system with ample memory provided to Accumulo, long hold-times were observed whichblocks the ingest of new updates. Trying to free more server-side memory by running minorcompactions more frequently increased the overall throughput on the node. These changeswere made in ACCUMULO-2905.HeapIterator optimizationIterators, a notable feature of Accumulo, are provided to users as a server-side programmingconstruct, but are also used internally for numerous server operations. One of these system iteratoris the HeapIterator which implements a PriorityQueue of other Iterators. One way this iterator isused is to merge multiple files in HDFS to present a single, sorted stream of Key-Value pairs. ACCUMULO-2827introduces a performance optimization to the HeapIterator which can improve the speed of theHeapIterator in common cases.Write-Ahead log sync implementationIn Hadoop-2, two implementations of sync are provided: hflush and hsync. Both of thesemethods provide a way to request that the datanodes write the data to the underlyingmedium and not just hold it in memory (the fsync syscall). While both of these methodsinform the Datanodes to sync the relevant block(s), hflush does not wait for acknowledgementfrom the Datanodes that the sync finished, where hsync does. To provide the most reliable system“out of the box”, Accumulo defaults to hsync so that your data is as secure as possible ina variety of situations (notably, unexpected power outages).The downside is that performance tends to suffer because waiting for a sync to disk is a veryexpensive operation. ACCUMULO-2842 introduces a new system property, tserver.wal.sync.method,that lets users to change the HDFS sync implementation from hsync to hflush. Using hflush insteadof hsync may result in about a 30% increase in ingest performance.For users upgrading from Hadoop-1 or Hadoop-0.20 releases, hflush is the equivalent of howsync was implemented in these older versions of Hadoop and should give comparable performance.Server-side mutation queue sizeWhen users desire writes to be as durable as possible, using hsync, the ingest performanceof the system can be improved by increasing the tserver.mutation.queue.max property. The costof this change is that it will cause TabletServers to use additional memory per writer. In 1.5.1,the value of this parameter defaulted to a conservative 256K, which resulted in sub-par ingestperformance.1.5.2 and ACCUMULO-3018 increases this buffer to 1M which has a noticeable positive impact oningest performance with a minimal increase in TabletServer memory usage.Notable Bug FixesFixes MapReduce package name change1.5.1 inadvertently included a change to RangeInputSplit which created an incompatibilitywith 1.5.0. The original class has been restored to ensure that users accessingthe RangeInputSplit class do not have to alter their client code. See ACCUMULO-2586 formore informationAdd configurable maximum frame size to Apache Thrift proxyThe Thrift proxy server was subject to memory exhaustion, typicallydue to bad input, where the server would attempt to allocate a very largebuffer and die in the process. ACCUMULO-2658 introduces a configurationparameter, like ACCUMULO-2360, to prevent this error.Offline tables can prevent tablet balancingBefore 1.5.2, when a table with many tablets was created, ingested into, andtaken offline, tablet balancing may have stopped. This would happen if therewere tablet migrations for the table, because the migrations couldn’t occur.The balancer will not run when there are outstanding migrations; therefore, asystem could become unbalanced. ACCUMULO-2694 introduces a fix to ensurethat offline tables do not block balancing and improves the server-sidelogging.MiniAccumuloCluster process managementMiniAccumuloCluster had a few issues which could cause deadlock or a method thatnever returns. Most of these are related to management of the Accumulo processes(ACCUMULO-2764, ACCUMULO-2985, and ACCUMULO-3055).IteratorSettings not correctly serialized in RangeInputSplitThe Writable interface methods on the RangeInputSplit class accidentally omittedcalls to serialize the IteratorSettings configured for the Job. ACCUMULO-2962fixes the serialization and adds some additional tests.Constraint violation causes hung scansA failed bulk import transaction had the ability to create an infinitely retryingloop due to a constraint violation. This directly prevents scans from completing,but will also hang compactions. ACCUMULO-3096 fixes the issue so that theconstraint no longer hangs the entire system.DocumentationThe following documentation updates were made:  ACCUMULO-2540  ACCUMULO-2767  ACCUMULO-2796  ACCUMULO-2443  ACCUMULO-3008  ACCUMULO-2919  ACCUMULO-2874  ACCUMULO-2653  ACCUMULO-2437  ACCUMULO-3097  ACCUMULO-2499  ACCUMULO-1669TestingEach unit and functional test only runs on a single node, while the RandomWalk and Continuous Ingest tests runon any number of nodes. Agitation refers to randomly restarting Accumulo processes and Hadoop Datanode processes,and, in HDFS High-Availability instances, forcing NameNode failover.            OS      Hadoop      Nodes      ZooKeeper      HDFS High-Availability      Tests                  Gentoo      Apache 2.6.0-SNAPSHOT      1      Apache 3.4.5      No      Unit and Functional Tests, ContinuousIngest w/ verification (1B entries)              CentOS 6      Apache 2.3.0      20      Apache 3.4.5      No      24/hr RandomWalk, 24/hr ContinuousIngest w/ verification w/ and w/o agitation (30B and 23B entries)      ",
      "url": " /release/accumulo-1.5.2/",
      "categories": "release"
    }
    ,
  
    "blog-2014-09-02-generating-keystores-for-configuring-accumulo-with-ssl-html": {
      "title": "Generating Keystores for configuring Accumulo with SSL",
      "content": "Originally posted at https://blogs.apache.org/accumulo/entry/generating_keystores_for_configuring_accumuloOne of the major features added in Accumulo 1.6.0 was the ability to configure Accumulo so that the Thrift communications will run over SSL. Apache Thrift is the remote procedure call library that is leverage for both intra-server communication and client communication with Accumulo. Issuing these calls over a secure socket ensures that unwanted actors cannot inspect the traffic sent across the wire. Given the sometimes sensitive nature of data stored in Accumulo and the authentication details for users, ensuring that no prying eyes have access to these communications is critical.Due to the complex and deployment specific nature of the security model for some system, Accumulo expects users to provide their own certificates, guaranteeing that they are, in fact, secure. However, for those who want to get security who do not already operate within the confines of an established security infrastructure, OpenSSL and the Java keytool command can be used to generate the necessary components to enable wire encryption.To enable SSL with Accumulo, it is necessary to generate a certificate authority and certificates which are signed by that authority. Typically, each client and server has its own certificate which provides the finest level of control over a secure cluster when the certificates are properly secured.Generate a Certificate AuthorityThe certificate authority (CA) is what controls what certificates can be used to authenticate with each other. To create a secure connection with two certificates, each certificate must be signed by a certificate authority in the “truststore” (A Java KeyStore which contains at least one Certificate Authority’s public key). When creating your own certificate authority, a single CA is typically sufficient (and would result in a single public key in the truststore). Alternatively, a third party can also act as a certificate authority (to add an additional layer of security); however, these are typically not a free service.The below is an example of creating a certificate authority and adding its public key to a Java KeyStore to provide to Accumulo.# Create a private keyopenssl genrsa -des3 -out root.key 4096# Create a certificate request using the private keyopenssl req -x509 -new -key root.key -days 365 -out root.pem# Generate a Base64-encoded version of the PEM just createdopenssl x509 -outform der -in root.pem -out root.der# Import the key into a Java KeyStorekeytool -import -alias root-key -keystore truststore.jks -file root.der# Remove the DER formatted key file (as we don&#39;t need it anymore)rm root.derRemember to protect root.key and never distribute it as the private key is the basis for your circle of trust. The keytool command will prompt you about whether or not the certificate should be trusted: enter “yes”. The truststore.jks file, a “truststore”, is meant to be shared with all parties communicating with one another. The password provided to the truststore verifies that the contents of the truststore have not been tampered with.Generate a certificate/keystore per hostFor each host in the system, it’s desirable to generate a certificate. Typically, this corresponds to a certificate per host. Additionally, each client connecting to the Accumulo instance running with SSL should be issued their own certificate. By issuing individual certificates to each entity, it gives proper control to revoke/reissue certificates to clients as necessary, without widespread interruption.# Create the private key for our serveropenssl genrsa -out server.key 4096# Generate a certificate signing request (CSR) with our private keyopenssl req -new -key server.key -out server.csr# Use the CSR and the CA to create a certificate for the server (a reply to the CSR)openssl x509 -req -in server.csr -CA root.pem -CAkey root.key -CAcreateserial -out server.crt -days 365# Use the certificate and the private key for our server to create PKCS12 fileopenssl pkcs12 -export -in server.crt -inkey server.key -certfile server.crt -name &#39;server-key&#39; -out server.p12# Create a Java KeyStore for the server using the PKCS12 file (private key)keytool -importkeystore -srckeystore server.p12 -srcstoretype pkcs12 -destkeystore server.jks -deststoretype JKS# Remove the PKCS12 file as we don&#39;t need itrm server.p12# Import the CA-signed certificate to the keystorekeytool -import -trustcacerts -alias server-crt -file server.crt -keystore server.jksThese commands create a private key for the server, generated a certificate signing request created from that private key, used the certificate authority to generate the certificate using the signing request and then created a Java KeyStore with the certificate and the private key for our server. This, paired with the truststore, provide what is needed to configure Accumulo servers to run over SSL. Both the private key (server.key), the certificate signed by the CA (server.pem), and the keystore (server.jks) should be restricted to only be accessed by the user running Accumulo on the host it was generated for. Use chown and chmod to protect the files and do not distribute them over insecure networks.Configure Accumulo ServersNow that the Java KeyStores have been created with the necessary information, the Accumulo configuration must be updated so that Accumulo creates the Thrift server over SSL instead of a normal socket. In accumulo-site.xml, configure the following:&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;rpc.javax.net.ssl.keyStore&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;/path/to/server.jks&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;rpc.javax.net.ssl.keyStorePassword&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;server_password&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;rpc.javax.net.ssl.trustStore&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;/path/to/truststore.jks&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;rpc.javax.net.ssl.trustStorePassword&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;truststore_password&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;instance.rpc.ssl.enabled&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;The keystore and truststore paths are both absolute paths on the local filesystem (not HDFS). Remember that the server keystore should only be readable by the user running Accumulo and, if you place plaintext passwords in accumulo-site.xml, make sure that accumulo-site.xml is also not globally readable. To keep these passwords out of accumulo-site.xml, consider configuring your system with the new Hadoop CredentialProvider class, see ACCUMULO-2464 for more information which will be available in Accumulo-1.6.1.Also, be aware that if unique passwords are used for each server when generating the certificate, this will result in different accumulo-site.xml files for each host. Unique configuration files per host will add much complexity to the configuration management of your instance. The use of a CredentialProvider, a feature from Hadoop which allows for acquisitions of passwords from alternate systems) can be used to help alleviate the unique accumulo-site.xml files on each host. A Java KeyStore can be created using the CredentialProvider tools which removes the necessity of passwords to be stored in accumulo-site.xml and can instead point to the CredentialProvider URI which is consistent across hosts.Configure Accumulo ClientsTo configure Accumulo clients, use $HOME/.accumulo/config. This is a simple Java properties file: each line is a configuration, key and value can be separated by a space, and lines beginning with a # symbol are ignored. For example, if we generated a certificate and placed it in a keystore (as described above), we would generate the following file for the Accumulo client.instance.rpc.ssl.enabled truerpc.javax.net.ssl.keyStore  /path/to/client-keystore.jksrpc.javax.net.ssl.keyStorePassword  client-passwordrpc.javax.net.ssl.trustStore  /path/to/truststore.jksrpc.javax.net.ssl.trustStorePassword  truststore-passwordWhen creating a ZooKeeperInstance, the implementation will automatically look for this file and set up a connection with the methods defined in this configuration file. The ClientConfiguration class also contains methods that can be used instead of a configuration file on the filesystem. Again, the paths to the keystore and truststore are on the local filesystem, not HDFS.",
      "url": " /blog/2014/09/02/generating-keystores-for-configuring-accumulo-with-ssl.html",
      "categories": "blog"
    }
    ,
  
    "blog-2014-07-09-functional-reads-over-accumulo-html": {
      "title": "Functional reads over Accumulo",
      "content": "Originally posted at https://blogs.apache.org/accumulo/entry/thinking_about_reads_over_accumuloTable structure is a common area of discussion between all types of Accumulo users. In the relational database realm, there was often a straightforward way that most users could agree upon that would be ideal to store and query some dataset. Data was identified by its schema, some fixed set of columns where each value within that column had some given characteristic. One of the big pushes behind the “NoSQL” movement was a growing pain in representing evolving data within a static schema. Applications like Accumulo removed that notion for a more flexible layout where the columns vary per row, but this flexibility often sparks debates about how data is “best” stored that often ends without a clear-cut winner.In general, I’ve found that, with new users to Accumulo, it’s difficult to move beyond the basic concept of GETs and PUTs of some value for a key. Rightfully so, it’s analogous to a spreadsheet: get or update the cell in the given row and column. However, there’s a big difference in that the spreadsheet is running on your local desktop, instead of running across many machines. In the same way, while a local spreadsheet application has some similar functionality to Accumulo, it doesn’t really make sense to think about using Accumulo as you would a spreadsheet application. Personally, I’ve developed a functional-programming-inspired model which I tend to follow when implementing applications against Accumulo. The model encourages simple, efficient and easily testable code, mainly as a product of modeling the client interactions against Accumulo’s APIs.Read APIsAccumulo has two main classes for reading data from an Accumulo table: the Scanner and BatchScanner. Both accept Range(s) which limit the data read from the table based on a start and stop Key. Only data from the table that falls within those start and stop keys will be returned to the client. The reason that we have two “types” of classes to read data is that a Scanner will return data from a single Range in sorted order whereas the BatchScanner accepts multiple Ranges and returns the data unordered. In terms of Java language specifics, both the Scanner and BatchScanner are also Iterables, which return a Java Iterator that can be easily passed to some other function, transformation or for-loop.Having both a sorted, synchronous stream and an unsorted stream of Key-Value pairs from many servers in parallel allows for a variety of algorithms to be implemented against Accumulo. Both constructs allow for the transparency in where the data came from and encourage light-weight processing of those results on the client.Accumulo IteratorsOne notable feature of Accumulo is the SortedKeyValueIterator interface, or, more succinctly, Accumulo Iterators. Typically, these iterators run inside of the TabletServer process and perform much of the heavy lifting. Iterators are used to implement a breadth of internal features such as merged file reads, visibility label filtering, versioning, and more. However, users also have the ability to leverage this server-side processing mechanism to deploy their own custom code.One interesting detail about these iterators is that they each have an implicit source which provides them data to operate on. This source is also a SortedKeyValueIterator which means that the “local” SortedKeyValueIterator can use its own API on its data source. With this implicit hierarchy, Iterators act in concert with each other in some fixed order - they are stackable. The order in which Iterators are constructed, controlled by an Iterator’s priority, determines the order of the stack. An Iterator uses its “source” Iterator to read data, performs some operation, and then passes it on (the next element could be a client or another Iterator). The design behind iterators deserves its own blog post; however, the concept to see here is that iterators are best designed as stateless as possible (transformations, filters, or aggregations that always net the same results given the same input).Functional InfluencesIn practice, these two concepts mesh very well with each other. Data read from a table can be thought of as a “stream” which came from some number of operations on the server. For a Scanner, this stream of data is backed by one tablet at a time to preserve sorted-order of the table. In the case of the BatchScanner, this is happening in parallel across many tablets from many tabletservers, with the client receiving data from many distinct hosts at one time. Likewise, the Scanner and BatchScanner APIs also encourage stateless processing of this data by presenting the data as a Java Iterator. Exposing explicit batches of Key-Value pairs would encourage blocking processing of each batch would be counter-intuitive to what the server-side processing model is. It creates a more seamless implementation paradigm on both the client and the server.When we take a step back from Object-Oriented Java and start to think about applications in a Functional mindset, it becomes clear how these APIs encourage functional-esque code. We are less concerned about mutability and encapsulation, and more concerned about stateless operations over some immutable data. Modeling our client code like this helps encourage parallelism as application in some multi-threaded environment is much simpler.Practical ApplicationI started out talking about schemas and table layouts which might seem a bit unrelated to this discussion on the functional influences in the Accumulo API. Any decisions made on a table structure must take query requirements with respect to the underlying data into account. As a practical application of what might otherwise seem like pontification, let’s consider a hypothetical system that processes clickstream data using Accumulo.Clickstream data refers to logging users who visit a website, typically for the purpose of understanding usage patterns. If a website is thought of as a directed graph, where an anchor on one page which links to another page is an edge in that graph, a user’s actions on that website can be thought of as a “walk” over that graph. In managing a website, it’s typically very useful to understand usage patterns of your site: what page is most common? which links are most commonly clicked? what changes to a page make users act differently?Now, let’s abstractly consider that we store this clickstream data in Accumulo. Let’s not go into specifics, but say we retain the typical row-with-columns idea: each row represents some user visiting a page on your website using a globally unique identifier. Each column would contain some information about that visit: the user who is visiting the website, the page they’re visiting, the page they came from, the web-browser user-agent string, etc. Say you’re the owner of this website, and you recently made a modification to you website which added a prominent link to some new content on the front-page. You want to know how many people are visiting your new content with this new link you’ve added, so we want to answer the question “how many times was our new link on the index page clicked by any user?”. For the purposes of this example, let’s assume we don’t have any index tables which might help us answer this query more efficiently.Let’s think about this query in terms of stateless operations and performing as much of a reduction in data returned to the client as possible. We have a few basic steps:  Filter: Ignore all clickstream events that are not for the index page.  Filter: Ignore all clickstream events that are not for the given anchor.  Aggregation: Only a sum of the occurrences is needed, not the full record.The beauty in using Accumulo is that all three of these operations can be performed inside of the tablet server process without returning any unnecessary data to the client. Unwanted records can be easily skipped, while each record that matches our criteria is reduced to a single “+1” counter. Instead of returning each full record to the client, the tablet server can combine these counts together and simply return a sum to the client for the Range of data that was read.The other perk of thinking about the larger problem in discrete steps is that it is easily parallelized. Assuming we have many tablet servers hosting the tablets which make up our clickstream table, we can easily run this query in parallel across them all using the BatchScanner. Additionally, because we’ve reduced our initial problem from a large collection of records to a stream of partial sums, we’ve drastically reduced the amount of work that must be performed on our (single) client. Each key-value pair returned by a server is a partial-sum which can be combined together in a very lightweight operation (in both memory and computation) as the result is made available. The client then has the simple task of performing one summation. We took a hard problem and performed an extreme amount of heavy lifting server-side while performing next-to-no computation in our client which is great for web applications or thin clients.Tiered ComputationThis type of algorithm, a multi-stage computation, becomes very common when working with Accumulo because of the ability to push large amounts of computation to each tablet server. Tablet servers can compute aggregations, filters and/or transformations very “close” to the actual data, return some reduced view of the data being read. Even when some function is very efficient, computing it over large data sets can still be extremely time-consuming. Eliminating unwanted data as early as possible can often outweigh even the most optimal algorithms due to the orders of magnitude difference in the speed of CPU over disk and network.It’s important to remember that this idea isn’t new, though. The above model is actually very reminiscent of the MapReduce paradigm, just applied with different constraints. The types of problems efficiently solvable by MapReduce is also a super-set of what is possible with one representation of data stored in Accumulo. This also isn’t a recommendation Accumulo Iterators are not a complete replacement for MapReduce (a tool is rarely a 100% “better” replacement for another). In fact, Accumulo Iterators are often used as another level of computation to make an existing MapReduce job more efficient, typically through the AccumuloInputFormat.We’ve identified a category of problems - a function is applied to a batch of key-value pairs which reduces the complexity of a question asked over a distributed dataset - in which the features and APIs of Accumulo lend themselves extremely well to solving in an efficient and simple manner. The ability to leverage Accumulo to perform these computations requires foresight into the types of questions that are to be asked of a dataset, the structure of the dataset within Accumulo, and the reduction of a larger problem into discrete functions which are each applied to the dataset by an Accumulo Iterator.",
      "url": " /blog/2014/07/09/functional-reads-over-accumulo.html",
      "categories": "blog"
    }
    ,
  
    "blog-2014-06-25-scaling-accumulo-with-multivolume-support-html": {
      "title": "Scaling Accumulo with Multi-Volume Support",
      "content": "Originally posted at https://blogs.apache.org/accumulo/entry/scaling_accumulo_with_multi_volumeMapReduce is a commonly used approach to querying or analyzing large amounts of data. Typically MapReduce jobs are created using using some set of files in HDFS to produce a result. When new files come in, they get added to the set, and the job gets run again. A common Accumulo approach to this scenario is to load all of the data into a single instance of Accumulo.A single instance of Accumulo can scale quite largely(1, 2) to accommodate high levels of ingest and query. The manner in which ingest is performed typically depends on latency requirements. When the desired latency is small, inserts are performed directly into Accumulo. When the desired latency is allowed to be large, then a bulk style of ingest can be used. There are other factors to consider as well, but they are outside the scope of this article.On large clusters using the bulk style of ingest input files are typically batched into MapReduce jobs to create a set of output RFiles for import into Accumulo. The number of files per job is typically determined by the required latency and the number of MapReduce tasks that the cluster can complete in the given time-frame. The resulting RFiles, when imported into Accumulo, are added to the list of files for their associated tablets. Depending on the configuration this will cause Accumulo to major compact these tablets. If the configuration is tweaked to allow more files per tablet, to reduce the major compactions, then more files need to be opened at query time when performing scans on the tablet. Note that no single node is burdened by the file management; but, the number of file operations in aggregate is very large. If each server has several hundred tablets, and there are a thousand tablet servers, and each tablet compacts some files every few imports, we easily have 50,000 file operations (create, allocate a block, rename and delete) every ingest cycle.In addition to the NameNode operations caused by bulk ingest, other Accumulo processes (e.g. master, gc) require interaction with the NameNode. Single processes, like the garbage collector, can be starved of responses from the NameNode as the NameNode is limited on the number of concurrent operations. It is not unusual for an operator’s request for “hadoop fs -ls /accumulo” to take a minute before returning results during the peak file-management periods. In particular, the file garbage collector can fall behind, not finishing a cycle of unreferenced file removal before the next ingest cycle creates a new batch of files to be deleted.The Hadoop community addressed the NameNode bottleneck issue with HDFS federation which allows a datanode to serve up blocks for multiple namenodes. Additionally, ViewFS allows clients to communicate with multiple namenodes through the use of a client-side mount table. This functionality was insufficient for Accumulo in the 1.6.0 release as ViewFS works at a directory level; as an example, /dirA is mapped to one NameNode and /dirB is mapped to another, and Accumulo uses a single HDFS directory for its storage.Multi-Volume support (MVS), included in 1.6.0, includes the changes that allow Accumulo to work across multiple HDFS clusters (called volumes in Accumulo) while continuing to use a single HDFS directory. A new property, instance.volumes, can be configured with multiple HDFS nameservices and Accumulo will use them all to balance out NameNode operations. The nameservices configured in instance.volumes may optionally use the High Availability NameNode feature as it is transparent to Accumulo. With MVS you have two options to horizontally scale your Accumulo instance. You can use an HDFS cluster with Federation and multiple NameNodes or you can use separate HDFS clusters.By default Accumulo will perform round-robin file allocation for each tablet, spreading the files across the different volumes. The file balancer is pluggable, allowing for custom implementations. For example, if you don’t use Federation and use multiple HDFS clusters, you may want to allocate all files for a particular table to one volume.Comments in the JIRA regarding backups could lead to follow-on work. With the inclusion of snapshots in HDFS, you could easily envision an application that quiesces the database or some set of tables, flushes their entries from memory, and snapshots their directories. These snapshots could then be copied to another HDFS instance either for an on-disk backup, or bulk-imported into another instance of Accumulo for testing or some other use.The example configuration below shows how to set up Accumulo with HA NameNodes and Federation, as it is likely the most complex. We had to reference several web sites, one of the HDFS mailing lists, and the source code to find all of the configuration parameters that were needed. The configuration below includes two sets of HA namenodes, each set servicing an HDFS nameservice in a single HDFS cluster. In the example below, nameserviceA is serviced by name nodes 1 and 2, and nameserviceB is serviced by name nodes 3 and 4.core-site.xml&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;viewfs:///&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.viewfs.mounttable.default.link./nameserviceA&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;hdfs://nameserviceA&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.viewfs.mounttable.default.link./nameserviceB&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;hdfs://nameserviceB&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;fs.viewfs.mounttable.default.link./nameserviceA/accumulo/instance_id&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;hdfs://nameserviceA/accumulo/instance_id&amp;lt;/value&amp;gt;  &amp;lt;description&amp;gt;Workaround for ACCUMULO-2719&amp;lt;/description&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.ha.fencing.methods&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;sshfence(hdfs:22)         shell(/bin/true)&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.ha.fencing.ssh.private-key-files&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;&amp;lt;PRIVATE_KEY_LOCATION&amp;gt;&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.ha.fencing.ssh.connect-timeout&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;30000&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;ha.zookeeper.quorum&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;zkHost1:2181,zkHost2:2181,zkHost3:2181&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;hdfs-site.xml&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.nameservices&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;nameserviceA,nameserviceB&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.ha.namenodes.nameserviceA&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;nn1,nn2&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.ha.namenodes.nameserviceB&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;nn3,nn4&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.namenode.rpc-address.nameserviceA.nn1&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;host1:8020&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.namenode.rpc-address.nameserviceA.nn2&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;host2:8020&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.namenode.http-address.nameserviceA.nn1&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;host1:50070&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.namenode.http-address.nameserviceA.nn2&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;host2:50070&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.namenode.rpc-address.nameserviceB.nn3&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;host3:8020&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.namenode.rpc-address.nameserviceB.nn4&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;host4:8020&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.namenode.http-address.nameserviceB.nn3&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;host3:50070&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.namenode.http-address.nameserviceB.nn4&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;host4:50070&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.namenode.shared.edits.dir.nameserviceA.nn1&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceA&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.namenode.shared.edits.dir.nameserviceA.nn2&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceA&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.namenode.shared.edits.dir.nameserviceB.nn3&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceB&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.namenode.shared.edits.dir.nameserviceB.nn4&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;qjournal://jHost1:8485;jHost2:8485;jHost3:8485/nameserviceB&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.client.failover.proxy.provider.nameserviceA&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.client.failover.proxy.provider.nameserviceB&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.ha.automatic-failover.enabled.nameserviceA&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;dfs.ha.automatic-failover.enabled.nameserviceB&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;accumulo-site.xml&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;instance.volumes&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;hdfs://nameserviceA/accumulo,hdfs://nameserviceB/accumulo&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;",
      "url": " /blog/2014/06/25/scaling-accumulo-with-multivolume-support.html",
      "categories": "blog"
    }
    ,
  
    "blog-2014-05-27-getting-started-with-accumulo-1-6-0-html": {
      "title": "Getting Started with Apache Accumulo 1.6.0",
      "content": "Originally posted at https://blogs.apache.org/accumulo/entry/getting_started_with_apache_accumuloOn May 12th, 2014, the Apache Accumulo project happily announced version 1.6.0 to the community. This is a new major release for the project which contains many numerous new features and fixes. For the full list of notable changes, I’d recommend that you check out the release notes that were published alongside the release itself. For this post, I’d like to cover some of the changes that have been made at the installation level that are a change for users who are already familiar with the project.Download the releaseLike always, you can find out releases on the our downloads page.  You have the choice of downloading the source and building it yourself, or choosing the binary tarball which already contains pre-built jars for use.Native MapsOne of the major components of the original BigTable design was an “In-Memory Map” which provided fast insert and read operations. Accumulo implements this using a C++ sorted map with a custom allocator which is invoked by the TabletServer using JNI. Each TabletServer uses its own “native” map. It is highly desirable to use this native map as it comes with a notable performance increase over a Java map (which is the fallback when the Accumulo shared library is not found) in addition to greatly reducing the TabletServer’s JVM garbage collector stress when ingesting data.In previous versions, the binary tarball contained a pre-compiled version of the native library (under lib/native/). Shipping a compiled binary was a convenience but also left much confusion when it didn’t work on systems which had different, incompatible versions of GCC toolchains installed than what the binary was built against. As such, we have stopped bundling the pre-built shared library in favor of users building this library on their own, and instead include an accumulo-native.tar.gz file within the lib directory which contains the necessary files to build the library yourself.To reduce the burden on users, we’ve also introduced a new script inside of the bin directory:build_native_map.shInvoking this script will automatically unpack, build and install the native map in $ACCUMULO_HOME/lib/native. If you’ve used older versions of Accumulo, you will also notice that the library name is different in an attempt to better follow standard conventions: libaccumulo.so on Linux and libaccumulo.dylib on Mac OS X.Example ConfigurationsApache Accumulo still bundles a set of example configuration files in conf/examples. Each sub-directory contains the complete set of files to run on a single node with the named memory limitations. For example, the files contained in conf/examples/3GB/native-standalone will run Accumulo on a single node, with native maps (don’t forget to build them first!), within a total memory footprint of 3GB. Copy the contents of one of these directories into conf/ and make sure that your relevant installation details (e.g. HADOOP_PREFIX, JAVA_HOME, etc) are properly set in accumulo-env.sh. For example:cp $ACCUMULO_HOME/conf/examples/3G/native-standalone/* $ACCUMULO_HOME/confAlternatively, a new script, bootstrap_config.sh, was also introduced that can be invoked instead of manually copying files. It will step through a few choices (memory usage, in-memory map type, and Hadoop major version), and then automatically create the configuration files for you.$ACCUMULO_HOME/bin/bootstrap_config.shOne notable change in these scripts over previous versions is that they default to using Apache Hadoop 2 packaging details, such as the Hadoop conf directory and jar locations. It is highly recommended by the community that you use Apache Accumulo 1.6.0 with at least Apache Hadoop 2.2.0, most notably, to ensure that you will not lose data in the face of power failure. If you are still running on a Hadoop 1 release (1.2.1), you will need to edit both accumulo-env.sh and accumulo-site.xml. There are comments in each file which instruct you what needs to be changed.Starting AccumuloInitializing and starting Accumulo hasn’t changed at all! After you have created the configuration files and, if you’re using them, built the native maps, run:accumulo initThis will prompt you to name your Accumulo instance and set the Accumulo root user’s password, then start Accumulo using$ACCUMULO_HOME/bin/start-all.sh",
      "url": " /blog/2014/05/27/getting-started-with-accumulo-1.6.0.html",
      "categories": "blog"
    }
    ,
  
    "blog-2014-05-03-accumulo-classloader-html": {
      "title": "The Accumulo ClassLoader",
      "content": "Originally posted at https://blogs.apache.org/accumulo/entry/the_accumulo_classloaderThe Accumulo classloader is an integral part of the software. The classloader is created before each of the services (master, tserver, gc, etc) are started and it is set as the classloader for that service. The classloader was rewritten in version 1.5 and this article will explain the new behavior.First, some historyThe classloader in version 1.4 used a simple hierarchy of two classloaders that would load classes from locations specified by two properties. The locations specified by the “general.classpaths” property would be used to create a parent classloader and locations specified by the “general.dynamic.classpaths” property were used to create a child classloader. The child classloader would monitor the specified locations for changes and when a change occurred the child classloader would be replaced with a new instance. Classes that referenced the orphaned child classloader would continue to work and the classloader would be garbage collected when no longer referenced. The diagram below shows the relationship between the classloaders in Accumulo 1.4.The only place where the dynamic classloader would come into play is for user iterators and their dependencies. The general advice for using this classloader would be to put the jars containing your iterators in the dynamic location. Everything else that does not change very often or would require a restart should be put into the non-dynamic location.There are a couple of things to note about the classloader in 1.4. First, if you modified the dynamic locations too often, you would run out of perm-gen space. This is likely due to unreferenced classes not being unloaded from the JVM. This is captured in ACCUMULO-599. Secondly, when you modified files in dynamic locations within the same cycle, it would on occasion miss the second change.Out with the old, in with the newThe Accumulo classloader was rewritten in version 1.5. It maintains the same dynamic capability and includes a couple of new features. The classloader uses Commons VFS so that it can load jars and classes from a variety of sources, including HDFS. Being able to load jars from one location (hdfs, http, etc) will make it easier to deploy changes to your cluster. Additionally, we introduced the notion of classloader contexts into Accumulo. This is not a new concept for anyone that has used an application server, but the implementation is a little different for Accumulo.The hierarchy set up by the new classloader uses the same property names as the old classloader. In the most basic configuration the locations specified by “general.classpaths” are used to create the root of the application classloader hierarchy. This classloader is a URLClassLoader and it does not support dynamic reloading. If you only specify this property, then you are loading all of your jars from the local file system and they will not be monitored for changes. We will call this top level application classloader the SYSTEM classloader. Next, a classloader is created that supports VFS sources and reloading. The parent of this classloader is the SYSTEM classloader and we will call this the VFS classloader. If the “general.vfs.classpaths” property is set, the VFS classloader will use this location. If the property is not set, it will use the value of “general.dynamic.classpaths” with a default value of $ACCUMULO_HOME/lib/ext to support backwards compatibility. The diagram below shows the relationship between the classloaders in Accumulo 1.5.Running Accumulo From HDFSIf you have defined “general.vfs.classpaths” in your Accumulo configuration, then you can use the bootstrap_hdfs.sh script in the bin directory to seed HDFS with the Accumulo jars. A couple of jars will remain on the local file system for starting services. Now when you start up Accumulo the master, gc, tracer, and all of the tablet servers will get their jars and classes from HDFS. The bootstrap_hdfs.sh script sets the replication on the directory, but you may want to set it higher after bootstrapping. An example configuration setting would be:&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;general.vfs.classpaths&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;hdfs://localhost:8020/accumulo/system-classpath&amp;lt;/value&amp;gt;  &amp;lt;description&amp;gt;Configuration for a system level vfs classloader. Accumulo jars can be configured here and loaded out of HDFS.&amp;lt;/description&amp;gt;&amp;lt;/property&amp;gt;About ContextsYou can also define classloader contexts in your accumulo-site.xml file. A context is defined by a user supplied name and it references locations like the other classloader properties. When a context is defined in the configuration, it can then be applied to one or more tables. When a context is applied to a table, then a classloader is created for that context. If multiple tables use the same context, then they share the context classloader. The context classloader is a child to the VFS classloader created above.The goal here is to enable multiple tenants to share the same Accumulo instance. For example, we may have a context called ‘app1’ which references the jars for application A. We may also have another context called app2 which references the jars for application B. By default the context classloader delegates to the VFS classloader. This behavior may be overridden as seen in the app2 example below. The context classloader also supports reloading like the VFS classloader.&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;general.vfs.context.classpath.app1&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;hdfs://localhost:8020/applicationA/classpath/.*.jar,file:///opt/applicationA/lib/.*.jar&amp;lt;/value&amp;gt;  &amp;lt;description&amp;gt;Application A classpath, loads jars from HDFS and local file system&amp;lt;/description&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt;  &amp;lt;name&amp;gt;general.vfs.context.classpath.app2.delegation=post&amp;lt;/name&amp;gt;  &amp;lt;value&amp;gt;hdfs://localhost:8020/applicationB/classpath/.*.jar,http://my-webserver/applicationB/.*.jar&amp;lt;/value&amp;gt;  &amp;lt;description&amp;gt;Application B classpath, loads jars from HDFS and HTTP, does not delegate to parent first&amp;lt;/description&amp;gt;&amp;lt;/property&amp;gt;Context classloaders do not have to be defined in the accumulo-site.xml file. The “general.vfs.context.classpath.{context}” property can be defined on the table either programmatically or manually in the shell. Then set the “table.classpath.context” property on your table.Known IssuesRemember the two issues I mentioned above? Well, they are still a problem.  ACCUMULO-1507 is tracking VFS-487 for frequent modifications to files.  If you start running out of perm-gen space, take a look at ACCUMULO-599 and try applying the JVM settings for class unloading.  Additionally, there is an issue with the bootstrap_hdfs.sh script detailed in ACCUMULO-2761. There is a workaround listed in the issue.Please email the dev list for comments and questions.",
      "url": " /blog/2014/05/03/accumulo-classloader.html",
      "categories": "blog"
    }
    ,
  
    "release-accumulo-1-6-0": {
      "title": "Apache Accumulo 1.6.0",
      "content": "Apache Accumulo 1.6.0 adds some major new features and fixes many bugs.  This release contains changes from 609 issues contributed by 36 contributors and committers.Below are resources for this release:  User Manual  Javadocs  ExamplesAccumulo 1.6.0 runs on Hadoop 1, however Hadoop 2 with HA namenode is recommended for production systems.  In addition to HA, Hadoop 2 also offers better data durability guarantees, in the case when nodes lose power, than Hadoop 1.Notable ImprovementsMultiple volume supportBigTable’s design allows for its internal metadata to automatically spread across multiple nodes.  Accumulo has followed this design and scales very well as a result.  There is one impediment to scaling though, and this is the HDFS namenode.  There are two problems with the namenode when it comes to scaling.  First, the namenode stores all of its filesystem metadata in memory on a single machine.  This introduces an upper bound on the number of files Accumulo can have.  Second, there is an upper bound on the number of file operations per second that a single namenode can support.  For example, a namenode can only support a few thousand delete or create file request per second.To overcome this bottleneck, support for multiple namenodes was added under ACCUMULO-118.  This change allows Accumulo to store its files across multiple namenodes.  To use this feature, place comma separated list of namenode URIs in the new instance.volumes configuration property in accumulo-site.xml.  When upgrading to 1.6.0 and multiple namenode support is desired, modify this setting only after a successful upgrade.Table namespacesAdministering an Accumulo instance with many tables is cumbersome.  To ease this, ACCUMULO-802 introduced table namespaces which allow tables to be grouped into logical collections.  This allows configuration and permission changes to made to a namespace, which will apply to all of its tables.Conditional MutationsAccumulo now offers a way to make atomic read,modify,write row changes from the client side.  Atomic test and set row operations make this possible.  ACCUMULO-1000 added conditional mutations and a conditional writer.  A conditional mutation has tests on columns that must pass before any changes are made.  These test are executed in server processes while a row lock is held.  Below is a simple example of making atomic row changes using conditional mutations.  Read columns X,Y,SEQ into a,b,s from row R1 using an isolated scanner.  For row R1 write conditional mutation X=f(a),Y=g(b),SEQ=s+1 if SEQ==s.  If conditional mutation failed, then goto step 1.The only built in test that conditional mutations support are equality and isNull.  However, iterators can be configured on a conditional mutation to run before these test.  This makes it possible to implement any number of test such as less than, greater than, contains, etc.EncryptionEncryption is still an experimental feature, but much progress has been made since 1.5.0.  Support for encrypting rfiles and write ahead logs were added in ACCUMULO-958 and ACCUMULO-980.  Support for encrypting data over the wire using SSL was added in ACCUMULO-1009.When a tablet server fails, its write ahead logs are sorted and stored in HDFS.  In 1.6.0, encrypting these sorted write ahead logs is not supported.  ACCUMULO-981 is open to address this issue.Pluggable compaction strategiesOne of the key elements of the BigTable design is use of the Log Structured Merge Tree.  This entails sorting data in memory, writing out sorted files, and then later merging multiple sorted files into a single file.   These automatic merges happen in the background and Accumulo decides when to merge files based comparing relative sizes of files to a compaction ratio.  Before 1.6.0 adjusting the compaction ratio was the only way a user could control this process.  ACCUMULO-1451 introduces pluggable compaction strategies which allow users to choose when and what files to compact.  ACCUMULO-1808 adds a compaction strategy that prevents compaction of files over a configurable size.LexicodersAccumulo only sorts data lexicographically.  Getting something like a pair of (String,Integer) to sort correctly in Accumulo is tricky.  It’s tricky because you only want to compare the integers if the strings are equal.  It’s possible to make this sort properly in Accumulo if the data is encoded properly, but can be difficult.  To make this easier ACCUMULO-1336 added Lexicoders to the Accumulo API.  Lexicoders provide an easy way to serialize data so that it sorts properly lexicographically.  Below is a simple example.PairLexicoder plex = new PairLexicoder(new StringLexicoder(), new IntegerLexicoder());byte[] ba1 = plex.encode(new ComparablePair&amp;lt;String, Integer&amp;gt;(&quot;b&quot;,1));byte[] ba2 = plex.encode(new ComparablePair&amp;lt;String, Integer&amp;gt;(&quot;aa&quot;,1));byte[] ba3 = plex.encode(new ComparablePair&amp;lt;String, Integer&amp;gt;(&quot;a&quot;,2));byte[] ba4 = plex.encode(new ComparablePair&amp;lt;String, Integer&amp;gt;(&quot;a&quot;,1));byte[] ba5 = plex.encode(new ComparablePair&amp;lt;String, Integer&amp;gt;(&quot;aa&quot;,-3));//sorting ba1,ba2,ba3,ba4, and ba5 lexicographically will result in the same order as sorting the ComparablePairsLocality groups in memoryIn cases where a very small amount of data is stored in a locality group one would expect fast scans over that locality group.  However this was not always the case because recently written data stored in memory was not partitioned by locality group.  Therefore if a table had 100GB of data in memory and 1MB of that was in locality group A, then scanning A would have required reading all 100GB.  ACCUMULO-112 changes this and partitions data by locality group as its written.Service IP addressesPrevious versions of Accumulo always used IP addresses internally.  This could be problematic in virtual machine environments where IP addresses change.  In ACCUMULO-1585 this was changed, now Accumulo uses the exact hostnames from its config files for internal addressing.All Accumulo processes running on a cluster are locatable via zookeeper.  Therefore using well known ports is not really required.  ACCUMULO-1664 makes it possible to for all Accumulo processes to use random ports.  This makes it easier to run multiple Accumulo instances on a single node.While Hadoop does not support IPv6 networks, attempting to run on a system that does not have IPv6 completely disabled can cause strange failures. ACCUMULO-2262 invokes the JVM-provided configuration parameter at process startup to prefer IPv4 over IPv6.ViewFSMultiple bug-fixes were made to support running Accumulo over multiple HDFS instances using ViewFS. ACCUMULO-2047 is the parentticket that contains numerous fixes to enable this support.Maven PluginThis version of Accumulo is accompanied by a new maven plugin for testing client apps (ACCUMULO-1030). You can execute the accumulo-maven-plugin inside your project by adding the following to your pom.xml’s build plugins section:&amp;lt;plugin&amp;gt;  &amp;lt;groupId&amp;gt;org.apache.accumulo&amp;lt;/groupId&amp;gt;  &amp;lt;artifactId&amp;gt;accumulo-maven-plugin&amp;lt;/artifactId&amp;gt;  &amp;lt;version&amp;gt;1.6.0&amp;lt;/version&amp;gt;  &amp;lt;configuration&amp;gt;    &amp;lt;instanceName&amp;gt;plugin-it-instance&amp;lt;/instanceName&amp;gt;    &amp;lt;rootPassword&amp;gt;ITSecret&amp;lt;/rootPassword&amp;gt;  &amp;lt;/configuration&amp;gt;  &amp;lt;executions&amp;gt;    &amp;lt;execution&amp;gt;      &amp;lt;id&amp;gt;run-plugin&amp;lt;/id&amp;gt;      &amp;lt;goals&amp;gt;        &amp;lt;goal&amp;gt;start&amp;lt;/goal&amp;gt;        &amp;lt;goal&amp;gt;stop&amp;lt;/goal&amp;gt;      &amp;lt;/goals&amp;gt;    &amp;lt;/execution&amp;gt;  &amp;lt;/executions&amp;gt;&amp;lt;/plugin&amp;gt;This plugin is designed to work in conjunction with the maven-failsafe-plugin. A small test instance of Accumulo will run during the pre-integration-test phase of the Maven build lifecycle, and will be stopped in the post-integration-test phase. Your integration tests, executed by maven-failsafe-plugin can access this instance with a MiniAccumuloInstance connector (the plugin uses MiniAccumuloInstance, internally), as in the following example:private static Connector conn;@BeforeClasspublic static void setUp() throws Exception {  String instanceName = &quot;plugin-it-instance&quot;;  Instance instance = new MiniAccumuloInstance(instanceName, new File(&quot;target/accumulo-maven-plugin/&quot; + instanceName));  conn = instance.getConnector(&quot;root&quot;, new PasswordToken(&quot;ITSecret&quot;));}This plugin is quite limited, currently only supporting an instance name and a root user password as configuration parameters. Improvements are expected in future releases, so feedback is welcome and appreciated (file bugs/requests under the “maven-plugin” component in the Accumulo JIRA).PackagingOne notable change that was made to the binary tarball is the purposeful omission of a pre-built copy of the Accumulo “native map” library.This shared library is used at ingest time to implement an off-JVM-heap sorted map that greatly increases ingest throughput while side-steppingissues such as JVM garbage collection pauses. In earlier releases, a pre-built copy of this shared library was included in the binary tarball; however, the decision was made to omit this due to the potential variance in toolchains on the target system.It is recommended that users invoke the provided build_native_library.sh before running Accumulo:$ACCUMULO_HOME/bin/build_native_library.shBe aware that you will need a C++ compiler/toolchain installed to build this library. Check your GNU/Linux distribution documentation for the package manager command.Size-Based Constraint on New TablesA Constraint is an interface that can determine if a Mutation should be applied or rejected server-side. After ACCUMULO-466, new tables that are created in 1.6.0 will automatically have the DefaultKeySizeConstraint set.As performance can suffer when large Keys are inserted into a table, this Constraint will reject any Key that is larger than 1MB. If this constraint is undesired, it can be removed using the constraint shellcommand. See the help message on the command for more information.Other notable changes  ACCUMULO-842 Added FATE administration to shell  ACCUMULO-1042 CTRL-C no longer kills shell  ACCUMULO-1345 Stuck compactions now log a warning with a stack trace, tablet id, and filename.  ACCUMULO-1442 JLine2 support was added to the shell.  This adds features like history search and other nice things GNU Readline has.  ACCUMULO-1481 The root tablet is now the root table.  ACCUMULO-1537 Python functional test were converted to maven Integration test that use MAC  ACCUMULO-1566 When read-ahead starts in the scanner is now configurable.  ACCUMULO-1650 Made common admin commands easier to run, try bin/accumulo admin --help  ACCUMULO-1667 Added a synchronous version of online and offline table  ACCUMULO-1706 Admin utilities now respect EPIPE  ACCUMULO-1833 Multitable batch writer is faster now when used by multiple threads  ACCUMULO-1933 Lower case can be given for memory units now.  ACCUMULO-1985 Configuration to bind Monitor on all network interfaces.  ACCUMULO-2128 Provide resource cleanup via static utility  ACCUMULO-2360 Allow configuration of the maximum thrift message size a server will read.Notable Bug Fixes  ACCUMULO-324 System/site constraints and iterators should NOT affect the METADATA table  ACCUMULO-335 Can’t batchscan over the !METADATA table  ACCUMULO-391 Added support for reading from multiple tables in a Map Reduce job.  ACCUMULO-1018 Client does not give informative message when user can not read table  ACCUMULO-1492 bin/accumulo should follow symbolic links  ACCUMULO-1572 Single node zookeeper failure kills connected Accumulo servers  ACCUMULO-1661 AccumuloInputFormat cannot fetch empty column family  ACCUMULO-1696 Deep copy in the compaction scope iterators can throw off the stats  ACCUMULO-1698 stop-here doesn’t consider system hostname  ACCUMULO-1901 start-here.sh starts only one GC process even if more are defined  ACCUMULO-1920 Monitor was not seeing zookeeper updates for tables  ACCUMULO-1994 Proxy does not handle Key timestamps correctly  ACCUMULO-2037 Tablets are now assigned to the last location  ACCUMULO-2174 VFS Classloader has potential to collide localized resources  ACCUMULO-2225 Need to better handle DNS failure propagation from Hadoop  ACCUMULO-2234 Cannot run offline mapreduce over non-default instance.dfs.dir value  ACCUMULO-2261 Duplicate locations for a Tablet.  ACCUMULO-2334 Lacking fallback when ACCUMULO_LOG_HOST isn’t set  ACCUMULO-2408 metadata table not assigned after root table is loaded  ACCUMULO-2519 FATE operation failed across upgradeKnown IssuesSlower writes than previous Accumulo versionsWhen using Accumulo 1.6 and Hadoop 2, Accumulo will call hsync() on HDFS.Calling hsync improves durability by ensuring data is on disk (where other olderHadoop versions might lose data in the face of power failure); however, callinghsync frequently does noticeably slow writes. A simple work around is to increasethe value of the tserver.mutation.queue.max configuration parameter via accumulo-site.xml.A value of “4M” is a better recommendation, and memory consumption will increase bythe number of concurrent writers to that TabletServer. For example, a value of 4M with50 concurrent writers would equate to approximately 200M of Java heap being used formutation queues.For more information, see ACCUMULO-1950 and this comment.Another possible cause of slower writes is the change in write ahead log replicationbetween 1.4 and 1.5.  Accumulo 1.4. defaulted to two loggers servers.  Accumulo 1.5 and 1.6 storewrite ahead logs in HDFS and default to using three datanodes.BatchWriter hold time errorIf a BatchWriter fails with MutationsRejectedException and the  message contains&quot;# server errors 1&quot; then it may be ACCUMULO-2388.  To confirm this look in the tablet server logsfor org.apache.accumulo.tserver.HoldTimeoutException around the time the BatchWriter failed.If this is happening often a possible work around is to set general.rpc.timeout to 240s.Other known issues  ACCUMULO-981 Sorted write ahead logs are not encrypted.  ACCUMULO-1507 Dynamic Classloader still can’t keep proper track of jars  ACCUMULO-1588 Monitor XML and JSON differ  ACCUMULO-1628 NPE on deep copied dumped memory iterator  ACCUMULO-1708 ACCUMULO-2495 Out of memory errors do not always kill tservers leading to unexpected behavior  ACCUMULO-2008 Block cache reserves section for in-memory blocks  ACCUMULO-2059 Namespace constraints easily get clobbered by table constraints  ACCUMULO-2677 Tserver failure during map reduce reading from table can cause sub-optimal performanceDocumentation updates  ACCUMULO-1218 document the recovery from a failed zookeeper  ACCUMULO-1375 Update README files in proxy module.  ACCUMULO-1407 Fix documentation for deleterows  ACCUMULO-1428 Document native maps  ACCUMULO-1946 Include dfs.datanode.synconclose in hdfs configuration documentation  ACCUMULO-1956 Add section on decommissioning or adding nodes to an Accumulo cluster  ACCUMULO-2441 Document internal state stored in RFile names  ACCUMULO-2590 Update public API in readme to clarify what’s includedAPI ChangesThe following deprecated methods were removed in ACCUMULO-1533  Many map reduce methods deprecated in ACCUMULO-769 were removed  SecurityErrorCode o.a.a.core.client.AccumuloSecurityException.getErrorCode() deprecated in ACCUMULO-970  Connector o.a.a.core.client.Instance.getConnector(AuthInfo) deprecated in ACCUMULO-1024  Connector o.a.a.core.client.ZooKeeperInstance.getConnector(AuthInfo) deprecated in ACCUMULO-1024  static String o.a.a.core.client.ZooKeeperInstance.getInstanceIDFromHdfs(Path) deprecated in ACCUMULO-1  static String ZooKeeperInstance.lookupInstanceName (ZooCache,UUID) deprecated in ACCUMULO-765  void o.a.a.core.client.ColumnUpdate.setSystemTimestamp(long)  deprecated in ACCUMULO-786TestingBelow is a list of all platforms that 1.6.0 was tested against by developers. Each Apache Accumulo releasehas a set of tests that must be run before the candidate is capable of becoming an official release. That list includes the following:  Successfully run all unit tests  Successfully run all functional test (test/system/auto)  Successfully complete two 24-hour RandomWalk tests (LongClean module), with and without “agitation”  Successfully complete two 24-hour Continuous Ingest tests, with and without “agitation”, with data verification  Successfully complete two 72-hour Continuous Ingest tests, with and without “agitation”Each unit and functional test only runs on a single node, while the RandomWalk and Continuous Ingest tests runon any number of nodes. Agitation refers to randomly restarting Accumulo processes and Hadoop Datanode processes,and, in HDFS High-Availability instances, forcing NameNode failover.The following acronyms are used in the test testing table.  CI : Continuous Ingest  HA : High-Availability  IT : Integration test, run w/ mvn verify  RW : Random Walk            OS      Java      Hadoop      Nodes      ZooKeeper      HDFS HA      Version/Commit hash      Tests                  CentOS 6.5      CentOS OpenJDK 1.7      Apache 2.2.0      20 EC2 nodes      Apache 3.4.5      No      1.6.0 RC1 + ACCUMULO_2668 patch      24-hour CI w/o agitation. Verified.              CentOS 6.5      CentOS OpenJDK 1.7      Apache 2.2.0      20 EC2 nodes      Apache 3.4.5      No      1.6.0 RC2      24-hour RW (Conditional.xml module) w/o agitation              CentOS 6.5      CentOS OpenJDK 1.7      Apache 2.2.0      20 EC2 nodes      Apache 3.4.5      No      1.6.0 RC5      24-hour CI w/ agitation. Verified.              CentOS 6.5      CentOS OpenJDK 1.6 and 1.7      Apache 1.2.1, 2.2.0      Single      Apache 3.3.6      No      1.6.0 RC5      All unit and ITs w/  -Dhadoop.profile=2 and -Dhadoop.profile=1              Gentoo      Sun JDK 1.6.0_45      Apache 1.2.1, 2.2.0, 2.3.0, 2.4.0      Single      Apache 3.4.5      No      1.6.0 RC5      All unit and ITs. 2B entries ingested/verified with CI              CentOS 6.4      Sun JDK 1.6.0_31      CDH 4.5.0      7      CDH 4.5.0      Yes      1.6.0 RC4 and RC5      24-hour RW (LongClean) with and without agitation              CentOS 6.4      Sun JDK 1.6.0_31      CDH 4.5.0      7      CDH 4.5.0      Yes      3a1b38      72-hour CI with and without agitation. Verified.              CentOS 6.4      Sun JDK 1.6.0_31      CDH 4.5.0      7      CDH 4.5.0      Yes      1.6.0 RC2      24-hour CI without agitation. Verified.              CentOS 6.4      Sun JDK 1.6.0_31      CDH 4.5.0      7      CDH 4.5.0      Yes      1.6.0 RC3      24-hour CI with agitation. Verified.      ",
      "url": " /release/accumulo-1.6.0/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-4-5": {
      "title": "Apache Accumulo 1.4.5",
      "content": "Apache Accumulo 1.4.5 is a maintenance release containing these changes.Below are resources for this release:  User Manual - In-depth developer and administrator documentation  Javadocs - Accumulo API  Examples - Example Accumulo code",
      "url": " /release/accumulo-1.4.5/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-5-1": {
      "title": "Apache Accumulo 1.5.1",
      "content": "Apache Accumulo 1.5.1 is a maintenance release on the 1.5 version branch.This release contains changes from over 200 issues, comprised of bug fixes(client side and server side), new test cases, and updated Hadoop supportcontributed by over 30 different contributors and committers.Below are resources for this release:  User Manual  Javadocs  ExamplesAs this is a maintenance release, Apache Accumulo 1.5.1 has no client APIincompatibilities over Apache Accumulo 1.5.0 and requires no manual upgradeprocess. Users of 1.5.0 are strongly encouraged to update as soon as possibleto benefit from the improvements.Notable ImprovementsWhile new features are typically not added in a bug-fix release as 1.5.1, thecommunity does create a variety of improvements that are API compatible. Containedhere are some of the more notable improvements.PermGen Leak from Client APIAccumulo’s client code creates background threads that users presently cannotstop through the API. This is quick to cause problems when invoking the AccumuloAPI in application containers such as Apache Tomcat or JBoss and repeatedlyredeploying an application. ACCUMULO-2128 introduces a static utility,org.apache.accumulo.core.util.CleanUp, that users can invoke as part of ateardown hook in their container that will stop these threads and avoidthe eventual OutOfMemoryError “PermGen space”.Prefer IPv4 when starting Accumulo processesWhile Hadoop does not support IPv6 networks, attempting to run on asystem that does not have IPv6 completely disabled can cause strange failures.ACCUMULO-2262 invokes the JVM-provided configuration parameter at processstartup to prefer IPv4 over IPv6.Memory units in configurationIn previous versions, units of memory had to be provided as upper-case (e.g. ‘2G’, not ‘2g’).Additionally, a non-intuitive error was printed when a lower-case unit was provided.ACCUMULO-1933 allows lower-case memory units in all Accumulo configurations.Apache Thrift maximum frame sizeApache Thrift is used as the internal RPC service. ACCUMULO-2360 allowsusers to configure the maximum frame size an Accumulo server will read. Thisprevents non Accumulo client from connecting and causing memory exhaustion.MultiTableBatchWriter concurrencyThe MultiTableBatchWriter is a class which allows multiple tables to be written tofrom a single object that maintains a single buffer for caching Mutations across all tables. This is desirableas it greatly simplifies the JVM heap usage from caching Mutations acrossmany tables. Sadly, in Apache Accumulo 1.5.0, concurrent access to a single MultiTableBatchWriterheavily suffered from synchronization issues. ACCUMULO-1833 introduces a fixwhich alleviates the blocking and idle-wait that previously occurred when multiple threads accesseda single MultiTableBatchWriter instance concurrently.Hadoop VersionsSince Apache Accumulo 1.5.0 was released, Apache Hadoop 2.2.0 was also releasedas the first generally available (GA) Hadoop 2 release. This was a very exciting releasefor a number of reasons, but this also caused additional effort on Accumulo’s part toensure that Apache Accumulo continues to work across multiple Hadoop versions. Apache Accumulo 1.5.1should function with any recent Hadoop 1 or Hadoop 2 without any special steps, tricks or instructionsrequired.Notable Bug FixesAs with any Apache Accumulo release, we have numerous bug fixes that have been fixed. Mostare very subtle and won’t affect the common user; however, some notable bugs were resolvedas a part of 1.5.1 that are rather common.Failure of ZooKeeper server in quorum kills connected Accumulo servicesApache ZooKeeper provides a number of wonderful features that Accumulo uses to accomplisha variety of tasks, most notably a distributed locking service. Typically, multiple ZooKeeperservers are run to provide resilience against a certain number of node failures. ACCUMULO-1572resolves an issue where Accumulo processes would kill themselves when the ZooKeeper server theywere communicating with died instead of failing over to another ZooKeeper server in the quorum.Monitor table state isn’t updatedThe Accumulo Monitor contains a column for the state of each table in the Accumulo instance.The previous resolution was to restart the Monitor process when it got in this state.ACCUMULO-1920 resolves an issue where the Monitor would not see updates from ZooKeeper.Two locations for the same extentThe !METADATA table is the brains behind the data storage for each table, tracking informationlike which files comprise a Tablet, and which TabletServers are hosting which Tablets. ACCUMULO-2057fixes an issue where the !METADATA table contained multiple locations (hosting server) fora single Tablet.Deadlock on !METADATA tablet unloadTablets are unloaded, typically, when a shutdown request is issued. ACCUMULO-1143 resolvesa potential deadlock issue when a merging-minor compaction is issued to flush in-memory datato disk before unloading a Tablet.Other notable fixes  ACCUMULO-1800 Fixed deletes made via the Proxy.  ACCUMULO-1994 Fixed ranges in the Proxy.  ACCUMULO-2234 Fixed offline map reduce over non default HDFS location.  ACCUMULO-1615 Fixed service accumulo-tserver stop.  ACCUMULO-1876 Fixed issues depending on Accumulo using Apache Ivy.  ACCUMULO-2261 Duplicate locations for a Tablet.  ACCUMULO-2037 Tablets assigned to previous location.  ACCUMULO-1821 Avoid recovery on recovering Tablets.  ACCUMULO-2078 Incorrectly computed ACCUMULO_LOG_HOST in example configurations.  ACCUMULO-1985 Configuration to bind Monitor on all network interfaces.  ACCUMULO-1999 Allow ‘0’ to signify random port for the Master.  ACCUMULO-1630 Fixed GC to interpret any IP/hostname.Known IssuesWhen using Accumulo 1.5 and Hadoop 2, Accumulo will call hsync() on HDFS.Calling hsync improves durability by ensuring data is on disk (where other olderHadoop versions might lose data in the face of power failure); however, callinghsync frequently does noticeably slow writes. A simple work around is to increasethe value of the tserver.mutation.queue.max configuration parameter via accumulo-site.xml.A value of “4M” is a better recommendation, and memory consumption will increase bythe number of concurrent writers to that TabletServer. For example, a value of 4M with50 concurrent writers would equate to approximately 200M of Java heap being used formutation queues.For more information, see ACCUMULO-1950 and this comment.DocumentationThe following documentation updates were made:  ACCUMULO-1956  ACCUMULO-1428  ACCUMULO-1687  ACCUMULO-2141  ACCUMULO-1946  ACCUMULO-2223  ACCUMULO-2226  ACCUMULO-1470TestingBelow is a list of all platforms that 1.5.1 was tested against by developers. Each Apache Accumulo releasehas a set of tests that must be run before the candidate is capable of becoming an official release. That list includes the following:  Successfully run all unit tests  Successfully run all functional test (test/system/auto)  Successfully complete two 24-hour RandomWalk tests (LongClean module), with and without “agitation”  Successfully complete two 24-hour Continuous Ingest tests, with and without “agitation”, with data verification  Successfully complete two 72-hour Continuous Ingest tests, with and without “agitation”Each unit and functional test only runs on a single node, while the RandomWalk and Continuous Ingest tests runon any number of nodes. Agitation refers to randomly restarting Accumulo processes and Hadoop Datanode processes,and, in HDFS High-Availability instances, forcing NameNode failover.            OS      Hadoop      Nodes      ZooKeeper      HDFS High-Availability      Tests                  CentOS 6.5      HDP 2.0 (Apache 2.2.0)      6      HDP 2.0 (Apache 3.4.5)      Yes (QJM)      All required tests              CentOS 6.4      CDH 4.5.0 (2.0.0+cdh4.5.0)      7      CDH 4.5.0 (3.4.5+cdh4.5.0)      Yes (QJM)      Unit, functional and 24hr Randomwalk w/ agitation              CentOS 6.4      CDH 4.5.0 (2.0.0+cdh4.5.0)      7      CDH 4.5.0 (3.4.5+cdh4.5.0)      Yes (QJM)      2x 24/hr continuous ingest w/ verification              CentOS 6.3      Apache 1.0.4      1      Apache 3.3.5      No      Local testing, unit and functional tests              RHEL 6.4      Apache 2.2.0      10      Apache 3.4.5      No      Functional tests      ",
      "url": " /release/accumulo-1.5.1/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-4-4": {
      "title": "Apache Accumulo 1.4.4",
      "content": "Apache Accumulo 1.4.4 is a maintenance release containing these changes.Below are resources for this release:  User Manual - In-depth developer and administrator documentation  Javadocs - Accumulo API  Examples - Example Accumulo code",
      "url": " /release/accumulo-1.4.4/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-5-0": {
      "title": "Apache Accumulo 1.5.0",
      "content": "Apache Accumulo 1.5.0 is a significant release containing these changes.Below are resources for this release:  User Manual - In-depth developer and administrator documentation  Javadocs - Accumulo API  Examples - Example Accumulo code",
      "url": " /release/accumulo-1.5.0/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-4-3": {
      "title": "Apache Accumulo 1.4.3",
      "content": "Apache Accumulo 1.4.3 is a maintenance release containing these changes.Below are resources for this release:  User Manual - In-depth developer and administrator documentation  Javadocs - Accumulo API  Examples - Example Accumulo code",
      "url": " /release/accumulo-1.4.3/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-4-2": {
      "title": "Apache Accumulo 1.4.2",
      "content": "Apache Accumulo 1.4.2 is a maintenance release containing these changes.Below are resources for this release:  User Manual - In-depth developer and administrator documentation  Javadocs - Accumulo API  Examples - Example Accumulo code",
      "url": " /release/accumulo-1.4.2/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-3-6": {
      "title": "Apache Accumulo 1.3.6",
      "content": "Apache Accumulo 1.3.6 is a maintenance release containing these changes.Below are resources for this release:  User Manual - In-depth developer and administrator documentation",
      "url": " /release/accumulo-1.3.6/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-4-1": {
      "title": "Apache Accumulo 1.4.1",
      "content": "Apache Accumulo 1.4.1 is a maintenance release containing these changes.Below are resources for this release:  User Manual - In-depth developer and administrator documentation  Javadocs - Accumulo API  Examples - Example Accumulo code",
      "url": " /release/accumulo-1.4.1/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-4-0": {
      "title": "Apache Accumulo 1.4.0",
      "content": "Apache Accumulo 1.4.0 is a significant release containing these changes.Below are resources for this release:  User Manual - In-depth developer and administrator documentation  Javadocs - Accumulo API  Examples - Example Accumulo code",
      "url": " /release/accumulo-1.4.0/",
      "categories": "release"
    }
    ,
  
    "release-accumulo-1-3-5-incubating": {
      "title": "Apache Accumulo 1.3.5-incubating",
      "content": "Apache Accumulo 1.3.5-incubating is the first release of Accumulo containing these changes.Below are resources for this release:  User Manual - In-depth developer and administrator documentation",
      "url": " /release/accumulo-1.3.5-incubating/",
      "categories": "release"
    }
    
  
}
